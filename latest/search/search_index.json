{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"textX is a meta-language (i.e. a language for language definition) for domain-specific language (DSL) specification in Python. In a nutshell, textX will help you build your textual language in an easy way. You can invent your own language or build a support for an already existing textual language or file format. From a single grammar description, textX automatically builds a meta-model (in the form of Python classes) and a parser for your language. The parser will parse expressions of your language and automatically build a graph of Python objects (i.e. the model) corresponding to the meta-model. textX is inspired by Xtext - a Java based language workbench for building DSLs with full tooling support (editors, debuggers etc.) on the Eclipse platform. If you like Java and Eclipse check it out. It is a great tool. A video tutorial for textX installation and implementation of a simple data modeling language is bellow. For a not-so-basic video tutorial check out State Machine video tutorial . Feature highlights \u00b6 Meta-model/parser from a single description A single description is used to define both language concrete syntax and its meta-model (a.k.a. abstract syntax). See the description of grammar and metamodel . Automatic model (AST) construction Parse tree will automatically be transformed to a graph of python objects (a.k.a. the model). See the model section. Python classes will be created by textX but, if needed, user supplied classes may be used. See custom classes . Automatic linking You can have references to other objects in your language and the textual representation of the reference will be resolved to the proper python reference automatically. Automatic parent-child relationships textX will maintain a parent-child relationships imposed by the grammar. See parent-child relationships . Parser control Parser can be configured with regard to case handling, whitespace handling, keyword handling etc. See parser configuration . Model/object post-processing A callbacks (so called processors) can be registered for models and individual classes. This enables model/object postprocessing (validation, additional changes etc.). See processors section. Grammar modularization - imports Grammar can be split into multiple files and then files/grammars can be imported where needed. See Grammar modularization . Scope Providers Scope Providers allow different types of scoping. See Scoping . Multi-meta-model support Different meta-models can be combined. Typically some of these meta-models extend other meta-models (grammar modularization) and reference each other. Special scope providers support file-extension-based allocation of model files to meta models. See Multi meta-model support Meta-model/model visualization Both meta-model and parsed models can be visulized using GraphViz software package. See visualization section. Installation \u00b6 $ pip install textX Note: Previous command requires pip to be installed. To verify that textX is properly installed run: $ textx You should get output like this: error: the following arguments are required: cmd, metamodel usage: textx [-h] [-i] [-d] cmd metamodel [model] textX checker and visualizer ... Python versions \u00b6 textX works with Python 2.7, 3.3+. Other versions might work but are not tested. Getting started \u00b6 See textX Tutorials to get you started: Hello World Robot Entity State Machine - video tutorial Toy language compiler For specific information read various User Guide sections. Also, you can check out examples . Discussion and help \u00b6 For general discussion and help please use GitHub issue tracker . Projects using textX \u00b6 Here is a non-complete list of projects using textX. Open-source pyecore - ECore implementation in Python. Vincent Aranega is doing a great work on integrating textX with pyecore. The idea is that the integration eventually gets merged to the main textX repo. For now, you can follow his work on his fork of textX . pyTabs - A Domain-Specific Language (DSL) for simplified music notation applang - Textual DSL for generating mobile applications pyFlies - DSL for cognitive experiments modeling ppci - Pure python compiler infrastructure. Expremigen - Expressive midi generation fanalyse - Fortran code parser/analyser Commercial textX is used as a part of Typhoon-HIL's schematic editor for the description of power electronic and DSP schemes and components. JournaKit Followship .ows - A new language for social media management with an interactive console to follow and discover Twitter users (code released under GNU GPL 3). Read an announcement at author's blog post . If you are using textX to build some cool stuff drop me a line at igor dot dejanovic at gmail. I would like to hear from you! Editor/IDE support \u00b6 Visual Studio Code support \u00b6 There is currently an ongoing effort to build tooling support around Visual Studio Code . The idea is to auto-generate VCS plugin with syntax highlighting, outline, InteliSense, navigation, visualization. The input for the generator would be your language grammar and additional information specified using various DSLs. You can follow the progress at the textX-tools GitHub organization . Projects that are currently in progress are: viewX - creating visualizators for textX languages textX-languageserver - Language Server Protocol support for textX languages textX-extensions - syntax highlighting, code outline Stay tuned ;) Other editors \u00b6 If you are a vim editor user check out support for vim . For emacs there is textx-mode which is also available in MELPA . You can also check out textX-ninja project . It is currently unmaintained. Citing textX \u00b6 If you are using textX in your research project we would be very grateful if you cite our paper: Dejanovi\u0107 I., Vaderna R., Milosavljevi\u0107 G., Vukovi\u0107 \u017d. (2017). TextX: A Python tool for Domain-Specific Languages implementation. Knowledge-Based Systems, 115, 1-4. @article{Dejanovic2017, author = {Dejanovi\\'{c}, I. and Vaderna, R. and Milosavljevi\\'{c}, G. and Vukovi\\'{c}, \\v{Z}.}, doi = {10.1016/j.knosys.2016.10.023}, issn = {0950-7051}, journal = {Knowledge-Based Systems}, keywords = {Domain-Specific Language; Meta-model; Model; Model-Driven software development; Parser; Python }, note = {}, pages = {1--4}, title = {{TextX: A Python tool for Domain-Specific Languages implementation}}, url = {http://www.sciencedirect.com/science/article/pii/S0950705116304178}, volume = {115}, year = {2017} }","title":"Home"},{"location":"#feature-highlights","text":"Meta-model/parser from a single description A single description is used to define both language concrete syntax and its meta-model (a.k.a. abstract syntax). See the description of grammar and metamodel . Automatic model (AST) construction Parse tree will automatically be transformed to a graph of python objects (a.k.a. the model). See the model section. Python classes will be created by textX but, if needed, user supplied classes may be used. See custom classes . Automatic linking You can have references to other objects in your language and the textual representation of the reference will be resolved to the proper python reference automatically. Automatic parent-child relationships textX will maintain a parent-child relationships imposed by the grammar. See parent-child relationships . Parser control Parser can be configured with regard to case handling, whitespace handling, keyword handling etc. See parser configuration . Model/object post-processing A callbacks (so called processors) can be registered for models and individual classes. This enables model/object postprocessing (validation, additional changes etc.). See processors section. Grammar modularization - imports Grammar can be split into multiple files and then files/grammars can be imported where needed. See Grammar modularization . Scope Providers Scope Providers allow different types of scoping. See Scoping . Multi-meta-model support Different meta-models can be combined. Typically some of these meta-models extend other meta-models (grammar modularization) and reference each other. Special scope providers support file-extension-based allocation of model files to meta models. See Multi meta-model support Meta-model/model visualization Both meta-model and parsed models can be visulized using GraphViz software package. See visualization section.","title":"Feature highlights"},{"location":"#installation","text":"$ pip install textX Note: Previous command requires pip to be installed. To verify that textX is properly installed run: $ textx You should get output like this: error: the following arguments are required: cmd, metamodel usage: textx [-h] [-i] [-d] cmd metamodel [model] textX checker and visualizer ...","title":"Installation"},{"location":"#python-versions","text":"textX works with Python 2.7, 3.3+. Other versions might work but are not tested.","title":"Python versions"},{"location":"#getting-started","text":"See textX Tutorials to get you started: Hello World Robot Entity State Machine - video tutorial Toy language compiler For specific information read various User Guide sections. Also, you can check out examples .","title":"Getting started"},{"location":"#discussion-and-help","text":"For general discussion and help please use GitHub issue tracker .","title":"Discussion and help"},{"location":"#projects-using-textx","text":"Here is a non-complete list of projects using textX. Open-source pyecore - ECore implementation in Python. Vincent Aranega is doing a great work on integrating textX with pyecore. The idea is that the integration eventually gets merged to the main textX repo. For now, you can follow his work on his fork of textX . pyTabs - A Domain-Specific Language (DSL) for simplified music notation applang - Textual DSL for generating mobile applications pyFlies - DSL for cognitive experiments modeling ppci - Pure python compiler infrastructure. Expremigen - Expressive midi generation fanalyse - Fortran code parser/analyser Commercial textX is used as a part of Typhoon-HIL's schematic editor for the description of power electronic and DSP schemes and components. JournaKit Followship .ows - A new language for social media management with an interactive console to follow and discover Twitter users (code released under GNU GPL 3). Read an announcement at author's blog post . If you are using textX to build some cool stuff drop me a line at igor dot dejanovic at gmail. I would like to hear from you!","title":"Projects using textX"},{"location":"#editoride-support","text":"","title":"Editor/IDE support"},{"location":"#visual-studio-code-support","text":"There is currently an ongoing effort to build tooling support around Visual Studio Code . The idea is to auto-generate VCS plugin with syntax highlighting, outline, InteliSense, navigation, visualization. The input for the generator would be your language grammar and additional information specified using various DSLs. You can follow the progress at the textX-tools GitHub organization . Projects that are currently in progress are: viewX - creating visualizators for textX languages textX-languageserver - Language Server Protocol support for textX languages textX-extensions - syntax highlighting, code outline Stay tuned ;)","title":"Visual Studio Code support"},{"location":"#other-editors","text":"If you are a vim editor user check out support for vim . For emacs there is textx-mode which is also available in MELPA . You can also check out textX-ninja project . It is currently unmaintained.","title":"Other editors"},{"location":"#citing-textx","text":"If you are using textX in your research project we would be very grateful if you cite our paper: Dejanovi\u0107 I., Vaderna R., Milosavljevi\u0107 G., Vukovi\u0107 \u017d. (2017). TextX: A Python tool for Domain-Specific Languages implementation. Knowledge-Based Systems, 115, 1-4. @article{Dejanovic2017, author = {Dejanovi\\'{c}, I. and Vaderna, R. and Milosavljevi\\'{c}, G. and Vukovi\\'{c}, \\v{Z}.}, doi = {10.1016/j.knosys.2016.10.023}, issn = {0950-7051}, journal = {Knowledge-Based Systems}, keywords = {Domain-Specific Language; Meta-model; Model; Model-Driven software development; Parser; Python }, note = {}, pages = {1--4}, title = {{TextX: A Python tool for Domain-Specific Languages implementation}}, url = {http://www.sciencedirect.com/science/article/pii/S0950705116304178}, volume = {115}, year = {2017} }","title":"Citing textX"},{"location":"debugging/","text":"Debugging \u00b6 textX supports debugging on the meta-model (grammar) and model levels. If debugging is enabled, textX will print various debugging messages. If the debug parameter of the meta-model construction is set to True , debug messages during grammar parsing and meta-model construction will be printed. Additionally, a parse tree created during the grammar parsing as well as meta-model (if constructed successfully) dot files will be generated: form textx.metamodel import metamodel_from_file robot_metamodel = metamodel_from_file('robot.tx', debug=True) If debug is set in the model_from_file/str calls, various messages during the model parsing and construction will be printed. Additionally, parse tree created from the input as well as the model will be exported to dot files. robot_program = robot_metamodel.model_from_file('program.rbt', debug=True) Alternatively, you can use textx check or visualize command in debug mode. $ textx -d visualize robot.tx program.rbt *** PARSING LANGUAGE DEFINITION *** New rule: grammar_to_import -> RegExMatch New rule: import_stm -> Sequence New rule: rule_name -> RegExMatch New rule: param_name -> RegExMatch New rule: string_value -> OrderedChoice New rule: rule_param -> Sequence Rule rule_param founded in cache. New rule: rule_params -> Sequence ... >> Matching rule textx_model=Sequence at position 0 => */* This ex >> Matching rule ZeroOrMore in textx_model at position 0 => */* This ex >> Matching rule import_stm=Sequence in textx_model at position 0 => */* This ex ?? Try match rule StrMatch(import) in import_stm at position 0 => */* This ex >> Matching rule comment=OrderedChoice in import_stm at position 0 => */* This ex ?? Try match rule comment_line=RegExMatch(//.*?$) in comment at position 0 => */* This ex -- NoMatch at 0 ?? Try match rule comment_block=RegExMatch(/\\*(.|\\n)*?\\*/) in comment at position 0 => */* This ex ... Generating 'robot.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O robot.tx.dot' Generating 'program.rbt.dot' file for model. To convert to png run 'dot -Tpng -O program.rbt.dot' This command renders parse trees and parser models of both textX and your language to dot files. Also, a meta-model and model of the language will be rendered if parsed correctly. Note By default all debug messages will be printed to stdout. You can provide file parameter to metamodel_from_file/str to specify file-like object where all messages should go.","title":"Debugging"},{"location":"debugging/#debugging","text":"textX supports debugging on the meta-model (grammar) and model levels. If debugging is enabled, textX will print various debugging messages. If the debug parameter of the meta-model construction is set to True , debug messages during grammar parsing and meta-model construction will be printed. Additionally, a parse tree created during the grammar parsing as well as meta-model (if constructed successfully) dot files will be generated: form textx.metamodel import metamodel_from_file robot_metamodel = metamodel_from_file('robot.tx', debug=True) If debug is set in the model_from_file/str calls, various messages during the model parsing and construction will be printed. Additionally, parse tree created from the input as well as the model will be exported to dot files. robot_program = robot_metamodel.model_from_file('program.rbt', debug=True) Alternatively, you can use textx check or visualize command in debug mode. $ textx -d visualize robot.tx program.rbt *** PARSING LANGUAGE DEFINITION *** New rule: grammar_to_import -> RegExMatch New rule: import_stm -> Sequence New rule: rule_name -> RegExMatch New rule: param_name -> RegExMatch New rule: string_value -> OrderedChoice New rule: rule_param -> Sequence Rule rule_param founded in cache. New rule: rule_params -> Sequence ... >> Matching rule textx_model=Sequence at position 0 => */* This ex >> Matching rule ZeroOrMore in textx_model at position 0 => */* This ex >> Matching rule import_stm=Sequence in textx_model at position 0 => */* This ex ?? Try match rule StrMatch(import) in import_stm at position 0 => */* This ex >> Matching rule comment=OrderedChoice in import_stm at position 0 => */* This ex ?? Try match rule comment_line=RegExMatch(//.*?$) in comment at position 0 => */* This ex -- NoMatch at 0 ?? Try match rule comment_block=RegExMatch(/\\*(.|\\n)*?\\*/) in comment at position 0 => */* This ex ... Generating 'robot.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O robot.tx.dot' Generating 'program.rbt.dot' file for model. To convert to png run 'dot -Tpng -O program.rbt.dot' This command renders parse trees and parser models of both textX and your language to dot files. Also, a meta-model and model of the language will be rendered if parsed correctly. Note By default all debug messages will be printed to stdout. You can provide file parameter to metamodel_from_file/str to specify file-like object where all messages should go.","title":"Debugging"},{"location":"error_handling/","text":"Error handling \u00b6 textX will raise an error if a syntax or semantic error is detected during meta-model or model parsing/construction. For a syntax error TextXSyntaxError is raised. For a semantic error TextXSemanticError is raised. Both exceptions inherit from TextXError . These exceptions are located in textx.exceptions module. All exceptions have message attribute with the error message, and line and col attributes which represent line and column where the error was found. Note See also textx command/tool for (meta)model checking from command line.","title":"Error handling"},{"location":"error_handling/#error-handling","text":"textX will raise an error if a syntax or semantic error is detected during meta-model or model parsing/construction. For a syntax error TextXSyntaxError is raised. For a semantic error TextXSemanticError is raised. Both exceptions inherit from TextXError . These exceptions are located in textx.exceptions module. All exceptions have message attribute with the error message, and line and col attributes which represent line and column where the error was found. Note See also textx command/tool for (meta)model checking from command line.","title":"Error handling"},{"location":"grammar/","text":"textX grammar \u00b6 The language syntax and the meta-model are defined by the textX grammar in the form of a set of textX rules. Rules \u00b6 The basic building blocks of the textX language are rules. Each rule is written in the following form: Hello: 'hello' who=ID ; This rule is called Hello . After the rule name, there is a colon. The body of the rule is given as a textX expression, starting at the colon and ending with a semicolon. This rule tells us that the pattern of Hello objects in input strings consists of the string literal hello , followed by the ID rule (here ID is a reference to a built-in rule, more about this in a moment). These are valid Hello objects: hello Alice hello Bob hello foo1234 Rule Hello at the same time defines a Python class Hello . When the rule is recognized in the input stream, an object of this class will get created and the attribute who will be set to whatever the rule ID has matched after the word hello (this is specified by the assignment who=ID ). Of course, there are many more rule expressions than those shown in this small example. In the next section, a detailed description of each textX expression is given. textX base types \u00b6 In the previous example you have seen an ID rule. This rule is one of the built-in rules that form the base of textX's type system. Base types/rules are depicted in the following figure: ID rule: matches a common identifier consisting of letters, digits and underscores. The regex pattern that describe this rule is '[^\\d\\W]\\w*\\b' . This match will be converted to a Python string. INT rule: matches an integer number. This match will be converted to a Python int instance. FLOAT rule: will match a floating point number. This match will be converted to a Python float instance. BOOL rule: matches the words true or false . This match will be converted to a Python bool instance. STRING rule: matches a quoted string. This match will be converted to a Python str instance. textX base types are automatically converted to python types during object instantiation. See auto-initialization for more information. Rule expressions \u00b6 Rule expressions represent the body of a rule. They is specified using basic expressions and operators. The basic expressions are: Matches String match ( '...' or \"...\" ) Regex match ( /.../ ) Sequence Ordered choice ( | ) Optional ( ? ) Repetitions Zero or more ( * ) One or more ( + ) Unordered group ( # ) References Match reference Link reference ( [..] ) Assignments Plain ( = ) Boolean ( ?= ) Zero or more ( *= ) One or more ( += ) Syntactic predicates Not ( ! ) - negative lookahead And ( & ) - positive lookahead Match suppression Matches \u00b6 Match expressions are, besides base type rules, the expressions at the lowest level. They are the basic building blocks for more complex expressions. These expressions will consume input on success. There are two types of match expressions: String match - is written as a single quoted string. It will match a literal string on the input. Here are a few examples of string matches: 'blue' 'zero' 'person' Regex match - uses regular expression defined inside / / to match the input. Therefore, it defines a whole class of strings that can be matched. Internally a python re module is used. Here are few example of regex matches: /\\d*/ /\\d{3,4}-\\d{3}/ /[-\\w]*\\b/ /[^}]*/ For more information on Regular Expression in Python see Regular Expression HOWTO . When the metamodel has the option use_regexp_group set to True , then a regular expression with exactly one group is replaced by the group. This can be used to define multiline strings to be stored in the model without the surrounding limiters: Model: 'data' '=' data=/(?ms)\\\"{3}(.*?)\\\"{3}/; An example model could be data = \"\"\" This is a multiline text! \"\"\" When creating a metamodel with this grammar and the option use_regexp_group enabled, a multiline string delimited with \"\"\" is accepted: (?ms) activates the multiline option and the dot matches everything option. \\\"{3} matches the delimited \"\"\" . The pattern \"(.*?) is a non-greedy variant of get anything . metamodel = metamodel_from_str(grammar, use_regexp_group=True) Sequence \u00b6 Sequence is a textX expression that is given by just writing contained sub-expressions one after another. For example,the following rule: Colors: \"red\" \"green\" \"blue\" ; is defined as a sequence consisting of three string matches ( red green and blue ). Contained expressions will be matched in the exact order they are given. If some of the expressions do not match, the sequence as a whole will fail. The above rule defined by the sequence will match only the following string: red green blue Note If whitespace skipping is enabled (it is by default), arbitrary whitespaces can occur between matched words. Ordered choice \u00b6 Ordered choice is given as a set of expression separated by the | operator. This operator will try to match contained expression from left to right and the first match that succeeds will be used. Example: Color: \"red\" | \"green\" | \"blue\" ; This will match either red or green or blue and the parser will try to match the expressions in that order. Note In most classic parsing technologies an unordered match (alternative) is used. This may lead to ambiguous grammars where multiple parse tree may exist for the same input string. Underlying parsing technology of textX is Arpeggio which is a parser based on PEG grammars and thus the | operator directly translates to Arpeggio's PEG ordered choice. Using ordered choice yields unambiguous parsing. If the text parses there is only one possible parse tree. Optional \u00b6 Optional is an expression that will match the contained expression if that is possible, but will not fail otherwise. Thus, optional expression always succeeds. Example: MoveUp: 'up' INT? ; INT match is optional in this example. This means that the up keyword is required, but the following integer may or may not be found. Following lines will match: up 45 up 1 up Optional expressions can be more complex. For example: MoveUp: 'up' ( INT | FLOAT )? Now, an ordered choice in the parentheses is optional. Repetitions \u00b6 Zero or more repetition is specified by the * operator and will match the contained expression zero or more times. Here is an example: Colors: (\"red\"|\"green\"|\"blue\")* ; In this example zero or more repetition is applied on the ordered choice . In each repeated match one color will be matched, trying from left to right. Thus, Colors rule will match color as many times as possible, but will not fail if no color exists in the input string. The following would be matched by the Colors rule: red blue green but also: red blue blue red red green or an empty string. One or more repetition is specified by + operator and will match the contained expression one or more times. Thus, everything that is written for zero or more applies here except that at least one match must be found for this expression to succeed. Here is an above example modified to match at least one color: Colors: (\"red\"|\"green\"|\"blue\")+ ; Unordered group is a special kind of a sequence. Syntactically it is similar to a repetition. It is specified by the # operator and must be applied to sequences. This operator will match each element of the sequence in an arbitrary order: Colors: (\"red\" \"green\" \"blue\")# ; For the previous example all following lines are valid: red blue green red green blue blue green red ... But, the following lines are not valid: red blue red green blue green Consider this example: Modifier: (static?='static' final?='final' visibility=Visibility)# ; Visibility: 'public' | 'private' | 'protected'; We want to provide modifiers to the type declarations in our language. Furthermore, we want modifiers to be written in any order. The following lines will match (thanks to ?= operator, only visibility must be specified): public public static final protected static ... Note Unordered group may also have repetition modifiers defined. Assignments \u00b6 Assignments are used as a part of the meta-model deduction process. Each assignment will result in an attribute of the meta-class created by the rule. Each assignment consists of the LHS (left-hand side) and the RHS (right-hand side). The LHS is always a name of the meta-class attribute while the RHS can be a reference to other rules (either a match or a link reference) or a simple match (string or regex match). For example: Person: name=Name ',' surename=Surename ',' age=INT ',' height=INT ';' ; The Name and Surename rules referenced in the RHS of the first two assignments are not given in this example. This example describes the rule and meta-class Person , that will parse and instantiate the Person objects with these four attributes: name - which will use the rule Name to match the input, it will be a reference to the instance of the Name class, surename - will use Surename rule to match the input, age - will use the built-in type INT to match a number from the input string. age will be converted to the python int type. height - the same as age , but the matched number will be assigned to the height attribute of the Person instance. Notice the comma as the separator between matches and the semicolon match at the end of the rule. Those matches must be found in the input but the matched strings will be discarded. They represent a syntactic noise. If the RHS is one of textX BASETYPEs, then the matched string will be converted to some of the plain python types (e.g. int , string , boolean ). If the RHS is a string or regex match like in this example: Color: color=/\\w+/ ; then the attribute given by the LHS will be set as the string matched by the RHS regular expression or string. If the RHS is a reference to some other rule, then the attribute given by the LHS will be set to refer to the object created by the RHS rule. Following strings are matched by the Person rule from above: Petar, Petrovic, 27, 185; John, Doe, 34, 178; There are four types of assignments: Plain assignment ( = ) will match its RHS once and assign what is matched to the attribute given by the LHS. The above example uses plain assignments. Examples: a=INT b=FLOAT c=/[a-Z0-9]+/ dir=Direction Boolean assignment ( ?= ) will set the attribute to True if the RHS match succeeds and to False otherwise. Examples:: cold ?= 'cold' number_given ?= INT Zero or more assignment ( *= ) - LHS attribute will be a list . This assignment will keep matching the RHS as long as the match succeeds and each matched object will be appended to the attribute. If no match succeeds, the attribute will be an empty list. Examples:: commands*=Command numbers*=INT One or more assignment ( += ) - same as the previous assignment, but it must match the RHS at least once. If no match succeeds, this assignment does not succeed. Multiple assignment to the same attribute \u00b6 textX allows for multiple assignments to the same attribute. For example: MyRule: a=INT b=FLOAT a*=ID ; Here a attribute will always be a Python list. The type of a will be OBJECT as the two assignments have declared different types for a ( INT and ID ). Consider this example: Method: 'func(' (params+=Parameter[','])? ')' ; Parameter: type=ID name=ID | name=ID ; In Parameter rule, the name attribute assignments are part of different ordered choice alternatives and thus name will never have more than one value and thus should not be a list. The type of name is consistent in both assignments so it will be ID . The rule of the thumb for multiple assignments is that if there is no possibility for an attribute to collect more than one value during parsing it will be a single value object, otherwise it will be a list. References \u00b6 Rules can reference each other. References are usually used as a RHS of the assignments. There are two types of rule references: Match rule reference - will call another rule. When instance of the called rule is created, it will be assigned to the attribute on the LHS. We say that the referred object is contained inside the referring object (i.e. they form a parent-child relationship ). Example:: Structure: 'structure' '{' elements*=StructureElement '}' ; StructureElement will be matched zero or more times. With each match, a new instance of the StructureElement will be created and appended to the elements python list. A parent attribute of each StructureElement will be set to the containing Structure . Link rule reference - will match an identifier of some class object at the given place and convert that identifier to a python reference to the target object. This reference resolving is done automatically by textX. By default, a name attribute is used as the identifier of the object. Currently, there is no automatic support for namespaces in textX. All objects of the same class are in a single namespace. Example: ScreenType: 'screen' name=ID \"{\" '}' ; ScreenInstance: 'screen' type=[ScreenType] ; The type attribute is a link to the ScreenType object. This is a valid usage: // This is a definition of the ScreenType object screen Introduction { } // And this is a reference link to the ScreenType object defined above // ScreenInstance instance screen Introduction Introduction will be matched, the ScreenType object with that name will be found and the type attribute of ScreenInstance instance will be set to it. ID rule is used by default to match the link identifier. If you want to change that, you can use the following syntax: ScreenInstance: 'screen' type=[ScreenType|WORD] ; Here, instead of ID a WORD rule is used to match the object's identifier. Note Attributes with name names are used for reference auto-resolving. A dict lookup is used, thus they must be of a hashable type. See issue #40 . Syntactic predicates \u00b6 Syntactic predicates are operators that are used to implement lookahead. The lookahead is used to do parsing decision based on the part of the input ahead of the current position. Syntactic predicates are written as a prefix of some textX rule expression. The rule expression will be used to match input ahead of the current location in the input string. It will either fail or succeed but will never consume any input. There are two type of syntactic predicates: Not - negative lookahead ( ! ) - will succeed if the current input doesn't match the expression given after the ! operator. Example problem: Expression: Let | ID | NUMBER; Let: 'let' expr+=Expression 'end' ; In this example we have nested expressions built with indirectly recurssive Let rule. The problem is that the ID rule from Expression will match keyword end and thus will consume end of Let rule, so the parser will hit EOF without completing any Let rules. To fix this, we can specify that ID will match any identifier except keywords let and end like this: Expression: Let | MyID | NUMBER; Let: 'let' expr+=Expression 'end' ; Keyword: 'let' | 'end'; MyID: !Keyword ID; Now, MyID will match ID only if it is not a keyword. And - positive lookahead ( & ) - will succeed if the current input starts with the string matched by the expression given after the & operator. Example: Model: elements+=Element ; Element: AbeforeB | A | B ; AbeforeB: a='a' &'b' // this succeeds only if 'b' follows 'a' ; A: a='a'; B: a='b'; Given the input string a a a b first two a chars will be matched by the rule A , but the third a will be matched by the rule AbeforeB . So, even when AbeforeB matches only a and is tried before any other match, it will not succeed for the first two a chars because they are not followed by b . Match suppression \u00b6 Sometimes it is necessary to define match rules that should return only parts of the match. For that we use match the suppression operator ( - ) after the expression you want to suppress. For example: FullyQualifiedID[noskipws]: /\\s*/- QuotedID+['.'] /\\s*/- ; QuotedID: '\"'?- ID '\"'?- ; Because we use noskipws rule modifier, FullyQualifiedID does not skip whitespaces automatically. Thus, we have to match whitespaces ourself, but we don't want those whitespaces in the resulting string. You might wonder why we are using noskipws . It is because we do not want whitespaces in between each QuotedID match. So, for example, first. second shouldn't match but first.second should. In the rule FullyQualifiedID we are suppressing whitespace matches /\\s*/- . We also state in QuotedID that there are optional quotation marks around each ID, but we don't want those either '\"'?- . Given this input: first.\"second\".third.\"fourth\" FullyQualifiedID will return: first.second.third.fourth Repetition modifiers \u00b6 Repetition modifiers are used for the modification of the repetition expressions ( * , + , # , *= , += ). They are specified in brackets [ ] . If there are more modifiers, they are separated by a comma. Currently, there are two modifiers defined: Separator modifier - is used to define separator on multiple matches. Separator is a simple match (string match or regex match). Example: numbers*=INT[','] Here, a separator string match is defined ( ',' ). This will match zero or more integers separated by commas. 45, 47, 3, 78 A regex can also be specified as a separator. fields += ID[/;|,|:/] This will match IDs separated by either ; or , or : . first, second; third, fourth: fifth End-of-line terminate modifier ( eolterm ) - used to terminate repetition on end-of-line. By default the repetition match will span lines. When this modifier is specified, repetition will work inside the current line only. Example: STRING*[',', eolterm] Here we have a separator as well as the eolterm defined. This will match zero or more strings separated by commas inside one line. \"first\", \"second\", \"third\" \"fourth\" If we run the example expression once on this string, it will match the first line only. \"fourth\" in the second line will not be matched. Warning Be aware that when eolterm modifier is used, its effect starts from the previous match. For example: Conditions: 'conditions' '{' varNames+=WORD[eolterm] // match var names until end of line '}' In this example varNames must be matched in the same line as conditions { because eolterm effect start immediately. In this example we wanted to give the user the freedom to specify var names on the next line, even to put some empty lines if he/she wishes. In order to do that, we should modify the example like this:: Conditions: 'conditions' '{' /\\s*/ varNames+=WORD[eolterm] // match var names until end of line '}' Regex match /\\s*/ will collect whitespaces (spaces and new-lines) before the WORD match begins. Afterwards, repeated matches will work inside one line only. Rule types \u00b6 There are three kinds of rules in textX: Common rules (or just rules) Abstract rules Match rules Common rules are rules that contain at least one assignment, i.e., they have attributes defined. For example: InitialCommand: 'initial' x=INT ',' y=INT ; This rule has two defined attributes: x and y . Abstract rules are rules that have no assignments and reference at least one abstract or common rule. They are usually given as an ordered choice of other rules and they are used to generalize other rules. For example: Program: 'begin' commands*=Command 'end' ; Command: MoveCommand | InitialCommand ; In this example, Python objects in the commands list will either contain instances of MoveCommand or InitialCommand . Command rule is abstract. A meta-class of this rule will never be instantiated. Abstract rule can also be used in link rule references: ListOfCommands: commands*=[Command][','] ; Abstract rules may reference match rules and base types. For example: Value: STRING | FLOAT | BOOL | Object | Array | \"null\" ; In this example, the base types as well as the string match \"null\" are all match rules, but Object and Array are common rules and therefore Value is abstract. Abstract rules can be a complex mix of rule references and match expressions as long as there is at least one abstract or common reference. For example: Value: 'id' /\\d+-\\d+/ | FLOAT | Object ; A rule with a single reference to an abstract or common rule is also abstract: Value: OtherRule ; Match rules are rules that have no assignments either direct or indirect, i.e. all referenced rules are match rules too. They are usually used to specify enumerated values or some complex string matches that can't be done with regular expressions. Examples: Widget: \"edit\"|\"combo\"|\"checkbox\"|\"togglebutton\" ; Name: STRING|/(\\w|\\+|-)+/ ; Value: /(\\w|\\+|-)+/ | FLOAT | INT ; These rules can be used in match references only (i.e., you can't link to these rules as they don't exists as objects), and they produce objects of the base python types ( str , int , bool , float ). All base type rules (e.g., INT , STRING , BASETYPE ) are match rules. Rule modifiers \u00b6 Rule modifiers are used for the modification of the rule's expression. They are specified in brackets ( [ ] ) at the beginning of the rule's definition after the rule's name. Currently, they are used to alter parser configuration for whitespace handling on the rule level. There are two rule modifiers at the moment: skipws, noskipws - are used to enable/disable whitespace skipping during parsing. This will change the global parser's skipws setting given during the meta-model instantiation. Example: Rule: 'entity' name=ID /\\s*/ call=Rule2; Rule2[noskipws]: 'first' 'second'; In this example Rule rule will use default parser behaviour set during the meta-model instantiation, while Rule2 rule will disable whitespace skipping. This will change Rule2 to match the word firstsecond , but not words first second with whitespaces in between. Note Remember that whitespace handling modification will start immediately after the previous match. In the above example, additional /\\s*/ is given before the Rule2 call to consume all whitespaces before trying to match Rule2 . ws - used to redefine what is considered to be a whitespaces on the rule level. textX by default treats space, tab and new-line as a whitespace characters. This can be changed globally during the meta-model instantiation (see Whitespace handling ) or per rule using this modifier. Example: Rule: 'entity' name=ID /\\s*/ call=Rule2; Rule2[ws='\\n']: 'first' 'second'; In this example Rule will use the default parser behavior but the Rule2 will alter the white-space definition to be new-line only. This means that the words first and second will get matched only if they are on separate lines or in the same line but without other characters in between (even tabs and spaces). Note As in the previous example, the modification will start immediately, so if you want to consume preceding spaces you must do that explicitly, as given with /\\s*/ in the : Rule . Grammar comments \u00b6 Syntax for comments inside a grammar is // for line comments and /* ... */ for block comments. Language comments \u00b6 To support comments in your DSL use a special grammar rule Comment . textX will try to match this rule in between each other normal grammar match (similarly to the whitespace matching). If the match succeeds, the matched content will be discarded. For example, in the robot language example comments are defined like this: Comment: /\\/\\/.*$/ ; Which states that everything starting with // and continuing until the end of line is a comment. Grammar modularization \u00b6 Grammars can be defined in multiple files and than imported. Rules used in the references are first searched for in the current file and then in the imported files, in the order of the import. Example: import scheme Library: 'library' name=Name '{' attributes*=LibraryAttribute scheme=Scheme '}' ; Scheme rule is defined in scheme.tx grammar file imported at the beginning. Grammar files may be located in folders. In that case, dot notation is used. Example: import component.types types.tx grammar is located in the component folder relatively to the current grammar file. If you want to override the default search order, you can specify a fully qualified name of the rule using dot notation when giving the name of the referring object. Example: import component.types MyRule: a = component.types.List ; List: '[' values+=BASETYPE[','] ']' ; List from component.types is matched/instantiated and set to a attribute.","title":"Grammar"},{"location":"grammar/#textx-grammar","text":"The language syntax and the meta-model are defined by the textX grammar in the form of a set of textX rules.","title":"textX grammar"},{"location":"grammar/#rules","text":"The basic building blocks of the textX language are rules. Each rule is written in the following form: Hello: 'hello' who=ID ; This rule is called Hello . After the rule name, there is a colon. The body of the rule is given as a textX expression, starting at the colon and ending with a semicolon. This rule tells us that the pattern of Hello objects in input strings consists of the string literal hello , followed by the ID rule (here ID is a reference to a built-in rule, more about this in a moment). These are valid Hello objects: hello Alice hello Bob hello foo1234 Rule Hello at the same time defines a Python class Hello . When the rule is recognized in the input stream, an object of this class will get created and the attribute who will be set to whatever the rule ID has matched after the word hello (this is specified by the assignment who=ID ). Of course, there are many more rule expressions than those shown in this small example. In the next section, a detailed description of each textX expression is given.","title":"Rules"},{"location":"grammar/#textx-base-types","text":"In the previous example you have seen an ID rule. This rule is one of the built-in rules that form the base of textX's type system. Base types/rules are depicted in the following figure: ID rule: matches a common identifier consisting of letters, digits and underscores. The regex pattern that describe this rule is '[^\\d\\W]\\w*\\b' . This match will be converted to a Python string. INT rule: matches an integer number. This match will be converted to a Python int instance. FLOAT rule: will match a floating point number. This match will be converted to a Python float instance. BOOL rule: matches the words true or false . This match will be converted to a Python bool instance. STRING rule: matches a quoted string. This match will be converted to a Python str instance. textX base types are automatically converted to python types during object instantiation. See auto-initialization for more information.","title":"textX base types"},{"location":"grammar/#rule-expressions","text":"Rule expressions represent the body of a rule. They is specified using basic expressions and operators. The basic expressions are: Matches String match ( '...' or \"...\" ) Regex match ( /.../ ) Sequence Ordered choice ( | ) Optional ( ? ) Repetitions Zero or more ( * ) One or more ( + ) Unordered group ( # ) References Match reference Link reference ( [..] ) Assignments Plain ( = ) Boolean ( ?= ) Zero or more ( *= ) One or more ( += ) Syntactic predicates Not ( ! ) - negative lookahead And ( & ) - positive lookahead Match suppression","title":"Rule expressions"},{"location":"grammar/#matches","text":"Match expressions are, besides base type rules, the expressions at the lowest level. They are the basic building blocks for more complex expressions. These expressions will consume input on success. There are two types of match expressions: String match - is written as a single quoted string. It will match a literal string on the input. Here are a few examples of string matches: 'blue' 'zero' 'person' Regex match - uses regular expression defined inside / / to match the input. Therefore, it defines a whole class of strings that can be matched. Internally a python re module is used. Here are few example of regex matches: /\\d*/ /\\d{3,4}-\\d{3}/ /[-\\w]*\\b/ /[^}]*/ For more information on Regular Expression in Python see Regular Expression HOWTO . When the metamodel has the option use_regexp_group set to True , then a regular expression with exactly one group is replaced by the group. This can be used to define multiline strings to be stored in the model without the surrounding limiters: Model: 'data' '=' data=/(?ms)\\\"{3}(.*?)\\\"{3}/; An example model could be data = \"\"\" This is a multiline text! \"\"\" When creating a metamodel with this grammar and the option use_regexp_group enabled, a multiline string delimited with \"\"\" is accepted: (?ms) activates the multiline option and the dot matches everything option. \\\"{3} matches the delimited \"\"\" . The pattern \"(.*?) is a non-greedy variant of get anything . metamodel = metamodel_from_str(grammar, use_regexp_group=True)","title":"Matches"},{"location":"grammar/#sequence","text":"Sequence is a textX expression that is given by just writing contained sub-expressions one after another. For example,the following rule: Colors: \"red\" \"green\" \"blue\" ; is defined as a sequence consisting of three string matches ( red green and blue ). Contained expressions will be matched in the exact order they are given. If some of the expressions do not match, the sequence as a whole will fail. The above rule defined by the sequence will match only the following string: red green blue Note If whitespace skipping is enabled (it is by default), arbitrary whitespaces can occur between matched words.","title":"Sequence"},{"location":"grammar/#ordered-choice","text":"Ordered choice is given as a set of expression separated by the | operator. This operator will try to match contained expression from left to right and the first match that succeeds will be used. Example: Color: \"red\" | \"green\" | \"blue\" ; This will match either red or green or blue and the parser will try to match the expressions in that order. Note In most classic parsing technologies an unordered match (alternative) is used. This may lead to ambiguous grammars where multiple parse tree may exist for the same input string. Underlying parsing technology of textX is Arpeggio which is a parser based on PEG grammars and thus the | operator directly translates to Arpeggio's PEG ordered choice. Using ordered choice yields unambiguous parsing. If the text parses there is only one possible parse tree.","title":"Ordered choice"},{"location":"grammar/#optional","text":"Optional is an expression that will match the contained expression if that is possible, but will not fail otherwise. Thus, optional expression always succeeds. Example: MoveUp: 'up' INT? ; INT match is optional in this example. This means that the up keyword is required, but the following integer may or may not be found. Following lines will match: up 45 up 1 up Optional expressions can be more complex. For example: MoveUp: 'up' ( INT | FLOAT )? Now, an ordered choice in the parentheses is optional.","title":"Optional"},{"location":"grammar/#repetitions","text":"Zero or more repetition is specified by the * operator and will match the contained expression zero or more times. Here is an example: Colors: (\"red\"|\"green\"|\"blue\")* ; In this example zero or more repetition is applied on the ordered choice . In each repeated match one color will be matched, trying from left to right. Thus, Colors rule will match color as many times as possible, but will not fail if no color exists in the input string. The following would be matched by the Colors rule: red blue green but also: red blue blue red red green or an empty string. One or more repetition is specified by + operator and will match the contained expression one or more times. Thus, everything that is written for zero or more applies here except that at least one match must be found for this expression to succeed. Here is an above example modified to match at least one color: Colors: (\"red\"|\"green\"|\"blue\")+ ; Unordered group is a special kind of a sequence. Syntactically it is similar to a repetition. It is specified by the # operator and must be applied to sequences. This operator will match each element of the sequence in an arbitrary order: Colors: (\"red\" \"green\" \"blue\")# ; For the previous example all following lines are valid: red blue green red green blue blue green red ... But, the following lines are not valid: red blue red green blue green Consider this example: Modifier: (static?='static' final?='final' visibility=Visibility)# ; Visibility: 'public' | 'private' | 'protected'; We want to provide modifiers to the type declarations in our language. Furthermore, we want modifiers to be written in any order. The following lines will match (thanks to ?= operator, only visibility must be specified): public public static final protected static ... Note Unordered group may also have repetition modifiers defined.","title":"Repetitions"},{"location":"grammar/#assignments","text":"Assignments are used as a part of the meta-model deduction process. Each assignment will result in an attribute of the meta-class created by the rule. Each assignment consists of the LHS (left-hand side) and the RHS (right-hand side). The LHS is always a name of the meta-class attribute while the RHS can be a reference to other rules (either a match or a link reference) or a simple match (string or regex match). For example: Person: name=Name ',' surename=Surename ',' age=INT ',' height=INT ';' ; The Name and Surename rules referenced in the RHS of the first two assignments are not given in this example. This example describes the rule and meta-class Person , that will parse and instantiate the Person objects with these four attributes: name - which will use the rule Name to match the input, it will be a reference to the instance of the Name class, surename - will use Surename rule to match the input, age - will use the built-in type INT to match a number from the input string. age will be converted to the python int type. height - the same as age , but the matched number will be assigned to the height attribute of the Person instance. Notice the comma as the separator between matches and the semicolon match at the end of the rule. Those matches must be found in the input but the matched strings will be discarded. They represent a syntactic noise. If the RHS is one of textX BASETYPEs, then the matched string will be converted to some of the plain python types (e.g. int , string , boolean ). If the RHS is a string or regex match like in this example: Color: color=/\\w+/ ; then the attribute given by the LHS will be set as the string matched by the RHS regular expression or string. If the RHS is a reference to some other rule, then the attribute given by the LHS will be set to refer to the object created by the RHS rule. Following strings are matched by the Person rule from above: Petar, Petrovic, 27, 185; John, Doe, 34, 178; There are four types of assignments: Plain assignment ( = ) will match its RHS once and assign what is matched to the attribute given by the LHS. The above example uses plain assignments. Examples: a=INT b=FLOAT c=/[a-Z0-9]+/ dir=Direction Boolean assignment ( ?= ) will set the attribute to True if the RHS match succeeds and to False otherwise. Examples:: cold ?= 'cold' number_given ?= INT Zero or more assignment ( *= ) - LHS attribute will be a list . This assignment will keep matching the RHS as long as the match succeeds and each matched object will be appended to the attribute. If no match succeeds, the attribute will be an empty list. Examples:: commands*=Command numbers*=INT One or more assignment ( += ) - same as the previous assignment, but it must match the RHS at least once. If no match succeeds, this assignment does not succeed.","title":"Assignments"},{"location":"grammar/#multiple-assignment-to-the-same-attribute","text":"textX allows for multiple assignments to the same attribute. For example: MyRule: a=INT b=FLOAT a*=ID ; Here a attribute will always be a Python list. The type of a will be OBJECT as the two assignments have declared different types for a ( INT and ID ). Consider this example: Method: 'func(' (params+=Parameter[','])? ')' ; Parameter: type=ID name=ID | name=ID ; In Parameter rule, the name attribute assignments are part of different ordered choice alternatives and thus name will never have more than one value and thus should not be a list. The type of name is consistent in both assignments so it will be ID . The rule of the thumb for multiple assignments is that if there is no possibility for an attribute to collect more than one value during parsing it will be a single value object, otherwise it will be a list.","title":"Multiple assignment to the same attribute"},{"location":"grammar/#references","text":"Rules can reference each other. References are usually used as a RHS of the assignments. There are two types of rule references: Match rule reference - will call another rule. When instance of the called rule is created, it will be assigned to the attribute on the LHS. We say that the referred object is contained inside the referring object (i.e. they form a parent-child relationship ). Example:: Structure: 'structure' '{' elements*=StructureElement '}' ; StructureElement will be matched zero or more times. With each match, a new instance of the StructureElement will be created and appended to the elements python list. A parent attribute of each StructureElement will be set to the containing Structure . Link rule reference - will match an identifier of some class object at the given place and convert that identifier to a python reference to the target object. This reference resolving is done automatically by textX. By default, a name attribute is used as the identifier of the object. Currently, there is no automatic support for namespaces in textX. All objects of the same class are in a single namespace. Example: ScreenType: 'screen' name=ID \"{\" '}' ; ScreenInstance: 'screen' type=[ScreenType] ; The type attribute is a link to the ScreenType object. This is a valid usage: // This is a definition of the ScreenType object screen Introduction { } // And this is a reference link to the ScreenType object defined above // ScreenInstance instance screen Introduction Introduction will be matched, the ScreenType object with that name will be found and the type attribute of ScreenInstance instance will be set to it. ID rule is used by default to match the link identifier. If you want to change that, you can use the following syntax: ScreenInstance: 'screen' type=[ScreenType|WORD] ; Here, instead of ID a WORD rule is used to match the object's identifier. Note Attributes with name names are used for reference auto-resolving. A dict lookup is used, thus they must be of a hashable type. See issue #40 .","title":"References"},{"location":"grammar/#syntactic-predicates","text":"Syntactic predicates are operators that are used to implement lookahead. The lookahead is used to do parsing decision based on the part of the input ahead of the current position. Syntactic predicates are written as a prefix of some textX rule expression. The rule expression will be used to match input ahead of the current location in the input string. It will either fail or succeed but will never consume any input. There are two type of syntactic predicates: Not - negative lookahead ( ! ) - will succeed if the current input doesn't match the expression given after the ! operator. Example problem: Expression: Let | ID | NUMBER; Let: 'let' expr+=Expression 'end' ; In this example we have nested expressions built with indirectly recurssive Let rule. The problem is that the ID rule from Expression will match keyword end and thus will consume end of Let rule, so the parser will hit EOF without completing any Let rules. To fix this, we can specify that ID will match any identifier except keywords let and end like this: Expression: Let | MyID | NUMBER; Let: 'let' expr+=Expression 'end' ; Keyword: 'let' | 'end'; MyID: !Keyword ID; Now, MyID will match ID only if it is not a keyword. And - positive lookahead ( & ) - will succeed if the current input starts with the string matched by the expression given after the & operator. Example: Model: elements+=Element ; Element: AbeforeB | A | B ; AbeforeB: a='a' &'b' // this succeeds only if 'b' follows 'a' ; A: a='a'; B: a='b'; Given the input string a a a b first two a chars will be matched by the rule A , but the third a will be matched by the rule AbeforeB . So, even when AbeforeB matches only a and is tried before any other match, it will not succeed for the first two a chars because they are not followed by b .","title":"Syntactic predicates"},{"location":"grammar/#match-suppression","text":"Sometimes it is necessary to define match rules that should return only parts of the match. For that we use match the suppression operator ( - ) after the expression you want to suppress. For example: FullyQualifiedID[noskipws]: /\\s*/- QuotedID+['.'] /\\s*/- ; QuotedID: '\"'?- ID '\"'?- ; Because we use noskipws rule modifier, FullyQualifiedID does not skip whitespaces automatically. Thus, we have to match whitespaces ourself, but we don't want those whitespaces in the resulting string. You might wonder why we are using noskipws . It is because we do not want whitespaces in between each QuotedID match. So, for example, first. second shouldn't match but first.second should. In the rule FullyQualifiedID we are suppressing whitespace matches /\\s*/- . We also state in QuotedID that there are optional quotation marks around each ID, but we don't want those either '\"'?- . Given this input: first.\"second\".third.\"fourth\" FullyQualifiedID will return: first.second.third.fourth","title":"Match suppression"},{"location":"grammar/#repetition-modifiers","text":"Repetition modifiers are used for the modification of the repetition expressions ( * , + , # , *= , += ). They are specified in brackets [ ] . If there are more modifiers, they are separated by a comma. Currently, there are two modifiers defined: Separator modifier - is used to define separator on multiple matches. Separator is a simple match (string match or regex match). Example: numbers*=INT[','] Here, a separator string match is defined ( ',' ). This will match zero or more integers separated by commas. 45, 47, 3, 78 A regex can also be specified as a separator. fields += ID[/;|,|:/] This will match IDs separated by either ; or , or : . first, second; third, fourth: fifth End-of-line terminate modifier ( eolterm ) - used to terminate repetition on end-of-line. By default the repetition match will span lines. When this modifier is specified, repetition will work inside the current line only. Example: STRING*[',', eolterm] Here we have a separator as well as the eolterm defined. This will match zero or more strings separated by commas inside one line. \"first\", \"second\", \"third\" \"fourth\" If we run the example expression once on this string, it will match the first line only. \"fourth\" in the second line will not be matched. Warning Be aware that when eolterm modifier is used, its effect starts from the previous match. For example: Conditions: 'conditions' '{' varNames+=WORD[eolterm] // match var names until end of line '}' In this example varNames must be matched in the same line as conditions { because eolterm effect start immediately. In this example we wanted to give the user the freedom to specify var names on the next line, even to put some empty lines if he/she wishes. In order to do that, we should modify the example like this:: Conditions: 'conditions' '{' /\\s*/ varNames+=WORD[eolterm] // match var names until end of line '}' Regex match /\\s*/ will collect whitespaces (spaces and new-lines) before the WORD match begins. Afterwards, repeated matches will work inside one line only.","title":"Repetition modifiers"},{"location":"grammar/#rule-types","text":"There are three kinds of rules in textX: Common rules (or just rules) Abstract rules Match rules Common rules are rules that contain at least one assignment, i.e., they have attributes defined. For example: InitialCommand: 'initial' x=INT ',' y=INT ; This rule has two defined attributes: x and y . Abstract rules are rules that have no assignments and reference at least one abstract or common rule. They are usually given as an ordered choice of other rules and they are used to generalize other rules. For example: Program: 'begin' commands*=Command 'end' ; Command: MoveCommand | InitialCommand ; In this example, Python objects in the commands list will either contain instances of MoveCommand or InitialCommand . Command rule is abstract. A meta-class of this rule will never be instantiated. Abstract rule can also be used in link rule references: ListOfCommands: commands*=[Command][','] ; Abstract rules may reference match rules and base types. For example: Value: STRING | FLOAT | BOOL | Object | Array | \"null\" ; In this example, the base types as well as the string match \"null\" are all match rules, but Object and Array are common rules and therefore Value is abstract. Abstract rules can be a complex mix of rule references and match expressions as long as there is at least one abstract or common reference. For example: Value: 'id' /\\d+-\\d+/ | FLOAT | Object ; A rule with a single reference to an abstract or common rule is also abstract: Value: OtherRule ; Match rules are rules that have no assignments either direct or indirect, i.e. all referenced rules are match rules too. They are usually used to specify enumerated values or some complex string matches that can't be done with regular expressions. Examples: Widget: \"edit\"|\"combo\"|\"checkbox\"|\"togglebutton\" ; Name: STRING|/(\\w|\\+|-)+/ ; Value: /(\\w|\\+|-)+/ | FLOAT | INT ; These rules can be used in match references only (i.e., you can't link to these rules as they don't exists as objects), and they produce objects of the base python types ( str , int , bool , float ). All base type rules (e.g., INT , STRING , BASETYPE ) are match rules.","title":"Rule types"},{"location":"grammar/#rule-modifiers","text":"Rule modifiers are used for the modification of the rule's expression. They are specified in brackets ( [ ] ) at the beginning of the rule's definition after the rule's name. Currently, they are used to alter parser configuration for whitespace handling on the rule level. There are two rule modifiers at the moment: skipws, noskipws - are used to enable/disable whitespace skipping during parsing. This will change the global parser's skipws setting given during the meta-model instantiation. Example: Rule: 'entity' name=ID /\\s*/ call=Rule2; Rule2[noskipws]: 'first' 'second'; In this example Rule rule will use default parser behaviour set during the meta-model instantiation, while Rule2 rule will disable whitespace skipping. This will change Rule2 to match the word firstsecond , but not words first second with whitespaces in between. Note Remember that whitespace handling modification will start immediately after the previous match. In the above example, additional /\\s*/ is given before the Rule2 call to consume all whitespaces before trying to match Rule2 . ws - used to redefine what is considered to be a whitespaces on the rule level. textX by default treats space, tab and new-line as a whitespace characters. This can be changed globally during the meta-model instantiation (see Whitespace handling ) or per rule using this modifier. Example: Rule: 'entity' name=ID /\\s*/ call=Rule2; Rule2[ws='\\n']: 'first' 'second'; In this example Rule will use the default parser behavior but the Rule2 will alter the white-space definition to be new-line only. This means that the words first and second will get matched only if they are on separate lines or in the same line but without other characters in between (even tabs and spaces). Note As in the previous example, the modification will start immediately, so if you want to consume preceding spaces you must do that explicitly, as given with /\\s*/ in the : Rule .","title":"Rule modifiers"},{"location":"grammar/#grammar-comments","text":"Syntax for comments inside a grammar is // for line comments and /* ... */ for block comments.","title":"Grammar comments"},{"location":"grammar/#language-comments","text":"To support comments in your DSL use a special grammar rule Comment . textX will try to match this rule in between each other normal grammar match (similarly to the whitespace matching). If the match succeeds, the matched content will be discarded. For example, in the robot language example comments are defined like this: Comment: /\\/\\/.*$/ ; Which states that everything starting with // and continuing until the end of line is a comment.","title":"Language comments"},{"location":"grammar/#grammar-modularization","text":"Grammars can be defined in multiple files and than imported. Rules used in the references are first searched for in the current file and then in the imported files, in the order of the import. Example: import scheme Library: 'library' name=Name '{' attributes*=LibraryAttribute scheme=Scheme '}' ; Scheme rule is defined in scheme.tx grammar file imported at the beginning. Grammar files may be located in folders. In that case, dot notation is used. Example: import component.types types.tx grammar is located in the component folder relatively to the current grammar file. If you want to override the default search order, you can specify a fully qualified name of the rule using dot notation when giving the name of the referring object. Example: import component.types MyRule: a = component.types.List ; List: '[' values+=BASETYPE[','] ']' ; List from component.types is matched/instantiated and set to a attribute.","title":"Grammar modularization"},{"location":"metamodel/","text":"textX meta-models \u00b6 textX meta-model is a Python object that knows about all classes that can be instantiated while parsing the input. A meta-model is built from the grammar by the functions metamodel_from_file or metamodel_from_str in the textx.metamodel module. from textx import metamodel_from_file my_metamodel = metamodel_from_file('my_grammar.tx') Each rule from the grammar will result in a Python class kept in the meta-model. Besides, meta-model knows how to parse the input strings and convert them to model . Parsing the input and creating the model is done by model_from_file and model_from_str methods of the meta-model object: my_model = my_metamodel.model_from_file('some_input.md') When parsing a model file or string a new parser is cloned for each model. This parser can be accessed via the model attribute _tx_parser . Custom classes \u00b6 For each grammar rule a Python class with the same name is created dynamically. These classes are instantiated during the parsing of the input string/file to create a graph of python objects, a.k.a. model or Abstract-Syntax Tree (AST). Most of the time dynamically created classes will be sufficient, but sometimes you will want to use your own classes instead. To do so use parameter classes during the meta-model instantiation. This parameter is a list of your classes that should be named the same as the rules from the grammar which they represent. from textx import metamodel_from_str grammar = ''' EntityModel: entities+=Entity // each model has one or more entities ; Entity: 'entity' name=ID '{' attributes+=Attribute // each entity has one or more attributes '}' ; Attribute: name=ID ':' type=[Entity] // type is a reference to an entity. There are // built-in entities registered on the meta-model // for primitive types (integer, string) ; ''' class Entity(object): def __init__(self, parent, name, attributes): self.parent = parent self.name = name self.attributes = attributes # Use our Entity class. \"Attribute\" class will be created dynamically. entity_mm = metamodel_from_str(grammar, classes=[Entity]) Now entity_mm can be used to parse the input models where our Entity class will be instantiated to represent each Entity rule from the grammar. Note Constructor of the user-defined classes should accept all attributes defined by the corresponding rule from the grammar. In the previous example, we have provided name and attributes attributes from the Entity rule. If the class is a child in a parent-child relationship (see the next section), then the parent constructor parameter should also be given. Parent-child relationships \u00b6 There is often an intrinsic parent-child relationship between object in the model. In the previous example, each Attribute instance will always be a child of some Entity object. textX gives automatic support for these relationships by providing the parent attribute on each child object. When you navigate model each child instance will have a parent attribute. Note Always provide the parent parameter in user-defined classes for each class that is a child in a parent-child relationship. Processors \u00b6 To specify static semantics of the language textX uses a concept called processor . Processors are python callables that can modify the model elements during model parsing/instantiation or do some additional checks that are not possible to do by the grammar alone. There are two types of processors: model processors - are callables that are called at the end of the parsing when the whole model is instantiated. These processors accept the model and meta-model as parameters. object processors - are registered for particular classes (grammar rules) and are called when the objects of the given class is instantiated. Processors can modify model/objects or raise exception ( TextXSemanticError ) if some constraint is not met. User code that calls the model instantiation/parsing can catch and handle this exception. Model processors \u00b6 To register a model processor call register_model_processor on the meta-model instance. from textx import metamodel_from_file # Model processor is a callable that will accept meta-model and model as its # parameters. def check_some_semantics(model, metamodel): ... ... Do some check on the model and raise TextXSemanticError if the semantics ... rules are violated. my_metamodel = metamodel_from_file('mygrammar.tx') # Register model processor on the meta-model instance my_metamodel.register_model_processor(check_some_semantics) # Parse the model. check_some_semantics will be called automatically after # a successful parse to do further checks. If the rules are not met, # an instance of TextXSemanticError will be raised. my_metamodel.model_from_file('some_model.ext') Object processors \u00b6 The purpose of the object processors is to validate or alter the object being constructed. They are registered per class/rule. Let's do some additional checks for the above Entity DSL example. def entity_obj_processor(entity): ''' Check that Ethe ntity names are capitalized. This could also be specified in the grammar using regex match but we will do that check here just as an example. ''' if entity.name != entity.name.capitalize(): raise TextXSemanticError('Entity name \"%s\" must be capitalized.' % entity.name) def attribute_obj_processor(attribute): ''' Obj. processors can also introduce changes in the objects they process. Here we set \"primitive\" attribute based on the Entity they refer to. ''' attribute.primitive = attribute.type.name in ['integer', 'string'] # Object processors are registered by defining a map between a rule name # and the callable that will process the instances of that rule/class. obj_processors = { 'Entity': entity_obj_processor, 'Attribute': attribute_obj_processor, } # This map/dict is registered on a meta-model by the \"register_obj_processors\" # call. entity_mm.register_obj_processors(obj_processors) # Parse the model. At each successful parse of Entity or Attribute, the registered # processor will be called and the semantics error will be raised if the # check does not pass. entity_mm.model_from_file('my_entity_model.ent') For another example of the usage of an object processor that modifies the objects, see object processor move_command_processor robot example . If object processor returns a value that value will be used instead of the original object. This can be used to implement e.g. expression evaluators or on-the-fly model interpretation. For more information Note For more information please take a look at the tests . Built-in objects \u00b6 Often, you will need objects that should be a part of each model and you do not want users to specify them in every model they create. The most notable example are primitive types (e.g. integer , string , bool ). Let's provide integer and string Entities to our Entity meta-model in order to simplify the model creation so that the users can use the names of these two entities as the Attribute types. class Entity(object): def __init__(self, parent, name, attributes): self.parent = parent self.name = name self.attributes = attributes entity_builtins = { 'integer': Entity(None, 'integer', []), 'string': Entity(None, 'string', []) } entity_mm = metamodel_from_file( 'entity.tx', classes=[Entity] # Register Entity user class, builtins=entity_builtins # Register integer and string built-in objs ) Now an integer and string Attribute types can be used. See model and Entitiy example for more. Creating your own base type \u00b6 Match rules by default return Python string type. Built-in match rules (i.e. BASETYPEs ) on the other hand return Python base types. You can use object processors to create your type by specifying match rule in the grammar and object processor for that rule that will create an object of a proper Python type. Example: Model: 'begin' some_number=MyFloat 'end' ; MyFloat: /\\d+\\.(\\d+)?/ ; In this example MyFloat rule is a match rule and by default will return Python string , so attribute some_number will be of string type. To change that, register object processor for MyFloat rule: mm = metamodel_from_str(grammar) mm.register_obj_processors({'MyFloat': lambda x: float(x)})) Now, MyFloat will always be converted to Python float type. Using match filters you can override built-in rule's conversions like this: Model: some_float=INT ; mm = metamodel_from_str(grammar) mm.register_obj_processors({'INT': lambda x: float(x)})) In this example we use built-in rule INT that returns Python int type. Registering object processor with the key INT we can change default behaviour and convert what is matched by this rule to some other object ( float in this case). Auto-initialization of the attributes \u00b6 Each object that is recognized in the input string will be instantiated and its attributes will be set to the values parsed from the input. In the event that a defined attribute is optional, it will nevertheless be created on the instance and set to the default value. Here is a list of the default values for each base textX type: ID - empty string - '' INT - int - 0 FLOAT - float - 0.0 BOOL - bool - False STRING - empty string - '' Each attribute with zero or more multiplicity ( *= ) that does not match any object from the input will be initialized to an empty list. An attribute declared with one or more multiplicity ( += ) must match at least one object from the input and will therefore be transformed to python list containing all matched objects. The drawback of this auto-initialization system is that we can't be sure if the attribute was missing from the input or was matched, but the given value was the same as the default value. In some applications it is important to distinguish between those two situations. For that purpose, there is a parameter auto_init_attributes of the meta-model constructor that is True by default, but can be set to False to prevent auto-initialization from taking place. If auto-initialization is disabled, then each optional attribute that was not matched on the input will be set to None . This is true for the plain assignments ( = ). An optional assignment ( ?= ) will always be False if the RHS object is not matched in the input. The multiplicity assignments ( *= and += ) will always be python lists.","title":"Meta-model"},{"location":"metamodel/#textx-meta-models","text":"textX meta-model is a Python object that knows about all classes that can be instantiated while parsing the input. A meta-model is built from the grammar by the functions metamodel_from_file or metamodel_from_str in the textx.metamodel module. from textx import metamodel_from_file my_metamodel = metamodel_from_file('my_grammar.tx') Each rule from the grammar will result in a Python class kept in the meta-model. Besides, meta-model knows how to parse the input strings and convert them to model . Parsing the input and creating the model is done by model_from_file and model_from_str methods of the meta-model object: my_model = my_metamodel.model_from_file('some_input.md') When parsing a model file or string a new parser is cloned for each model. This parser can be accessed via the model attribute _tx_parser .","title":"textX meta-models"},{"location":"metamodel/#custom-classes","text":"For each grammar rule a Python class with the same name is created dynamically. These classes are instantiated during the parsing of the input string/file to create a graph of python objects, a.k.a. model or Abstract-Syntax Tree (AST). Most of the time dynamically created classes will be sufficient, but sometimes you will want to use your own classes instead. To do so use parameter classes during the meta-model instantiation. This parameter is a list of your classes that should be named the same as the rules from the grammar which they represent. from textx import metamodel_from_str grammar = ''' EntityModel: entities+=Entity // each model has one or more entities ; Entity: 'entity' name=ID '{' attributes+=Attribute // each entity has one or more attributes '}' ; Attribute: name=ID ':' type=[Entity] // type is a reference to an entity. There are // built-in entities registered on the meta-model // for primitive types (integer, string) ; ''' class Entity(object): def __init__(self, parent, name, attributes): self.parent = parent self.name = name self.attributes = attributes # Use our Entity class. \"Attribute\" class will be created dynamically. entity_mm = metamodel_from_str(grammar, classes=[Entity]) Now entity_mm can be used to parse the input models where our Entity class will be instantiated to represent each Entity rule from the grammar. Note Constructor of the user-defined classes should accept all attributes defined by the corresponding rule from the grammar. In the previous example, we have provided name and attributes attributes from the Entity rule. If the class is a child in a parent-child relationship (see the next section), then the parent constructor parameter should also be given.","title":"Custom classes"},{"location":"metamodel/#parent-child-relationships","text":"There is often an intrinsic parent-child relationship between object in the model. In the previous example, each Attribute instance will always be a child of some Entity object. textX gives automatic support for these relationships by providing the parent attribute on each child object. When you navigate model each child instance will have a parent attribute. Note Always provide the parent parameter in user-defined classes for each class that is a child in a parent-child relationship.","title":"Parent-child relationships"},{"location":"metamodel/#processors","text":"To specify static semantics of the language textX uses a concept called processor . Processors are python callables that can modify the model elements during model parsing/instantiation or do some additional checks that are not possible to do by the grammar alone. There are two types of processors: model processors - are callables that are called at the end of the parsing when the whole model is instantiated. These processors accept the model and meta-model as parameters. object processors - are registered for particular classes (grammar rules) and are called when the objects of the given class is instantiated. Processors can modify model/objects or raise exception ( TextXSemanticError ) if some constraint is not met. User code that calls the model instantiation/parsing can catch and handle this exception.","title":"Processors"},{"location":"metamodel/#model-processors","text":"To register a model processor call register_model_processor on the meta-model instance. from textx import metamodel_from_file # Model processor is a callable that will accept meta-model and model as its # parameters. def check_some_semantics(model, metamodel): ... ... Do some check on the model and raise TextXSemanticError if the semantics ... rules are violated. my_metamodel = metamodel_from_file('mygrammar.tx') # Register model processor on the meta-model instance my_metamodel.register_model_processor(check_some_semantics) # Parse the model. check_some_semantics will be called automatically after # a successful parse to do further checks. If the rules are not met, # an instance of TextXSemanticError will be raised. my_metamodel.model_from_file('some_model.ext')","title":"Model processors"},{"location":"metamodel/#object-processors","text":"The purpose of the object processors is to validate or alter the object being constructed. They are registered per class/rule. Let's do some additional checks for the above Entity DSL example. def entity_obj_processor(entity): ''' Check that Ethe ntity names are capitalized. This could also be specified in the grammar using regex match but we will do that check here just as an example. ''' if entity.name != entity.name.capitalize(): raise TextXSemanticError('Entity name \"%s\" must be capitalized.' % entity.name) def attribute_obj_processor(attribute): ''' Obj. processors can also introduce changes in the objects they process. Here we set \"primitive\" attribute based on the Entity they refer to. ''' attribute.primitive = attribute.type.name in ['integer', 'string'] # Object processors are registered by defining a map between a rule name # and the callable that will process the instances of that rule/class. obj_processors = { 'Entity': entity_obj_processor, 'Attribute': attribute_obj_processor, } # This map/dict is registered on a meta-model by the \"register_obj_processors\" # call. entity_mm.register_obj_processors(obj_processors) # Parse the model. At each successful parse of Entity or Attribute, the registered # processor will be called and the semantics error will be raised if the # check does not pass. entity_mm.model_from_file('my_entity_model.ent') For another example of the usage of an object processor that modifies the objects, see object processor move_command_processor robot example . If object processor returns a value that value will be used instead of the original object. This can be used to implement e.g. expression evaluators or on-the-fly model interpretation. For more information Note For more information please take a look at the tests .","title":"Object processors"},{"location":"metamodel/#built-in-objects","text":"Often, you will need objects that should be a part of each model and you do not want users to specify them in every model they create. The most notable example are primitive types (e.g. integer , string , bool ). Let's provide integer and string Entities to our Entity meta-model in order to simplify the model creation so that the users can use the names of these two entities as the Attribute types. class Entity(object): def __init__(self, parent, name, attributes): self.parent = parent self.name = name self.attributes = attributes entity_builtins = { 'integer': Entity(None, 'integer', []), 'string': Entity(None, 'string', []) } entity_mm = metamodel_from_file( 'entity.tx', classes=[Entity] # Register Entity user class, builtins=entity_builtins # Register integer and string built-in objs ) Now an integer and string Attribute types can be used. See model and Entitiy example for more.","title":"Built-in objects"},{"location":"metamodel/#creating-your-own-base-type","text":"Match rules by default return Python string type. Built-in match rules (i.e. BASETYPEs ) on the other hand return Python base types. You can use object processors to create your type by specifying match rule in the grammar and object processor for that rule that will create an object of a proper Python type. Example: Model: 'begin' some_number=MyFloat 'end' ; MyFloat: /\\d+\\.(\\d+)?/ ; In this example MyFloat rule is a match rule and by default will return Python string , so attribute some_number will be of string type. To change that, register object processor for MyFloat rule: mm = metamodel_from_str(grammar) mm.register_obj_processors({'MyFloat': lambda x: float(x)})) Now, MyFloat will always be converted to Python float type. Using match filters you can override built-in rule's conversions like this: Model: some_float=INT ; mm = metamodel_from_str(grammar) mm.register_obj_processors({'INT': lambda x: float(x)})) In this example we use built-in rule INT that returns Python int type. Registering object processor with the key INT we can change default behaviour and convert what is matched by this rule to some other object ( float in this case).","title":"Creating your own base type"},{"location":"metamodel/#auto-initialization-of-the-attributes","text":"Each object that is recognized in the input string will be instantiated and its attributes will be set to the values parsed from the input. In the event that a defined attribute is optional, it will nevertheless be created on the instance and set to the default value. Here is a list of the default values for each base textX type: ID - empty string - '' INT - int - 0 FLOAT - float - 0.0 BOOL - bool - False STRING - empty string - '' Each attribute with zero or more multiplicity ( *= ) that does not match any object from the input will be initialized to an empty list. An attribute declared with one or more multiplicity ( += ) must match at least one object from the input and will therefore be transformed to python list containing all matched objects. The drawback of this auto-initialization system is that we can't be sure if the attribute was missing from the input or was matched, but the given value was the same as the default value. In some applications it is important to distinguish between those two situations. For that purpose, there is a parameter auto_init_attributes of the meta-model constructor that is True by default, but can be set to False to prevent auto-initialization from taking place. If auto-initialization is disabled, then each optional attribute that was not matched on the input will be set to None . This is true for the plain assignments ( = ). An optional assignment ( ?= ) will always be False if the RHS object is not matched in the input. The multiplicity assignments ( *= and += ) will always be python lists.","title":"Auto-initialization of the attributes"},{"location":"model/","text":"textX models \u00b6 Model is a python object graph consisting of POPOs (Plain Old Python Objects) constructed from the input string that conforms to your DSL defined by the grammar and additional model and object processors . In a sense, this structure is an Abstract Syntax Tree (AST) known from classic parsing theory, but it is actually a graph structure where each reference is resolved to a proper python reference. Each object is an instance of a class from the meta-model. Classes are created on-the-fly from the grammar rules or are supplied by the user . A model is created from the input string using the model_from_file and model_from_str methods of the meta-model instance. from textx import metamodel_from_file my_mm = metamodel_from_file('mygrammar.tx') # Create model my_model = my_mm.model_from_file('some_model.ext') Let's take the Entity language used in Custom Classes section. Content of entity.tx file: EntityModel: entities+=Entity // each model has one or more entities ; Entity: 'entity' name=ID '{' attributes+=Attribute // each entity has one or more attributes '}' ; Attribute: name=ID ':' type=[Entity] // type is a reference to an entity. There are // built-in entities registered on the meta-model // for the primitive types (integer, string) ; For the meta-model construction and built-in registration see Custom Classes and Builtins sections. Now, we can use the entity_mm meta-model to parse and create Entity models. person_model = entity_mm.model_from_file('person.ent') Where person.ent file might contain this: entity Person { name : string address: Address age: integer } entity Address { street : string city : string country : string } Model API \u00b6 Functions given in this section can be imported from textx.model module. get_model(obj) \u00b6 obj (model object) Finds the root of the model following parent references. get_metamodel(obj) \u00b6 Returns metamodel for the model given obj belongs to. get_parent_of_type(typ, obj) \u00b6 typ (str or class) : the name of type of the type itself of the model object searched for. obj (model object) : model object to start search from. Finds first object up the parent chain of the given type. If no parent of the given type exists None is returned. get_children_of_type(typ, root) \u00b6 typ (str or python class) : The type of the model object we are looking for. root (model object) : Python model object which is the start of the search process. Returns a list of all model elements of type typ starting from model element root . The search process will follow containment links only. Non-containing references shall not be followed. Special model object's attributes \u00b6 Beside attributes specified by the grammar, there are several special attributes on model objects created by textX. All special attributes' names start with prefix _tx . These special attributes don't exist if the type of the resulting model object don't allow dynamic attribute creation (e.g. for Python base builtin types - str, int). _tx_position and _tx_position_end \u00b6 _tx_position attribute holds the position in the input string where the object has been matched by the parser. Each object from the model object graph has this attribute. This is an absolute position in the input stream. To convert it to line/column format use pos_to_linecol method of the parser. line, col = entity_model._tx_parser.pos_to_linecol( person_model.entities[0]._tx_position) Where entity_model is a model constructed by textX. Previous example will give the line/column position of the first entity. _tx_position_end is the position in the input stream where the object ends. This position is one char past the last char belonging to the object. Thus, _tx_position_end - _tx_position == length of the object str representation . _tx_filename \u00b6 This attribute exists only on the root of the model. If the model is loaded from a file, this attribute will be the full path of the source file. If the model is created from a string this attribute will be None . _tx_parser \u00b6 This attribute represents the concrete parser instance used for the model (the attribute _parser of the _tx_metamodel is only a blueprint for the parser of each model instance and cannot be used, e.g., to determine model element positions in a file. Use the _tx_parser attribute of the model instead). _tx_metamodel \u00b6 This attribute exists only on the root of the model. It is a reference to the meta-model object used for creating the model. _tx_fqn \u00b6 Is the fully qualified name of the grammar rule/Python class in regard to the import path of the grammar file where the rule is defined. This attribute is used in __repr__ of auto-generated Python classes. _tx_model_repository \u00b6 The model may have a model repository (initiated by some scope provider or by the metamodel). This object is responsible to provide and cache other model instances (see textx.scoping.providers).","title":"Model"},{"location":"model/#textx-models","text":"Model is a python object graph consisting of POPOs (Plain Old Python Objects) constructed from the input string that conforms to your DSL defined by the grammar and additional model and object processors . In a sense, this structure is an Abstract Syntax Tree (AST) known from classic parsing theory, but it is actually a graph structure where each reference is resolved to a proper python reference. Each object is an instance of a class from the meta-model. Classes are created on-the-fly from the grammar rules or are supplied by the user . A model is created from the input string using the model_from_file and model_from_str methods of the meta-model instance. from textx import metamodel_from_file my_mm = metamodel_from_file('mygrammar.tx') # Create model my_model = my_mm.model_from_file('some_model.ext') Let's take the Entity language used in Custom Classes section. Content of entity.tx file: EntityModel: entities+=Entity // each model has one or more entities ; Entity: 'entity' name=ID '{' attributes+=Attribute // each entity has one or more attributes '}' ; Attribute: name=ID ':' type=[Entity] // type is a reference to an entity. There are // built-in entities registered on the meta-model // for the primitive types (integer, string) ; For the meta-model construction and built-in registration see Custom Classes and Builtins sections. Now, we can use the entity_mm meta-model to parse and create Entity models. person_model = entity_mm.model_from_file('person.ent') Where person.ent file might contain this: entity Person { name : string address: Address age: integer } entity Address { street : string city : string country : string }","title":"textX models"},{"location":"model/#model-api","text":"Functions given in this section can be imported from textx.model module.","title":"Model API"},{"location":"model/#get_modelobj","text":"obj (model object) Finds the root of the model following parent references.","title":"get_model(obj)"},{"location":"model/#get_metamodelobj","text":"Returns metamodel for the model given obj belongs to.","title":"get_metamodel(obj)"},{"location":"model/#get_parent_of_typetyp-obj","text":"typ (str or class) : the name of type of the type itself of the model object searched for. obj (model object) : model object to start search from. Finds first object up the parent chain of the given type. If no parent of the given type exists None is returned.","title":"get_parent_of_type(typ, obj)"},{"location":"model/#get_children_of_typetyp-root","text":"typ (str or python class) : The type of the model object we are looking for. root (model object) : Python model object which is the start of the search process. Returns a list of all model elements of type typ starting from model element root . The search process will follow containment links only. Non-containing references shall not be followed.","title":"get_children_of_type(typ, root)"},{"location":"model/#special-model-objects-attributes","text":"Beside attributes specified by the grammar, there are several special attributes on model objects created by textX. All special attributes' names start with prefix _tx . These special attributes don't exist if the type of the resulting model object don't allow dynamic attribute creation (e.g. for Python base builtin types - str, int).","title":"Special model object's attributes"},{"location":"model/#_tx_position-and-_tx_position_end","text":"_tx_position attribute holds the position in the input string where the object has been matched by the parser. Each object from the model object graph has this attribute. This is an absolute position in the input stream. To convert it to line/column format use pos_to_linecol method of the parser. line, col = entity_model._tx_parser.pos_to_linecol( person_model.entities[0]._tx_position) Where entity_model is a model constructed by textX. Previous example will give the line/column position of the first entity. _tx_position_end is the position in the input stream where the object ends. This position is one char past the last char belonging to the object. Thus, _tx_position_end - _tx_position == length of the object str representation .","title":"_tx_position and _tx_position_end"},{"location":"model/#_tx_filename","text":"This attribute exists only on the root of the model. If the model is loaded from a file, this attribute will be the full path of the source file. If the model is created from a string this attribute will be None .","title":"_tx_filename"},{"location":"model/#_tx_parser","text":"This attribute represents the concrete parser instance used for the model (the attribute _parser of the _tx_metamodel is only a blueprint for the parser of each model instance and cannot be used, e.g., to determine model element positions in a file. Use the _tx_parser attribute of the model instead).","title":"_tx_parser"},{"location":"model/#_tx_metamodel","text":"This attribute exists only on the root of the model. It is a reference to the meta-model object used for creating the model.","title":"_tx_metamodel"},{"location":"model/#_tx_fqn","text":"Is the fully qualified name of the grammar rule/Python class in regard to the import path of the grammar file where the rule is defined. This attribute is used in __repr__ of auto-generated Python classes.","title":"_tx_fqn"},{"location":"model/#_tx_model_repository","text":"The model may have a model repository (initiated by some scope provider or by the metamodel). This object is responsible to provide and cache other model instances (see textx.scoping.providers).","title":"_tx_model_repository"},{"location":"multimetamodel/","text":"Multi meta-model support \u00b6 As a feature of some scope providers , there is the possibility to define a file extension based allocation of model files to meta models. This global data is stored in the class textx.scoping.MetaModelProvider : Here, you can register meta models associated to files patterns. Thus, you can control which meta model to use when loading a file in a scope provider using the ImportURI -feature (e.g., FQNImportURI ) or with global scope providers (e.g., PlainNameGlobalRepo ). If no file pattern matches, the meta model of the current model is utilized. There are different ways to combine meta models: (1) a meta model can use another meta model to compose its own structures (extending a meta model) or (2) a meta model can reference elements from another meta model. Extending an existing meta model can be realized in TextX by defining a grammar extending an existing grammar. All user classes, scope providers and processors must be manually added to the new meta model. Such extended meta models can also reference elements of models created with the original meta model. Although the meta classes corresponding to inherited rules are redefined by the extending meta model, scope providers match the object types correctly. This is implemented by comparing the types by their name (see textx.scoping.tool.textx_isinstance). Simple examples: see tests/test_scoping/test_metamodel_provider*.py . Referencing elements from another meta model can be achieved without having the original grammar, nor any other details like scope providers, etc. Such references can, thus, be enabled while having access only to the meta model object of the meta model to be referenced. This object may originate from a library installed on the system (without sources, like the grammar). The meta model to be referenced is passed to the referencing meta model while constructing it. The referencing grammar can then reference the types (rules) of the referenced meta model. Rule lookup takes care of choosing the correct types. Simple examples: see tests/test_metamodel/test_multi_metamodel_refs.py . Thus, when designing a domain model (e.g., from the software test domain) to reference elements of another domain model (e.g., from the interface/communication domain), the second possibility (referencing) is probably a cleaner way to achieve the task than the first possibility (extending). Use Case: meta model referencing another meta model \u00b6 We have two grammars (grammar A nd B). \"grammarA\" defines named elements of type A: Model: a+=A; A:'A' name=ID; \"GrammarBWithImportURI\" defines named elements of type B referencing elements of type A (from \"grammarA\"). It also allows importing other model files. Model: imports+=Import b+=B; B:'B' name=ID '->' a=[A]; Import: 'import' importURI=STRING; Here, we create two meta models, where the second meta model allows referencing the first one ( referenced_metamodels=[mm_A] ). mm_A = metamodel_from_str(grammarA) mm_B = metamodel_from_str(grammarBWithImport, referenced_metamodels=[mm_A]) Then we define a default scope provider supporting the importURI-feature: mm_B.register_scope_providers({\"*.*\": scoping_providers.FQNImportURI()}) and we map file endings to the meta models: scoping.MetaModelProvider.add_metamodel(\"*.a\", mm_A) scoping.MetaModelProvider.add_metamodel(\"*.b\", mm_B) Full example (also with a globally shared repository discussion): see tests/test_metamodel/test_multi_metamodel_refs.py . Use Case: Recipes and Ingredients with global model sharing \u00b6 In this use case we define recipes (food preparation) including a list of ingredients. The ingredients of a recipe model element are defined by a count (e.g. 100), a unit (e.g. gram), and an ingredient reference (e.g. sugar). In a separate model the ingredients are defined: Here we can define multiple units to be used for each ingerdient (e.g. 60 gram of sugar or 1 cup of sugar ). Moreover some ingredients may inherit features of other ingredients (e.g. salt may have the same units as sugar). Here, two meta models are defined to handle ingredient definitions (e.g. fruits.ingredient ) and recipes (e.g. fruit_salad.recipe ). The MetaModelProvider is utilized to allocate the file extensions to the meta models (see test_metamodel_provider2.py ). Importantly, a common model repository ( global_repo ) is defined to share all model elements among both meta models: i_mm = get_meta_model( global_repo, this_folder + \"/metamodel_provider2/Ingredient.tx\") r_mm = get_meta_model( global_repo, this_folder + \"/metamodel_provider2/Recipe.tx\") scoping.MetaModelProvider.add_metamodel(\"*.recipe\", r_mm) scoping.MetaModelProvider.add_metamodel(\"*.ingredient\", i_mm) Use Case: meta model sharing with the ImportURI-feature \u00b6 In this use case we have a given meta model to define components and instances of components. A second model is added to define users to use instances of such components defned in the first model. The grammar for the user meta-model is given as follows (including the ability to import a component model file). import Components Model: imports+=Import users+=User ; User: \"user\" name=ID \"uses\" instance=[Instance|FQN] // Instance, FQN from other grammar ; Import: 'import' importURI=STRING; The global MetaModelProvider class is utilized to allocate the file extension to the corresponding meta model: scoping.MetaModelProvider.add_metamodel(\"*.components\", mm_components) scoping.MetaModelProvider.add_metamodel(\"*.users\", mm_users) With this construct we can define a user model referencing a component model: import \"example.components\" user pi uses usage.action1 Use Case: referencing non-textx meta-models/models \u00b6 You can reference an arbitrary python object using the OBJECT rule (see: test_reference_to_buildin_attribute.py ) Access: 'access' name=ID pyobj=[OBJECT] ('.' pyattr=[OBJECT])? In this case the referenced object is a python dictionary ( pyobj ) and the entry of such a dictionary ( pyattr ). An example model will look like: access AccessName1 foreign_model.name_of_entry A custom scope provider is used to achieve this mapping: def my_scope_provider(obj, attr, attr_ref): pyobj = obj.pyobj if attr_ref.obj_name in pyobj: return pyobj[attr_ref.obj_name] else: raise Exception(\"{} not found\".format(attr_ref.obj_name)) The scope provider is linked to the pyattr attribute of the rule Access : my_metamodel.register_scope_providers({ \"Access.pyattr\": my_scope_provider, }) With this, we can reference non-textx data elements from within our language. This can be used to, e.g., use a non-textx AST object and reference it from a textx model. Referencing non-textx meta-models/models with a json file \u00b6 In test_reference_to_nontextx_attribute.py we also demonstrate how such an external model can be loaded with our own language (using a json file as external model). import \"test_reference_to_nontextx_attribute/othermodel.json\" as data access A1 data.name access A2 data.gender Where the json file othermodel.json consists of: { \"name\": \"pierre\", \"gender\": \"male\" } Conclusion \u00b6 We provide a pragmatic way to define meta-models using other meta models. Mostly, we focus on textx meta-models using other textx meta-models. But scope providers may be used to also link a textx meta model to an arbitrary non-textx data structure.","title":"Multi meta-model"},{"location":"multimetamodel/#multi-meta-model-support","text":"As a feature of some scope providers , there is the possibility to define a file extension based allocation of model files to meta models. This global data is stored in the class textx.scoping.MetaModelProvider : Here, you can register meta models associated to files patterns. Thus, you can control which meta model to use when loading a file in a scope provider using the ImportURI -feature (e.g., FQNImportURI ) or with global scope providers (e.g., PlainNameGlobalRepo ). If no file pattern matches, the meta model of the current model is utilized. There are different ways to combine meta models: (1) a meta model can use another meta model to compose its own structures (extending a meta model) or (2) a meta model can reference elements from another meta model. Extending an existing meta model can be realized in TextX by defining a grammar extending an existing grammar. All user classes, scope providers and processors must be manually added to the new meta model. Such extended meta models can also reference elements of models created with the original meta model. Although the meta classes corresponding to inherited rules are redefined by the extending meta model, scope providers match the object types correctly. This is implemented by comparing the types by their name (see textx.scoping.tool.textx_isinstance). Simple examples: see tests/test_scoping/test_metamodel_provider*.py . Referencing elements from another meta model can be achieved without having the original grammar, nor any other details like scope providers, etc. Such references can, thus, be enabled while having access only to the meta model object of the meta model to be referenced. This object may originate from a library installed on the system (without sources, like the grammar). The meta model to be referenced is passed to the referencing meta model while constructing it. The referencing grammar can then reference the types (rules) of the referenced meta model. Rule lookup takes care of choosing the correct types. Simple examples: see tests/test_metamodel/test_multi_metamodel_refs.py . Thus, when designing a domain model (e.g., from the software test domain) to reference elements of another domain model (e.g., from the interface/communication domain), the second possibility (referencing) is probably a cleaner way to achieve the task than the first possibility (extending).","title":"Multi meta-model support"},{"location":"multimetamodel/#use-case-meta-model-referencing-another-meta-model","text":"We have two grammars (grammar A nd B). \"grammarA\" defines named elements of type A: Model: a+=A; A:'A' name=ID; \"GrammarBWithImportURI\" defines named elements of type B referencing elements of type A (from \"grammarA\"). It also allows importing other model files. Model: imports+=Import b+=B; B:'B' name=ID '->' a=[A]; Import: 'import' importURI=STRING; Here, we create two meta models, where the second meta model allows referencing the first one ( referenced_metamodels=[mm_A] ). mm_A = metamodel_from_str(grammarA) mm_B = metamodel_from_str(grammarBWithImport, referenced_metamodels=[mm_A]) Then we define a default scope provider supporting the importURI-feature: mm_B.register_scope_providers({\"*.*\": scoping_providers.FQNImportURI()}) and we map file endings to the meta models: scoping.MetaModelProvider.add_metamodel(\"*.a\", mm_A) scoping.MetaModelProvider.add_metamodel(\"*.b\", mm_B) Full example (also with a globally shared repository discussion): see tests/test_metamodel/test_multi_metamodel_refs.py .","title":"Use Case: meta model referencing another meta model"},{"location":"multimetamodel/#use-case-recipes-and-ingredients-with-global-model-sharing","text":"In this use case we define recipes (food preparation) including a list of ingredients. The ingredients of a recipe model element are defined by a count (e.g. 100), a unit (e.g. gram), and an ingredient reference (e.g. sugar). In a separate model the ingredients are defined: Here we can define multiple units to be used for each ingerdient (e.g. 60 gram of sugar or 1 cup of sugar ). Moreover some ingredients may inherit features of other ingredients (e.g. salt may have the same units as sugar). Here, two meta models are defined to handle ingredient definitions (e.g. fruits.ingredient ) and recipes (e.g. fruit_salad.recipe ). The MetaModelProvider is utilized to allocate the file extensions to the meta models (see test_metamodel_provider2.py ). Importantly, a common model repository ( global_repo ) is defined to share all model elements among both meta models: i_mm = get_meta_model( global_repo, this_folder + \"/metamodel_provider2/Ingredient.tx\") r_mm = get_meta_model( global_repo, this_folder + \"/metamodel_provider2/Recipe.tx\") scoping.MetaModelProvider.add_metamodel(\"*.recipe\", r_mm) scoping.MetaModelProvider.add_metamodel(\"*.ingredient\", i_mm)","title":"Use Case: Recipes and Ingredients with global model sharing"},{"location":"multimetamodel/#use-case-meta-model-sharing-with-the-importuri-feature","text":"In this use case we have a given meta model to define components and instances of components. A second model is added to define users to use instances of such components defned in the first model. The grammar for the user meta-model is given as follows (including the ability to import a component model file). import Components Model: imports+=Import users+=User ; User: \"user\" name=ID \"uses\" instance=[Instance|FQN] // Instance, FQN from other grammar ; Import: 'import' importURI=STRING; The global MetaModelProvider class is utilized to allocate the file extension to the corresponding meta model: scoping.MetaModelProvider.add_metamodel(\"*.components\", mm_components) scoping.MetaModelProvider.add_metamodel(\"*.users\", mm_users) With this construct we can define a user model referencing a component model: import \"example.components\" user pi uses usage.action1","title":"Use Case: meta model sharing with the ImportURI-feature"},{"location":"multimetamodel/#use-case-referencing-non-textx-meta-modelsmodels","text":"You can reference an arbitrary python object using the OBJECT rule (see: test_reference_to_buildin_attribute.py ) Access: 'access' name=ID pyobj=[OBJECT] ('.' pyattr=[OBJECT])? In this case the referenced object is a python dictionary ( pyobj ) and the entry of such a dictionary ( pyattr ). An example model will look like: access AccessName1 foreign_model.name_of_entry A custom scope provider is used to achieve this mapping: def my_scope_provider(obj, attr, attr_ref): pyobj = obj.pyobj if attr_ref.obj_name in pyobj: return pyobj[attr_ref.obj_name] else: raise Exception(\"{} not found\".format(attr_ref.obj_name)) The scope provider is linked to the pyattr attribute of the rule Access : my_metamodel.register_scope_providers({ \"Access.pyattr\": my_scope_provider, }) With this, we can reference non-textx data elements from within our language. This can be used to, e.g., use a non-textx AST object and reference it from a textx model.","title":"Use Case: referencing non-textx meta-models/models"},{"location":"multimetamodel/#referencing-non-textx-meta-modelsmodels-with-a-json-file","text":"In test_reference_to_nontextx_attribute.py we also demonstrate how such an external model can be loaded with our own language (using a json file as external model). import \"test_reference_to_nontextx_attribute/othermodel.json\" as data access A1 data.name access A2 data.gender Where the json file othermodel.json consists of: { \"name\": \"pierre\", \"gender\": \"male\" }","title":"Referencing non-textx meta-models/models with a json file"},{"location":"multimetamodel/#conclusion","text":"We provide a pragmatic way to define meta-models using other meta models. Mostly, we focus on textx meta-models using other textx meta-models. But scope providers may be used to also link a textx meta model to an arbitrary non-textx data structure.","title":"Conclusion"},{"location":"parser_config/","text":"Parser configuration \u00b6 Case sensitivity \u00b6 Parser is by default case sensitive. For DSLs that should be case insensitive use ignore_case parameter of the meta-model constructor call. from textx import metamodel_from_file my_metamodel = metamodel_from_file('mygrammar.tx', ignore_case=True) Whitespace handling \u00b6 The parser will skip whitespaces by default. Whitespaces are spaces, tabs and newlines by default. Skipping of the whitespaces can be disabled by skipws bool parameter in the constructor call. Also, what is a whitespace can be redefined by the ws string parameter. from textx import metamodel_from_file my_metamodel = metamodel_from_file('mygrammar.tx', skipws=False, ws='\\s\\n') Whitespaces and whitespace skipping can be defined in the grammar on the level of a single rule by rule modifiers . Automatic keywords \u00b6 When designing a DSL, it is usually desirable to match keywords on word boundaries. For example, if we have Entity grammar from the above, then a word entity will be considered a keyword and should be matched on word boundaries only. If we have a word entity2 in the input string at the place where entity should be matched, the match should not succeed. We could achieve this by using a regular expression match and the word boundaries regular expression rule for each keyword-like match. Enitity: /\\bentity\\b/ name=ID ... But the grammar will be cumbersome to read. textX can do automatic word boundary match for all keyword-like string matches. To enable this feature set parameter autokwd to True in the constructor call. from textx import metamodel_from_file my_metamodel = metamodel_from_file('mygrammar.tx', autokwd=True) Any simple match from the grammar that is matched by the regular expression [^\\d\\W]\\w* is considered to be a keyword. Memoization (a.k.a. packrat parsing) \u00b6 This technique is based on memoizing result on each parsing expression rule. For some grammars with a lot of backtracking this can yield a significant speed increase at the expense of some memory used for the memoization cache. Starting with textX 1.4 this feature is disabled by default. If you think that parsing is slow, try to enable memoization by setting memoization parameter to True during meta-model instantiation. from textx import metamodel_from_file my_metamodel = metamodel_from_file('mygrammar.tx', memoization=True)","title":"Parser configuration"},{"location":"parser_config/#parser-configuration","text":"","title":"Parser configuration"},{"location":"parser_config/#case-sensitivity","text":"Parser is by default case sensitive. For DSLs that should be case insensitive use ignore_case parameter of the meta-model constructor call. from textx import metamodel_from_file my_metamodel = metamodel_from_file('mygrammar.tx', ignore_case=True)","title":"Case sensitivity"},{"location":"parser_config/#whitespace-handling","text":"The parser will skip whitespaces by default. Whitespaces are spaces, tabs and newlines by default. Skipping of the whitespaces can be disabled by skipws bool parameter in the constructor call. Also, what is a whitespace can be redefined by the ws string parameter. from textx import metamodel_from_file my_metamodel = metamodel_from_file('mygrammar.tx', skipws=False, ws='\\s\\n') Whitespaces and whitespace skipping can be defined in the grammar on the level of a single rule by rule modifiers .","title":"Whitespace handling"},{"location":"parser_config/#automatic-keywords","text":"When designing a DSL, it is usually desirable to match keywords on word boundaries. For example, if we have Entity grammar from the above, then a word entity will be considered a keyword and should be matched on word boundaries only. If we have a word entity2 in the input string at the place where entity should be matched, the match should not succeed. We could achieve this by using a regular expression match and the word boundaries regular expression rule for each keyword-like match. Enitity: /\\bentity\\b/ name=ID ... But the grammar will be cumbersome to read. textX can do automatic word boundary match for all keyword-like string matches. To enable this feature set parameter autokwd to True in the constructor call. from textx import metamodel_from_file my_metamodel = metamodel_from_file('mygrammar.tx', autokwd=True) Any simple match from the grammar that is matched by the regular expression [^\\d\\W]\\w* is considered to be a keyword.","title":"Automatic keywords"},{"location":"parser_config/#memoization-aka-packrat-parsing","text":"This technique is based on memoizing result on each parsing expression rule. For some grammars with a lot of backtracking this can yield a significant speed increase at the expense of some memory used for the memoization cache. Starting with textX 1.4 this feature is disabled by default. If you think that parsing is slow, try to enable memoization by setting memoization parameter to True during meta-model instantiation. from textx import metamodel_from_file my_metamodel = metamodel_from_file('mygrammar.tx', memoization=True)","title":"Memoization (a.k.a. packrat parsing)"},{"location":"scoping/","text":"textX Scoping \u00b6 Motivation and Introduction to Scoping \u00b6 Assume a grammar with references as in the following example (grammar snippet). MyAttribute: ref=[MyInterface|FQN] name=ID ';' ; The scope provider is responsible for the reference resolution of such a reference. The default behavior (default scope provider) is looking for the referenced name globally. Other scope providers will take namespaces into account, support references to parts of the model stored in different files or even models defined by other metamodels (imported into the current metamodel). Moreover, scope providers exist allowing to reference model elements relative to other referenced model elements: This can be a referenced method defined in a referenced class of an instance (with a meta-model defining classes, methods and instances of classes). Usage \u00b6 The scope providers are registered to the metamodel and can be bound to specific parts of rules: e.g., my_meta_model.register_scope_providers({\"*.*\": scoping.providers.FQN()}) or: my_meta_model.register_scope_providers({\"MyAttribute.ref\": scoping.providers.FQN()}) or: my_meta_model.register_scope_providers({\"*.ref\": scoping.providers.FQN()}) or: my_meta_model.register_scope_providers({\"MyAttribute.*\": scoping.providers.FQN()}) Example (from tests/test_scoping/test_local_scope.py ): # Grammar snippet (Components.tx) Component: 'component' name=ID ('extends' extends+=[Component|FQN][','])? '{' slots*=Slot '}' ; Slot: SlotIn|SlotOut; # ... Instance: 'instance' name=ID ':' component=[Component|FQN] ; Connection: 'connect' from_inst=[Instance|ID] '.' from_port=[SlotOut|ID] 'to' to_inst=[Instance|ID] '.' to_port=[SlotIn|ID] ; # Python snippet my_meta_model = metamodel_from_file(abspath(dirname(__file__)) + '/components_model1/Components.tx') my_meta_model.register_scope_providers({ \"*.*\": scoping_providers.FQN(), \"Connection.from_port\": scoping_providers.RelativeName(\"from_inst.component.slots\"), \"Connection.to_port\": scoping_providers.RelativeName(\"to_inst.component.slots\"), }) This example selects the fully qualified name provider as default provider ( \"*.*\" ). Moreover, for special attributes of a Connection a relative name lookup is specified: Here the path from the rule Connection containing the attribute of interest (e.g. Connection.from_port ) to the referenced element is specified (the slot contained in from_inst.component.slots ). Since this attribute is a list, the list is searched to find the referenced name. Note Special rule selections (e.g., Connection.from_port ) are preferred to wildcard selection (e.e, \"*.*\" ). Scope Providers defined in Module \"textx.scoping.providers\" \u00b6 We provide some standard scope providers: textx.scoping.providers.PlainName : This is the default provider of textX. textx.scoping.providers.FQN : This is a provider similar to Java or Xtext name loopup . Example: see tests/test_scoping/test_full_qualified_name.py . textx.scoping.providers.ImportURI : This a provider which allows to load additional modules for lookup. You need to define a rule with an attribute importURI as string (like in Xtext). This string is then used to load other models. Moreover, you need to provide another scope provider to manage the concrete lookup, e.g., the scope_provider_plain_names or the scope_provider_fully_qualified_names . Model objects formed by the rules with an importURI attribute get an additional attribute _tx_loaded_models which is a list of the loaded models by this rule instance. Example: see tests/test_scoping/test_import_module.py . FQNImportURI (decorated scope provider) PlainNameImportURI (decorated scope provider) You can use globbing (import \"*.data\") with the ImportURI feature. This is implemented via the python \"glob\" module. Arguments can be passed to the glob.glob function (glob_args), e.g., to enable recursive globbing. Alternatively, you can also specify a list of search directories . In this case globbing is not allowed and is disabled (reason: it is unclear if the user wants to glob over all search path entries or to stop after the first match). Example: see tests/test_scoping/test_import_module_search_path_issue66.py . * textx.scoping.providers.GlobalRepo : This is a provider where you initially need to specifiy the model files to be loaded and used for lookup . Like for ImportURI you need to provide another scope provider for the concrete lookup. Example: see tests/test_scoping/test_global_import_modules.py . - textx.scoping.providers.FQNGlobalRepo (decorated scope provider) - textx.scoping.providers.PlainNameGlobalRepo (decorated scope provider) * textx.scoping.providers.RelativeName : This is a scope provider to resolve relative lookups : e.g., model-methods of a model-instance, defined by the class associated with the model-instance. Typically, another reference (the reference to the model-class of a model-instance) is used to determine the concrete referenced object (e.g. the model-method, owned by a model-class). Example: see tests/test_scoping/test_local_scope.py . * textx.scoping.providers.ExtRelativeName : The same as RelativeName allowing to model inheritance or chained lookups . Example: see tests/test_scoping/test_local_scope.py . Note on Uniqueness of Model Elements \u00b6 Two different models created using one single meta model (not using a scope provider like GlobalRepo , but by directly loading the models from file) have different instances of the same model elements. If you need two such models to share their model element instances, you can specify this, while creating the meta model ( global_repository=True ). Then, the meta model will store an own instance of a GlobalModelRepository as a base for all loaded models. Model elements in models including other parts of the model (possibly circular) have unique model elements (no double instances). Examples see tests/test_scoping/test_import_module.py . Technical aspects and implementation details \u00b6 The scope providers are python callables accepting obj, attr, obj_ref : obj : the object representing the start of the search (e.g., a rule (e.g. MyAttribute in the example above, or the model) attr : a reference to the attribute ref obj_ref : a textx.model.ObjCrossRef - the reference to be resolved The scope provider return the referenced object (e.g. a MyInterface object in the example illustrated in the Motivation and Introduction above (or None if nothing is found; or a Postponed object, see below). The scope provider is responsible to check the type and throw a TextXSemanticError if the type is not ok. Scope providers shall be stateless or have unmodifiable state after construction: this means they should allow to be reused for different models (created using the same meta-model) without interacting with each other. This means, they must save their state in the corresponding model, if they need to store data (e.g., if they load additional models from files during name resolution , they are not allowed to store them inside the scope provider. Models with references being resolved have a temporary attribute _tx_reference_resolver of type ReferenceResolver . This object can be used to resolve the object. It contains information, such as the parser in charge for the model (file) being processed. Note Scope providers as normal functions ( def <name>(...):... ), not accessing global data, are safe per se. The reason to be stateless, is that no side effects (beside, e.g., loading other models) should influence the name lookup. The state of model resolution should mainly consist of models already loaded. These models are stored in a GlobalModelRepository class. This class (if required) is stored in the model. An included model loaded from another including model \"inherits\" the part of the GlobalModelRepository representing all loaded models. This is done to (a) cache already loaded models and (b) guarantee, that every referenced model element is instantiated exactly once. Even in the case of circular inclusions. Scope providers may return an object of type Postponed , if they depend on another event to happen first. This event is typically the resolution of another reference. The resolution process will repeat multiple times over all unresolved references to be resolved until all references are resolved or no progress regarding the resolution is observed. In the latter case an error is raised. The control flow responsibility of the resolution process is allocated to the model.py module.","title":"Scoping"},{"location":"scoping/#textx-scoping","text":"","title":"textX Scoping"},{"location":"scoping/#motivation-and-introduction-to-scoping","text":"Assume a grammar with references as in the following example (grammar snippet). MyAttribute: ref=[MyInterface|FQN] name=ID ';' ; The scope provider is responsible for the reference resolution of such a reference. The default behavior (default scope provider) is looking for the referenced name globally. Other scope providers will take namespaces into account, support references to parts of the model stored in different files or even models defined by other metamodels (imported into the current metamodel). Moreover, scope providers exist allowing to reference model elements relative to other referenced model elements: This can be a referenced method defined in a referenced class of an instance (with a meta-model defining classes, methods and instances of classes).","title":"Motivation and Introduction to Scoping"},{"location":"scoping/#usage","text":"The scope providers are registered to the metamodel and can be bound to specific parts of rules: e.g., my_meta_model.register_scope_providers({\"*.*\": scoping.providers.FQN()}) or: my_meta_model.register_scope_providers({\"MyAttribute.ref\": scoping.providers.FQN()}) or: my_meta_model.register_scope_providers({\"*.ref\": scoping.providers.FQN()}) or: my_meta_model.register_scope_providers({\"MyAttribute.*\": scoping.providers.FQN()}) Example (from tests/test_scoping/test_local_scope.py ): # Grammar snippet (Components.tx) Component: 'component' name=ID ('extends' extends+=[Component|FQN][','])? '{' slots*=Slot '}' ; Slot: SlotIn|SlotOut; # ... Instance: 'instance' name=ID ':' component=[Component|FQN] ; Connection: 'connect' from_inst=[Instance|ID] '.' from_port=[SlotOut|ID] 'to' to_inst=[Instance|ID] '.' to_port=[SlotIn|ID] ; # Python snippet my_meta_model = metamodel_from_file(abspath(dirname(__file__)) + '/components_model1/Components.tx') my_meta_model.register_scope_providers({ \"*.*\": scoping_providers.FQN(), \"Connection.from_port\": scoping_providers.RelativeName(\"from_inst.component.slots\"), \"Connection.to_port\": scoping_providers.RelativeName(\"to_inst.component.slots\"), }) This example selects the fully qualified name provider as default provider ( \"*.*\" ). Moreover, for special attributes of a Connection a relative name lookup is specified: Here the path from the rule Connection containing the attribute of interest (e.g. Connection.from_port ) to the referenced element is specified (the slot contained in from_inst.component.slots ). Since this attribute is a list, the list is searched to find the referenced name. Note Special rule selections (e.g., Connection.from_port ) are preferred to wildcard selection (e.e, \"*.*\" ).","title":"Usage"},{"location":"scoping/#scope-providers-defined-in-module-textxscopingproviders","text":"We provide some standard scope providers: textx.scoping.providers.PlainName : This is the default provider of textX. textx.scoping.providers.FQN : This is a provider similar to Java or Xtext name loopup . Example: see tests/test_scoping/test_full_qualified_name.py . textx.scoping.providers.ImportURI : This a provider which allows to load additional modules for lookup. You need to define a rule with an attribute importURI as string (like in Xtext). This string is then used to load other models. Moreover, you need to provide another scope provider to manage the concrete lookup, e.g., the scope_provider_plain_names or the scope_provider_fully_qualified_names . Model objects formed by the rules with an importURI attribute get an additional attribute _tx_loaded_models which is a list of the loaded models by this rule instance. Example: see tests/test_scoping/test_import_module.py . FQNImportURI (decorated scope provider) PlainNameImportURI (decorated scope provider) You can use globbing (import \"*.data\") with the ImportURI feature. This is implemented via the python \"glob\" module. Arguments can be passed to the glob.glob function (glob_args), e.g., to enable recursive globbing. Alternatively, you can also specify a list of search directories . In this case globbing is not allowed and is disabled (reason: it is unclear if the user wants to glob over all search path entries or to stop after the first match). Example: see tests/test_scoping/test_import_module_search_path_issue66.py . * textx.scoping.providers.GlobalRepo : This is a provider where you initially need to specifiy the model files to be loaded and used for lookup . Like for ImportURI you need to provide another scope provider for the concrete lookup. Example: see tests/test_scoping/test_global_import_modules.py . - textx.scoping.providers.FQNGlobalRepo (decorated scope provider) - textx.scoping.providers.PlainNameGlobalRepo (decorated scope provider) * textx.scoping.providers.RelativeName : This is a scope provider to resolve relative lookups : e.g., model-methods of a model-instance, defined by the class associated with the model-instance. Typically, another reference (the reference to the model-class of a model-instance) is used to determine the concrete referenced object (e.g. the model-method, owned by a model-class). Example: see tests/test_scoping/test_local_scope.py . * textx.scoping.providers.ExtRelativeName : The same as RelativeName allowing to model inheritance or chained lookups . Example: see tests/test_scoping/test_local_scope.py .","title":"Scope Providers defined in Module \"textx.scoping.providers\""},{"location":"scoping/#note-on-uniqueness-of-model-elements","text":"Two different models created using one single meta model (not using a scope provider like GlobalRepo , but by directly loading the models from file) have different instances of the same model elements. If you need two such models to share their model element instances, you can specify this, while creating the meta model ( global_repository=True ). Then, the meta model will store an own instance of a GlobalModelRepository as a base for all loaded models. Model elements in models including other parts of the model (possibly circular) have unique model elements (no double instances). Examples see tests/test_scoping/test_import_module.py .","title":"Note on Uniqueness of Model Elements"},{"location":"scoping/#technical-aspects-and-implementation-details","text":"The scope providers are python callables accepting obj, attr, obj_ref : obj : the object representing the start of the search (e.g., a rule (e.g. MyAttribute in the example above, or the model) attr : a reference to the attribute ref obj_ref : a textx.model.ObjCrossRef - the reference to be resolved The scope provider return the referenced object (e.g. a MyInterface object in the example illustrated in the Motivation and Introduction above (or None if nothing is found; or a Postponed object, see below). The scope provider is responsible to check the type and throw a TextXSemanticError if the type is not ok. Scope providers shall be stateless or have unmodifiable state after construction: this means they should allow to be reused for different models (created using the same meta-model) without interacting with each other. This means, they must save their state in the corresponding model, if they need to store data (e.g., if they load additional models from files during name resolution , they are not allowed to store them inside the scope provider. Models with references being resolved have a temporary attribute _tx_reference_resolver of type ReferenceResolver . This object can be used to resolve the object. It contains information, such as the parser in charge for the model (file) being processed. Note Scope providers as normal functions ( def <name>(...):... ), not accessing global data, are safe per se. The reason to be stateless, is that no side effects (beside, e.g., loading other models) should influence the name lookup. The state of model resolution should mainly consist of models already loaded. These models are stored in a GlobalModelRepository class. This class (if required) is stored in the model. An included model loaded from another including model \"inherits\" the part of the GlobalModelRepository representing all loaded models. This is done to (a) cache already loaded models and (b) guarantee, that every referenced model element is instantiated exactly once. Even in the case of circular inclusions. Scope providers may return an object of type Postponed , if they depend on another event to happen first. This event is typically the resolution of another reference. The resolution process will repeat multiple times over all unresolved references to be resolved until all references are resolved or no progress regarding the resolution is observed. In the latter case an error is raised. The control flow responsibility of the resolution process is allocated to the model.py module.","title":"Technical aspects and implementation details"},{"location":"textx_command/","text":"textx command/tool \u00b6 To check and visualize (meta)models from the command line. Using the tool \u00b6 To get basic help: $ textx --help usage: textx [-h] [-i] [-d] cmd metamodel [model] textX checker and visualizer positional arguments: cmd Command - \"check\" or \"visualize\" metamodel Meta-model file name model Model file name optional arguments: -h, --help show this help message and exit -i case-insensitive parsing -d run in debug mode You can check and visualize (generate a .dot file) your meta-model or model using this tool. For example, to check and visualize a metamodel you could issue: $ textx visualize robot.tx Meta-model OK. Generating 'robot.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O robot.tx.dot' Create an image from the .dot file: $ dot -Tpng -O robot.tx.dot Or use some dot viewer. For example: $ xdot robot.tx.dot Visualize model: $ textx visualize robot.tx program.rbt Meta-model OK. Model OK. Generating 'robot.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O robot.tx.dot' Generating 'program.rbt.dot' file for model. To convert to png run 'dot -Tpng -O program.rbt.dot' To only check (meta)models use check command: $ textx check robot.tx program.rbt If there is an error you will get a nice error report: $ textx check robot.tx program.rbt Meta-model OK. Error in model file. Expected 'initial' or 'up' or 'down' or 'left' or 'right' or 'end' at program.rbt:(3, 3) => 'al 3, 1 *gore 4 '.","title":"textx command"},{"location":"textx_command/#textx-commandtool","text":"To check and visualize (meta)models from the command line.","title":"textx command/tool"},{"location":"textx_command/#using-the-tool","text":"To get basic help: $ textx --help usage: textx [-h] [-i] [-d] cmd metamodel [model] textX checker and visualizer positional arguments: cmd Command - \"check\" or \"visualize\" metamodel Meta-model file name model Model file name optional arguments: -h, --help show this help message and exit -i case-insensitive parsing -d run in debug mode You can check and visualize (generate a .dot file) your meta-model or model using this tool. For example, to check and visualize a metamodel you could issue: $ textx visualize robot.tx Meta-model OK. Generating 'robot.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O robot.tx.dot' Create an image from the .dot file: $ dot -Tpng -O robot.tx.dot Or use some dot viewer. For example: $ xdot robot.tx.dot Visualize model: $ textx visualize robot.tx program.rbt Meta-model OK. Model OK. Generating 'robot.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O robot.tx.dot' Generating 'program.rbt.dot' file for model. To convert to png run 'dot -Tpng -O program.rbt.dot' To only check (meta)models use check command: $ textx check robot.tx program.rbt If there is an error you will get a nice error report: $ textx check robot.tx program.rbt Meta-model OK. Error in model file. Expected 'initial' or 'up' or 'down' or 'left' or 'right' or 'end' at program.rbt:(3, 3) => 'al 3, 1 *gore 4 '.","title":"Using the tool"},{"location":"visualization/","text":"Visualization \u00b6 A meta-model, model and parse-tree can be exported to dot files ( GraphViz ) for visualization. Module textx.export contains functions metamodel_export and model_export that can export meta-model and model to dot files respectively. If debugging is enabled, meta-model, model and parse trees will automatically get exported to dot. Dot files can be viewed by dot viewers. There are quite a few dot viewers freely available (e.g. xdot , ZGRViewer ). Alternatively, dot files can be converted to image formats using dot command. For more info see this SO thread . Meta-model visualization \u00b6 To visualize a meta-model (see Entity example ) do: from textx import metamodel_from_file from textx.export import metamodel_export entity_mm = metamodel_from_file('entity.tx') metamodel_export(entity_mm, 'entity.dot') entity.dot file will be created. You can visualize this file by using various dot viewers or convert it to various image formats using the 'dot' tool. $ dot -Tpng -O entity.dot The following image is generated: Model visualization \u00b6 Similarly to meta-model visualization, you can also visualize your models (see Entity example ). from textx.export import model_export person_model = entity_mm.model_from_file('person.ent') model_export(person_model, 'person.dot') Convert this dot file to png with: $ dot -Tpng -O person.dot The following image is generated: Note Also, see textx command/tool for model visualization from the command line.","title":"Visualization"},{"location":"visualization/#visualization","text":"A meta-model, model and parse-tree can be exported to dot files ( GraphViz ) for visualization. Module textx.export contains functions metamodel_export and model_export that can export meta-model and model to dot files respectively. If debugging is enabled, meta-model, model and parse trees will automatically get exported to dot. Dot files can be viewed by dot viewers. There are quite a few dot viewers freely available (e.g. xdot , ZGRViewer ). Alternatively, dot files can be converted to image formats using dot command. For more info see this SO thread .","title":"Visualization"},{"location":"visualization/#meta-model-visualization","text":"To visualize a meta-model (see Entity example ) do: from textx import metamodel_from_file from textx.export import metamodel_export entity_mm = metamodel_from_file('entity.tx') metamodel_export(entity_mm, 'entity.dot') entity.dot file will be created. You can visualize this file by using various dot viewers or convert it to various image formats using the 'dot' tool. $ dot -Tpng -O entity.dot The following image is generated:","title":"Meta-model visualization"},{"location":"visualization/#model-visualization","text":"Similarly to meta-model visualization, you can also visualize your models (see Entity example ). from textx.export import model_export person_model = entity_mm.model_from_file('person.ent') model_export(person_model, 'person.dot') Convert this dot file to png with: $ dot -Tpng -O person.dot The following image is generated: Note Also, see textx command/tool for model visualization from the command line.","title":"Model visualization"},{"location":"about/comparison/","text":"Comparing textX to other tools \u00b6 There are generally two classes of textual DSL tools textX could be compared to. The first class comprises tools that use traditional parsing technologies, i.e. given a grammar (usually a context-free grammar ) they produce program code capable of recognizing whether a given textual input conforms to the given grammar. Furthermore, they enable either transformation of textual input to a tree structure (i.e. parse tree), that is processed afterwards, or definition of actions that should be executed during parsing if a particular pattern is recognized. Most popular representatives in this class are lex and yacc , ANTLR , GNU bison , These kind of tools are generally known by the name Parser Generators . textX's differences in regard to this first class are following: textX works as grammar interpreter i.e. parser code is not generated by the tool but the tools is configured by the grammar to recognize textual input on the language specified by the grammar. You can even embed your grammar as a Python string. This enables faster round-trip from grammar to the working parsers as the parser don't need to be regenerated but only reconfigured. Most of the classical parsing tools use context-free grammars while textX uses PEG grammars . The consequences are that lookahead is unlimited and there are no ambiguities possible as the alternative operator is ordered . Additionally, there is no need for a separate lexer. textX uses a single textual specification (grammar) to define not only the syntax of the language but also its meta-model (a.k.a. abstract syntax). The textX's meta-language is inspired by Xtext. This is very important feature which enables automatic construction of the model (a.k.a. abstract semantic graph - ASG or semantic model) without further work from the language designer. In traditional parsing tools transformation to the model usually involves coding of parse actions or manually written parse tree transformation. The second class of textual DSL tools are more powerful tools geared especially towards DSL construction. These kind of tools are generally known by the name Language Workbenches coined by Martin Fowler . Most popular representatives of this class are Xtext , Spoofax and MPS . These tools are much more complex, highly integrated to the particular development environment (IDE) but provide powerful tooling infrastructure for language development, debugging and evolving. These tools will build not only parser but also a language-specific editor, debugger, validator, visualiser etc. textX is positioned between these two classes of DSL tools. The goal of textX project is not a highly sophisticated DSL engineering platform but a simple DSL Python library that can be used in various Python applications and development environment. It can also be used for non-Python development using code generation from textX models (see Entity tutorial ). Tooling infrastructure, editor support etc. will be developed as independent projects (see for example textx-tools ). Difference to Xtext grammar language \u00b6 textX grammar language is inspired by Xtext and thus there are a lot of similarities between these tools when it come to grammar specification. But, there are also differences in several places. In this section we shall outline those differences to give users already familiar with Xtext a brief overview. Lexer and terminal rules \u00b6 textX uses PEG parsing which doesn't needs separate lexing phase. This eliminate the need to define lexemes in the grammar. Therefore, there is no terminal keyword in the textX nor any of special terminal definition rules used by Xtext. Types used for rules \u00b6 Xtext integrates tightly with Java and Ecore typing system providing keyword returns in rule definition by which language designer might define a class used to instantiate objects recognized by the parser. textX integrates with Python typing system. In textX there is no keyword returns . The class used for the rule will be dynamically created Python class for all non-match rules . Language designer can provide class using user classes registration on meta-model. If the rule is of [match type] than it will always return Python string or some of base Python types for BASETYPES inherited rules . Assignments \u00b6 In textX there are two types of many assignments ( *= - zero or more, += - one or more) whereas in Xtext there is only one ( += ) which defines the type of the inferred attribute but doesn't specify any information for the parser. Thus, if there should be zero or more matched elements you must additionally wrap your expression in zero or more match: In Xtext: Domainmodel : (elements+=Type)*; In textX: Domainmodel : elements*=Type; Similarly, optional assignment in Xtext is written as: static?='static'? In textX a '?' at the end of the expression is implied, i.e. rhs of the assignment will be optional: static?='static' Regular expression match \u00b6 In Xtext terminal rules are described using EBNF . In textX there is no difference between parser and terminal rules so you can use the full textX language to define terminals. Furthermore, textX gives you the full power of Python regular expressions through regular expression match . Regex matches are defined inside / / . Anything you can use in Python re module you can use here. This gives you quite powerful sublanguage for pattern definition. In Xtext: terminal ASCII: '0x' ('0'..'7') ('0'..'9'|'A'..'F'); In textX: ASCII: /0x[0-7]([0-9]|[A-F])/; Literal Regex match can be used anywhere a regular match rule can be used. For example: Person: name=/[a-zA-Z]+/ age=INT; Repetition modifiers \u00b6 textX provides a syntactic construct called repetition modifier which enables parser to be altered during parsing of a specific repetition expression. For example, there is often a need to define a separated list of elements. To match a list of integers separated by comma in Xtext you would write: list_of_ints+=INT (',' list_of_ints+=INT)* In textX the same expression can be written as: list_of_inst+=INT[','] The parser is instructed to parse one or more INT with commas in between. Repetition modifier can be a regular expression match too. For example, to match one or more integer separated by comma or semi-colon: list_of_ints+=INT[/,|;/] Inside square brackets more than one repetition modifier can be defined. See section in the docs for additional explanations. We are not aware of the similar feature in Xtext. Rule modifiers \u00b6 Similarly to repetition modifiers, in textX parser can be altered at the rule level too. Currently, only white-space alteration can be defined on the rule level: For example: Rule: 'entity' name=ID /\\s*/ call=Rule2; Rule2[noskipws]: 'first' 'second'; Parser will be altered for Rule2 not to skip white-spaces. All rules down the call chain inherit modifiers. There are hidden rules in Xtext which can achieve the similar effect, even define different kind of tokens that can be hidden from the semantic model, but the rule modifier in textX serve different purpose. It is a general mechanism for parser alteration per rule that can be used in the future to define some other alteration (e.g. case sensitivity). Unordered groups \u00b6 Xtext support unordered groups using & operator. For example: Modifier: static?='static'? & final?='final'? & visibility=Visibility; enum Visibility: PUBLIC='public' | PRIVATE='private' | PROTECTED='protected'; In textX unordered groups are specified as a special kind of repetitions. Thus, repetition modifiers can be applied also: Modifier: (static?='static' final?='final' visibility=Visibility)#[','] Visibility: 'public' | 'private' | 'protected'; Previous example will match any of the following: private, static, final static, private, final ... Notice the use of , separator as a repetition modifier. Syntactic predicates \u00b6 textX is based on PEG grammars. Unlike CFGs, PEGs can't be ambiguous, i.e. if an input parses it has exactly one parse tree. textX is backtracking parser and will try each alternative in predetermined order until it succeeds. Thus, textX grammar can't be ambiguous. Nevertheless, sometimes it is not possible to specify desired parse tree by reordering alternatives. In that case syntactic predicates are used. textX implements both and- and not- syntactic predicates . On the other hand, predictive non-backtracking parsers (as is ANTLR used by Xtext) must make a decision which alternative to chose. Thus, grammar might be ambiguous and additional specification is needed by a language designer to resolve ambiguity and choose desired parse tree. Xtext uses a positive lookahead syntactic predicates (=> and ->). See here . Hidden rules \u00b6 Xtext uses hidden terminal symbols to suppress non-important parts of the input from the semantic model. This is used for comments, whitespaces etc. Terminal rules are referenced from the hidden list in the parser rules. All rules called from the one using hidden terminals inherits them. textX provides support for whitespaces alteration on the parser level and rule level and a special Comment match rule that can be used to describe comments pattern which are suppressed from the model. Comment rule is currently defined for the whole grammar, i.e. can't be altered on a per-rule basis. Parent-child relationships \u00b6 textX will provide explicit parent reference on all objects that are contained inside some other objects. This attribute is a plain Python attribute. The relationship is imposed by the grammar. Xtext, begin based on Ecore, provides similar mechanism through Ecore API. Enums \u00b6 Xtext support Enum rules while textX does not. In textX you use match rule with ordered choice to mimic enums. Scoping \u00b6 At present stage textX doesn't provide builtin mechanism for scoping definition. However, this can be done in Python using object processors but there is no specific scoping API that could help language developer in resolving links. Xtext does provide a Scoping API which can be used by the Xtend code to specify scoping rules. Additional differences in the tool usage \u00b6 Some of the differences in tools usage are outlined here. REPL \u00b6 textX is Python based, thus it is easy to interactively play with it on the Python console. Example ipython session: In [1]: from textx import metamodel_from_str In [2]: mm = metamodel_from_str(\"\"\" ...: Model: points+=Point; ...: Point: x=INT ',' y=INT ';'; ...: \"\"\") In [3]: model = mm.model_from_str(\"\"\" ...: 34, 45; 56, 78; 88, 12;\"\"\") In [4]: model.points Out[4]: [<textx:Point object at 0x7fdfb4cda828>, <textx:Point object at 0x7fdfb4cdada0>, <textx:Point object at 0x7fdfb4cdacf8>] In [5]: model.points[1].x Out[5]: 56 In [6]: model.points[1].y Out[6]: 78 Xtext is Java based and works as generator thus it is not possible, as far as we know, to experiment in this way. Post-processing \u00b6 textX provide model objects post processing by registering a Python callable that will receive object as it is constructed. Post-processing is used for all sorts of things, from model semantic validation to model augmentation. An approach to augment model after loading in Xtext is given here . Parser control \u00b6 In textX several aspect of parsing can be controlled : Whitespaces Case sensitivity Keyword handling These settings are altered during meta-model construction. Whitespaces can be further controlled on a per-rule basis. Xtext enable hidden terminal symbols which can be used for whitespace handling. Case sensitivity can be altered for parser rules but not for lexer rules . Mapping to host language types \u00b6 textX will dynamically create ordinary Python classes from the grammar rules. You can register your own classes during meta-model construction which will be used instead. Thus, it is easy to provide your domain model in the form of Python classes. Xtext is based on ECore model, thus all concepts will be instances of ECore classes. Additionally, there is an API which can be used to dynamically build JVM types from the DSL concepts providing tight integration with JVM. Built-in objects \u00b6 In textX you can provide objects that will be available to every model. It is used to provide, e.g. built-in types of the language. For more details see built-in objects section in the docs. An approach to augment model after loading in Xtext is given here . Additional languages \u00b6 Xtext use two additional DSLs: Xbase - a general expression language Xtend - a modern Java dialect which can be used in various places in the Xtext framework The only additional DSL used in textX is genconf which is a DSL for generator configuration and has been developed as a part of textx-tools project . Template engines \u00b6 textX doesn't impose a particular template engine to be used for code generation. Although we use Jinja2 in some of the examples, there is nothing in textX that is Jinja2 specific. You can use any template engine you like. Xtext provide it's own template language as a part of Xtend DSL. This language nicely integrates in the overall platform. IDE integration \u00b6 Xtext is integrated in Eclipse and InteliJ IDEs and generates full language-specific tool-chain from the grammar description and additional specifications. textX does not provide IDE integrations. There is textx-tools project which provide pluggable platform for developing textX languages and generators with project scaffolding. Integration for popular code editors is planned. There is some basic support for vim and emacs at the moment. There is a support for visualization of grammars (meta-models) and models but the model visualization is generic, i.e. it will show you the object graph of your model objects. We plan to develop language-specific model visualization support.","title":"Comparison"},{"location":"about/comparison/#comparing-textx-to-other-tools","text":"There are generally two classes of textual DSL tools textX could be compared to. The first class comprises tools that use traditional parsing technologies, i.e. given a grammar (usually a context-free grammar ) they produce program code capable of recognizing whether a given textual input conforms to the given grammar. Furthermore, they enable either transformation of textual input to a tree structure (i.e. parse tree), that is processed afterwards, or definition of actions that should be executed during parsing if a particular pattern is recognized. Most popular representatives in this class are lex and yacc , ANTLR , GNU bison , These kind of tools are generally known by the name Parser Generators . textX's differences in regard to this first class are following: textX works as grammar interpreter i.e. parser code is not generated by the tool but the tools is configured by the grammar to recognize textual input on the language specified by the grammar. You can even embed your grammar as a Python string. This enables faster round-trip from grammar to the working parsers as the parser don't need to be regenerated but only reconfigured. Most of the classical parsing tools use context-free grammars while textX uses PEG grammars . The consequences are that lookahead is unlimited and there are no ambiguities possible as the alternative operator is ordered . Additionally, there is no need for a separate lexer. textX uses a single textual specification (grammar) to define not only the syntax of the language but also its meta-model (a.k.a. abstract syntax). The textX's meta-language is inspired by Xtext. This is very important feature which enables automatic construction of the model (a.k.a. abstract semantic graph - ASG or semantic model) without further work from the language designer. In traditional parsing tools transformation to the model usually involves coding of parse actions or manually written parse tree transformation. The second class of textual DSL tools are more powerful tools geared especially towards DSL construction. These kind of tools are generally known by the name Language Workbenches coined by Martin Fowler . Most popular representatives of this class are Xtext , Spoofax and MPS . These tools are much more complex, highly integrated to the particular development environment (IDE) but provide powerful tooling infrastructure for language development, debugging and evolving. These tools will build not only parser but also a language-specific editor, debugger, validator, visualiser etc. textX is positioned between these two classes of DSL tools. The goal of textX project is not a highly sophisticated DSL engineering platform but a simple DSL Python library that can be used in various Python applications and development environment. It can also be used for non-Python development using code generation from textX models (see Entity tutorial ). Tooling infrastructure, editor support etc. will be developed as independent projects (see for example textx-tools ).","title":"Comparing textX to other tools"},{"location":"about/comparison/#difference-to-xtext-grammar-language","text":"textX grammar language is inspired by Xtext and thus there are a lot of similarities between these tools when it come to grammar specification. But, there are also differences in several places. In this section we shall outline those differences to give users already familiar with Xtext a brief overview.","title":"Difference to Xtext grammar language"},{"location":"about/comparison/#lexer-and-terminal-rules","text":"textX uses PEG parsing which doesn't needs separate lexing phase. This eliminate the need to define lexemes in the grammar. Therefore, there is no terminal keyword in the textX nor any of special terminal definition rules used by Xtext.","title":"Lexer and terminal rules"},{"location":"about/comparison/#types-used-for-rules","text":"Xtext integrates tightly with Java and Ecore typing system providing keyword returns in rule definition by which language designer might define a class used to instantiate objects recognized by the parser. textX integrates with Python typing system. In textX there is no keyword returns . The class used for the rule will be dynamically created Python class for all non-match rules . Language designer can provide class using user classes registration on meta-model. If the rule is of [match type] than it will always return Python string or some of base Python types for BASETYPES inherited rules .","title":"Types used for rules"},{"location":"about/comparison/#assignments","text":"In textX there are two types of many assignments ( *= - zero or more, += - one or more) whereas in Xtext there is only one ( += ) which defines the type of the inferred attribute but doesn't specify any information for the parser. Thus, if there should be zero or more matched elements you must additionally wrap your expression in zero or more match: In Xtext: Domainmodel : (elements+=Type)*; In textX: Domainmodel : elements*=Type; Similarly, optional assignment in Xtext is written as: static?='static'? In textX a '?' at the end of the expression is implied, i.e. rhs of the assignment will be optional: static?='static'","title":"Assignments"},{"location":"about/comparison/#regular-expression-match","text":"In Xtext terminal rules are described using EBNF . In textX there is no difference between parser and terminal rules so you can use the full textX language to define terminals. Furthermore, textX gives you the full power of Python regular expressions through regular expression match . Regex matches are defined inside / / . Anything you can use in Python re module you can use here. This gives you quite powerful sublanguage for pattern definition. In Xtext: terminal ASCII: '0x' ('0'..'7') ('0'..'9'|'A'..'F'); In textX: ASCII: /0x[0-7]([0-9]|[A-F])/; Literal Regex match can be used anywhere a regular match rule can be used. For example: Person: name=/[a-zA-Z]+/ age=INT;","title":"Regular expression match"},{"location":"about/comparison/#repetition-modifiers","text":"textX provides a syntactic construct called repetition modifier which enables parser to be altered during parsing of a specific repetition expression. For example, there is often a need to define a separated list of elements. To match a list of integers separated by comma in Xtext you would write: list_of_ints+=INT (',' list_of_ints+=INT)* In textX the same expression can be written as: list_of_inst+=INT[','] The parser is instructed to parse one or more INT with commas in between. Repetition modifier can be a regular expression match too. For example, to match one or more integer separated by comma or semi-colon: list_of_ints+=INT[/,|;/] Inside square brackets more than one repetition modifier can be defined. See section in the docs for additional explanations. We are not aware of the similar feature in Xtext.","title":"Repetition modifiers"},{"location":"about/comparison/#rule-modifiers","text":"Similarly to repetition modifiers, in textX parser can be altered at the rule level too. Currently, only white-space alteration can be defined on the rule level: For example: Rule: 'entity' name=ID /\\s*/ call=Rule2; Rule2[noskipws]: 'first' 'second'; Parser will be altered for Rule2 not to skip white-spaces. All rules down the call chain inherit modifiers. There are hidden rules in Xtext which can achieve the similar effect, even define different kind of tokens that can be hidden from the semantic model, but the rule modifier in textX serve different purpose. It is a general mechanism for parser alteration per rule that can be used in the future to define some other alteration (e.g. case sensitivity).","title":"Rule modifiers"},{"location":"about/comparison/#unordered-groups","text":"Xtext support unordered groups using & operator. For example: Modifier: static?='static'? & final?='final'? & visibility=Visibility; enum Visibility: PUBLIC='public' | PRIVATE='private' | PROTECTED='protected'; In textX unordered groups are specified as a special kind of repetitions. Thus, repetition modifiers can be applied also: Modifier: (static?='static' final?='final' visibility=Visibility)#[','] Visibility: 'public' | 'private' | 'protected'; Previous example will match any of the following: private, static, final static, private, final ... Notice the use of , separator as a repetition modifier.","title":"Unordered groups"},{"location":"about/comparison/#syntactic-predicates","text":"textX is based on PEG grammars. Unlike CFGs, PEGs can't be ambiguous, i.e. if an input parses it has exactly one parse tree. textX is backtracking parser and will try each alternative in predetermined order until it succeeds. Thus, textX grammar can't be ambiguous. Nevertheless, sometimes it is not possible to specify desired parse tree by reordering alternatives. In that case syntactic predicates are used. textX implements both and- and not- syntactic predicates . On the other hand, predictive non-backtracking parsers (as is ANTLR used by Xtext) must make a decision which alternative to chose. Thus, grammar might be ambiguous and additional specification is needed by a language designer to resolve ambiguity and choose desired parse tree. Xtext uses a positive lookahead syntactic predicates (=> and ->). See here .","title":"Syntactic predicates"},{"location":"about/comparison/#hidden-rules","text":"Xtext uses hidden terminal symbols to suppress non-important parts of the input from the semantic model. This is used for comments, whitespaces etc. Terminal rules are referenced from the hidden list in the parser rules. All rules called from the one using hidden terminals inherits them. textX provides support for whitespaces alteration on the parser level and rule level and a special Comment match rule that can be used to describe comments pattern which are suppressed from the model. Comment rule is currently defined for the whole grammar, i.e. can't be altered on a per-rule basis.","title":"Hidden rules"},{"location":"about/comparison/#parent-child-relationships","text":"textX will provide explicit parent reference on all objects that are contained inside some other objects. This attribute is a plain Python attribute. The relationship is imposed by the grammar. Xtext, begin based on Ecore, provides similar mechanism through Ecore API.","title":"Parent-child relationships"},{"location":"about/comparison/#enums","text":"Xtext support Enum rules while textX does not. In textX you use match rule with ordered choice to mimic enums.","title":"Enums"},{"location":"about/comparison/#scoping","text":"At present stage textX doesn't provide builtin mechanism for scoping definition. However, this can be done in Python using object processors but there is no specific scoping API that could help language developer in resolving links. Xtext does provide a Scoping API which can be used by the Xtend code to specify scoping rules.","title":"Scoping"},{"location":"about/comparison/#additional-differences-in-the-tool-usage","text":"Some of the differences in tools usage are outlined here.","title":"Additional differences in the tool usage"},{"location":"about/comparison/#repl","text":"textX is Python based, thus it is easy to interactively play with it on the Python console. Example ipython session: In [1]: from textx import metamodel_from_str In [2]: mm = metamodel_from_str(\"\"\" ...: Model: points+=Point; ...: Point: x=INT ',' y=INT ';'; ...: \"\"\") In [3]: model = mm.model_from_str(\"\"\" ...: 34, 45; 56, 78; 88, 12;\"\"\") In [4]: model.points Out[4]: [<textx:Point object at 0x7fdfb4cda828>, <textx:Point object at 0x7fdfb4cdada0>, <textx:Point object at 0x7fdfb4cdacf8>] In [5]: model.points[1].x Out[5]: 56 In [6]: model.points[1].y Out[6]: 78 Xtext is Java based and works as generator thus it is not possible, as far as we know, to experiment in this way.","title":"REPL"},{"location":"about/comparison/#post-processing","text":"textX provide model objects post processing by registering a Python callable that will receive object as it is constructed. Post-processing is used for all sorts of things, from model semantic validation to model augmentation. An approach to augment model after loading in Xtext is given here .","title":"Post-processing"},{"location":"about/comparison/#parser-control","text":"In textX several aspect of parsing can be controlled : Whitespaces Case sensitivity Keyword handling These settings are altered during meta-model construction. Whitespaces can be further controlled on a per-rule basis. Xtext enable hidden terminal symbols which can be used for whitespace handling. Case sensitivity can be altered for parser rules but not for lexer rules .","title":"Parser control"},{"location":"about/comparison/#mapping-to-host-language-types","text":"textX will dynamically create ordinary Python classes from the grammar rules. You can register your own classes during meta-model construction which will be used instead. Thus, it is easy to provide your domain model in the form of Python classes. Xtext is based on ECore model, thus all concepts will be instances of ECore classes. Additionally, there is an API which can be used to dynamically build JVM types from the DSL concepts providing tight integration with JVM.","title":"Mapping to host language types"},{"location":"about/comparison/#built-in-objects","text":"In textX you can provide objects that will be available to every model. It is used to provide, e.g. built-in types of the language. For more details see built-in objects section in the docs. An approach to augment model after loading in Xtext is given here .","title":"Built-in objects"},{"location":"about/comparison/#additional-languages","text":"Xtext use two additional DSLs: Xbase - a general expression language Xtend - a modern Java dialect which can be used in various places in the Xtext framework The only additional DSL used in textX is genconf which is a DSL for generator configuration and has been developed as a part of textx-tools project .","title":"Additional languages"},{"location":"about/comparison/#template-engines","text":"textX doesn't impose a particular template engine to be used for code generation. Although we use Jinja2 in some of the examples, there is nothing in textX that is Jinja2 specific. You can use any template engine you like. Xtext provide it's own template language as a part of Xtend DSL. This language nicely integrates in the overall platform.","title":"Template engines"},{"location":"about/comparison/#ide-integration","text":"Xtext is integrated in Eclipse and InteliJ IDEs and generates full language-specific tool-chain from the grammar description and additional specifications. textX does not provide IDE integrations. There is textx-tools project which provide pluggable platform for developing textX languages and generators with project scaffolding. Integration for popular code editors is planned. There is some basic support for vim and emacs at the moment. There is a support for visualization of grammars (meta-models) and models but the model visualization is generic, i.e. it will show you the object graph of your model objects. We plan to develop language-specific model visualization support.","title":"IDE integration"},{"location":"about/contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated!. You can contribute code, documentation, tests, bug reports. Every little bit helps, and credit will always be given. If you plan to make a contribution it would be great if you first announce that on the issue tracker . You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/igordejanovic/textX/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 textX could always use more documentation, whether as part of the official textX docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/igordejanovic/textX/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up textX for local development. Fork the textX repo on GitHub. Clone your fork locally: $ git clone git@github.com:your_name_here/textX.git Install your local copy into a virtualenv. Assuming you have virtualenv installed, this is how you set up your fork for local development: $ cd textX/ $ virtualenv venv $ source venv/bin/activate $ pip install -r requirements_dev.txt $ pip install -e . Previous stuff is needed only the first time. To continue working on textX later you just do: $ cd textX/ $ source venv/bin/activate Note that on Windows sourcing syntax is a bit different. Check the docs for virtualenv. An excellent overview of available tools for Python environments management can be found here To verify that everything is setup properly run tests: $ flake8 tests/functional/ $ py.test tests/functional/ Create a branch for local development:: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass flake8 and the tests: $ flake8 textX tests $ py.test tests/functional/ Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds/changes functionality, the docs should be updated. The pull request should work for Python 2.7, 3.4-3.7. Check https://travis-ci.org/igordejanovic/textX/pull_requests and make sure that the tests pass for all supported Python versions. Tips \u00b6 To run a subset of tests: $ py.test tests/functional/mytest.py or a single test: $ py.test tests/functional/mytest.py::some_test Credit \u00b6 This guide is based on the guide generated by Cookiecutter and cookiecutter-pypackage project template.","title":"Contributing"},{"location":"about/contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated!. You can contribute code, documentation, tests, bug reports. Every little bit helps, and credit will always be given. If you plan to make a contribution it would be great if you first announce that on the issue tracker . You can contribute in many ways:","title":"Contributing"},{"location":"about/contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"about/contributing/#report-bugs","text":"Report bugs at https://github.com/igordejanovic/textX/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"about/contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"about/contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"about/contributing/#write-documentation","text":"textX could always use more documentation, whether as part of the official textX docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"about/contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/igordejanovic/textX/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"about/contributing/#get-started","text":"Ready to contribute? Here's how to set up textX for local development. Fork the textX repo on GitHub. Clone your fork locally: $ git clone git@github.com:your_name_here/textX.git Install your local copy into a virtualenv. Assuming you have virtualenv installed, this is how you set up your fork for local development: $ cd textX/ $ virtualenv venv $ source venv/bin/activate $ pip install -r requirements_dev.txt $ pip install -e . Previous stuff is needed only the first time. To continue working on textX later you just do: $ cd textX/ $ source venv/bin/activate Note that on Windows sourcing syntax is a bit different. Check the docs for virtualenv. An excellent overview of available tools for Python environments management can be found here To verify that everything is setup properly run tests: $ flake8 tests/functional/ $ py.test tests/functional/ Create a branch for local development:: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass flake8 and the tests: $ flake8 textX tests $ py.test tests/functional/ Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"about/contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds/changes functionality, the docs should be updated. The pull request should work for Python 2.7, 3.4-3.7. Check https://travis-ci.org/igordejanovic/textX/pull_requests and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"about/contributing/#tips","text":"To run a subset of tests: $ py.test tests/functional/mytest.py or a single test: $ py.test tests/functional/mytest.py::some_test","title":"Tips"},{"location":"about/contributing/#credit","text":"This guide is based on the guide generated by Cookiecutter and cookiecutter-pypackage project template.","title":"Credit"},{"location":"about/discuss/","text":"Discuss, ask questions \u00b6 For bug reports, general discussion and help please use GitHub issue tracker .","title":"Discuss"},{"location":"about/discuss/#discuss-ask-questions","text":"For bug reports, general discussion and help please use GitHub issue tracker .","title":"Discuss, ask questions"},{"location":"about/license/","text":"textX is distributed under the terms of MIT license. Copyright (c) 2014-2015 Igor R. Dejanovi\u0107 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"ideas/","text":"This is a place for new development ideas. Contributions are welcome. If you have new idea or would like to refine some of the existing make a fork of the repo, do your changes and send me a pull request.","title":"Home"},{"location":"ideas/domain_specific_visualization/","text":"Domain-Specific model visualization \u00b6 Currently, both meta-models and models can be visualized but the model visualization is not domain-specific. It would be nice to have a way do specify what our language looks like (see StateMachine example ). Some form of DSL for specifying the graphical syntax could help. Besides, graphical syntaxes are very good for the structure overview and navigation (panning, zooming). This feature would provide fast navigation over the model and could be synced with the textual representation in various editors (vim, emacs, atom, sublime...) using editor plugins. Therefore, editing would be done textually, but code comprehension and navigation could be aided by graphical visualization. TODO: Ideas for graphical syntax DSL","title":"Domain-Specific model visualization"},{"location":"ideas/domain_specific_visualization/#domain-specific-model-visualization","text":"Currently, both meta-models and models can be visualized but the model visualization is not domain-specific. It would be nice to have a way do specify what our language looks like (see StateMachine example ). Some form of DSL for specifying the graphical syntax could help. Besides, graphical syntaxes are very good for the structure overview and navigation (panning, zooming). This feature would provide fast navigation over the model and could be synced with the textual representation in various editors (vim, emacs, atom, sublime...) using editor plugins. Therefore, editing would be done textually, but code comprehension and navigation could be aided by graphical visualization. TODO: Ideas for graphical syntax DSL","title":"Domain-Specific model visualization"},{"location":"ideas/language_modularization/","text":"Language modularization \u00b6 This is a draft Currently, textX has import statement which enables one grammar to import rules form the other one. But for some more complex use-cases (see issue 26 ) this is not enough. Modularization at the grammar/meta-model level \u00b6 Motivation example (see issue 26 ). type_meta_def = ''' TypeDef: 'typedef' name=ID type=[Type]; Type: RangeType | SimpleType; SimpleTypeDecl: 'type' type=SimpleType; SimpleType: name=ID; RangeTypeDecl: 'type' type=RangeType; RangeType: base=[SimpleType] range=Range; Range: '[' from=INT (':' to=INT)? ']'; ''' struct_meta_def = ''' Model: elements+=Element; Element: Entity | TypeDef; ... ''' type_meta = metamodel_from_str(type_meta_def) struct_meta = metamodel_from_str(struct_meta_def) Problem \u00b6 Structural meta-model references a rule/class from the type meta-model ( Element references TypeDef ). Possible solution \u00b6 Let the meta-model inherit other meta-models struct_meta = metamodel_from_str(struct_meta_def, inherits=[type_meta]) All referenced but the missing rules from the struct meta-model will be searched in types meta-model. Currently, this is supported by the grammar import but it should be supported at the API level so that grammars could be defined as strings. To support multiple inheritance properly investigate C3 linearization as a solution. Open questions \u00b6 Should we override rules inside inherited grammars? For example, if we override Range to specify some different syntax or meta-class attributes, should the new definition be used from the Type rule in the type_meta_def grammar? If we instantiate model using type_meta , original Range will be used but if we instantiate TypeDef using struct_meta our redefined Range will be used. These will be objects of different Range classes. Do we want it to work that way? IMHO we should use one Range class for all our models. Possible solution \u00b6 Maybe it would be best to make one integral meta-model with strict rules for meta-class resolution but provide means to define the root rule for each model parse. For example: meta = metamodel_from_str(struct_meta_def, inherits=[type_meta]) We get a single integral meta-model. Imagine that we want to redefined ranges: new_range = ''' Range: '(' low=INT ('->' high=INT)? ')'; ''' new_range_meta = metamodel_from_str(new_range) Now an integral meta-model could be built with: meta = metamodel_from_str(struct_meta_def, inherits=[new_range_meta, type_meta]) Range will be resolved from the first meta-model following the C3 linearization. That will be new_range_meta , thus syntax and meta-class for Range will be changed in all model parsings using the meta meta-model. Parsing of models: # type_model will, in rule RangeType, use Range from new_range_meta type_model = meta.model_from_str(type_def, root_rule='TypeDef') struct_model = meta.model_from_str(struct_def, root_rule='Model', ref_models=[type_model]) ref_models is an iterable of models used for object reference resolving. Each nameable from the reference models can be used in the link rule reference resolving. type_model model parse could use redefined Range from the integral meta-model meta , thus, struct_model will reference right class instances. This idea is compatible with the current usage of import keyword in the meta-model definition. The semantics of import will change to use the same C3 linearization and inheritance semantics. Probably it would be best to change import keyword to inherits . In addition, a new function for composing the meta-model could be introduced: meta = compose_metamodels(struct_meta, new_range_meta, type_meta) To support this kind of composition, meta-model loading should not fail in the event of missing rule references. Meta-model might be constructed in the unresolved phase. Composing it with other meta-models might resolve it fully or partially. Only fully resolved meta-models can be used for model parsing. Modularization at the model level \u00b6 This could be supported by the ref_models iterable explained in the previous section. Open questions are: - How to handle scoping? - How to support independent model reloading? Motivation would be reloading of a model on a file change. It would be nice to reload just a single model and relink it. Some form of a reference tracking might be implemented.","title":"Language modularization"},{"location":"ideas/language_modularization/#language-modularization","text":"This is a draft Currently, textX has import statement which enables one grammar to import rules form the other one. But for some more complex use-cases (see issue 26 ) this is not enough.","title":"Language modularization"},{"location":"ideas/language_modularization/#modularization-at-the-grammarmeta-model-level","text":"Motivation example (see issue 26 ). type_meta_def = ''' TypeDef: 'typedef' name=ID type=[Type]; Type: RangeType | SimpleType; SimpleTypeDecl: 'type' type=SimpleType; SimpleType: name=ID; RangeTypeDecl: 'type' type=RangeType; RangeType: base=[SimpleType] range=Range; Range: '[' from=INT (':' to=INT)? ']'; ''' struct_meta_def = ''' Model: elements+=Element; Element: Entity | TypeDef; ... ''' type_meta = metamodel_from_str(type_meta_def) struct_meta = metamodel_from_str(struct_meta_def)","title":"Modularization at the grammar/meta-model level"},{"location":"ideas/language_modularization/#problem","text":"Structural meta-model references a rule/class from the type meta-model ( Element references TypeDef ).","title":"Problem"},{"location":"ideas/language_modularization/#possible-solution","text":"Let the meta-model inherit other meta-models struct_meta = metamodel_from_str(struct_meta_def, inherits=[type_meta]) All referenced but the missing rules from the struct meta-model will be searched in types meta-model. Currently, this is supported by the grammar import but it should be supported at the API level so that grammars could be defined as strings. To support multiple inheritance properly investigate C3 linearization as a solution.","title":"Possible solution"},{"location":"ideas/language_modularization/#open-questions","text":"Should we override rules inside inherited grammars? For example, if we override Range to specify some different syntax or meta-class attributes, should the new definition be used from the Type rule in the type_meta_def grammar? If we instantiate model using type_meta , original Range will be used but if we instantiate TypeDef using struct_meta our redefined Range will be used. These will be objects of different Range classes. Do we want it to work that way? IMHO we should use one Range class for all our models.","title":"Open questions"},{"location":"ideas/language_modularization/#possible-solution_1","text":"Maybe it would be best to make one integral meta-model with strict rules for meta-class resolution but provide means to define the root rule for each model parse. For example: meta = metamodel_from_str(struct_meta_def, inherits=[type_meta]) We get a single integral meta-model. Imagine that we want to redefined ranges: new_range = ''' Range: '(' low=INT ('->' high=INT)? ')'; ''' new_range_meta = metamodel_from_str(new_range) Now an integral meta-model could be built with: meta = metamodel_from_str(struct_meta_def, inherits=[new_range_meta, type_meta]) Range will be resolved from the first meta-model following the C3 linearization. That will be new_range_meta , thus syntax and meta-class for Range will be changed in all model parsings using the meta meta-model. Parsing of models: # type_model will, in rule RangeType, use Range from new_range_meta type_model = meta.model_from_str(type_def, root_rule='TypeDef') struct_model = meta.model_from_str(struct_def, root_rule='Model', ref_models=[type_model]) ref_models is an iterable of models used for object reference resolving. Each nameable from the reference models can be used in the link rule reference resolving. type_model model parse could use redefined Range from the integral meta-model meta , thus, struct_model will reference right class instances. This idea is compatible with the current usage of import keyword in the meta-model definition. The semantics of import will change to use the same C3 linearization and inheritance semantics. Probably it would be best to change import keyword to inherits . In addition, a new function for composing the meta-model could be introduced: meta = compose_metamodels(struct_meta, new_range_meta, type_meta) To support this kind of composition, meta-model loading should not fail in the event of missing rule references. Meta-model might be constructed in the unresolved phase. Composing it with other meta-models might resolve it fully or partially. Only fully resolved meta-models can be used for model parsing.","title":"Possible solution"},{"location":"ideas/language_modularization/#modularization-at-the-model-level","text":"This could be supported by the ref_models iterable explained in the previous section. Open questions are: - How to handle scoping? - How to support independent model reloading? Motivation would be reloading of a model on a file change. It would be nice to reload just a single model and relink it. Some form of a reference tracking might be implemented.","title":"Modularization at the model level"},{"location":"tutorials/entity/","text":"Entity tutorial \u00b6 A tutorial for building ER-like language and generating Java code. Entity language \u00b6 In this example we will see how to make a simple language for data modeling. We will use this language to generate Java source code (POJO classes). Our main concept will be Entity . Each entity will have one or more properties . Each property is defined by its name and its type . Let's sketch out a model in our language. entity Person { name : string address: Address age: integer } entity Address { street : string city : string country : string } The grammar \u00b6 In our example we see that each entity starts with a keyword entity . After that, we have a name that is the identifier and an open bracket. Between the brackets we have properties. In textX this is written as: Entity: 'entity' name=ID '{' properties+=Property '}' ; We can see that the Entity rule references Property rule from the assignment. Each property is defined by the name , colon ( : ) and the type's name. This can be written as: Property: name=ID ':' type=ID ; Now, grammar defined in this way will parse a single Entity . We haven't stated yet that our model consists of many Entity instances. Let's specify that. We are introducing a rule for the whole model which states that each entity model contains one or more entities . EntityModel: entities+=Entity ; This rule must be the first rule in the textX grammar file. First rule is always considered a root rule . This grammar will parse the example model from the beginning. At any time you can check and visualize entity meta-model and person model using command: $ textx visualize entity.tx person.ent Given grammar in file entity.tx and example Person model in file person.ent . This command will produce entity.tx.dot and person.ent.dot files which can be viewed by any dot viewer or converted to e.g. PNG format using command: $ dot -Tpng -O *.dot Note that GraphViz must be installed to use dot command line utility. Meta-model now looks like this: While the example (Person model) looks like this: What you see on the model diagram are actual Python objects. It looks good, but it would be even better if a reference to Address from properties was an actual Python reference, not just a value of str type. This resolving of object names to references can be done automatically by textX. To do so, we shall change our Property rule to be: Property: name=ID ':' type=[Entity] ; Now, we state that type is a reference (we are using [] ) to an object of the Entity class. This instructs textX to search for the name of the Entity after the colon and when it is found to resolve it to an Entity instance with the same name defined elsewhere in the model. But, we have a problem now. There are no entities called string and integer which we used for several properties in our model. To remedy this, we must introduce dummy entities with those names and change properties attribute assignment to be zero or more ( *= ) since our dummy entities will have no attributes. Although, this solution is possible it wouldn't be elegant at all. So let's do something better. First, let's introduce an abstract concept called Type which as the generalization of simple types (like integer and string ) and complex types (like Entity ). Type: SimpleType | Entity ; This is called abstract rule, and it means that Type is either a SimpleType or an Entity instance. Type class from the meta-model will never be instantiated. Now, we shall change our Property rule definition: Property: name=ID ':' type=[Type] ; And, at the end, there must be a way to specify our simple types. Let's do that at the beginning of our model. EntityModel: simple_types *= SimpleType entities += Entity ; And the definition of SimpleType would be: SimpleType: 'type' name=ID ; So, simple types are defined at the beginning of the model using the keyword type after which we specify the name of the type. Our person model will now begin with: type string type integer entity Person { ... Meta-model now looks like this: While the example (Person model) looks like this: But, we can make this language even better. We can define some built-in simple types so that the user does not need to specify them for every model. This has to be done from python during meta-model instantiation. We can instantiate integer and string simple types and introduce them in every model programmatically. The first problem is how to instantiate the SimpleType class. textX will dynamically create a Python class for each rule from the grammar but we do not have access to these classes in advance. Luckily, textX offers a way to override dynamically created classes with user supplied ones. So, we can create our class SimpleType and register that class during meta-model instantiation together with two of its instances ( integer and string ). class SimpleType(object): def __init__(self, parent, name): # remember to include parent param. self.parent = parent self.name = name Now, we can make a dict of builtin objects. myobjs = { 'integer': SimpleType(None, 'integer') 'string': SimpleType(None, 'string') } And register our custom class and two builtins on the meta-model: meta = metamodel_from_file('entity.tx', classes=[SimpleType], builtins=myobjs) Now, if we use meta to load our models we do not have to specify integer and string types. Furthermore, each instance of SimpleType will be an instance of our SimpleType class. We, can use this custom classes support to implement any custom behaviour in our object graph. Generating source code \u00b6 textX doesn't impose any specific library or process for source code generation. You can use anything you like. From the print function to template engines. I highly recommend you to use some of the well-established template engines . Here, we will see how to use Jinja2 template engine to generate Java source code from our entity models. First, install jinja2 using pip: $ pip install Jinja2 Now, for each entity in our model we will render one Java file with a pair of getters and setters for each property. Let's write Jinja2 template (file java.template ): // Autogenerated from java.template file class {{entity.name}} { {% for property in entity.properties %} protected {{property.type|javatype}} {{property.name}}; {% endfor %} {% for property in entity.properties %} public {{property.type|javatype}} get{{property.name|capitalize}}(){ return this.{{property.name}}; } public void set{{property.name|capitalize}}({{property.type|javatype}} new_value){ this.{{property.name}} = new_value; } {% endfor %} } Templates have static parts that will be rendered as they are, and variable parts whose content depends on the model. Variable parts are written inside {{}} . For example {{entity.name}} from the second line is the name of the current entity. The logic of rendering is controlled by tags written in {%...%} (e.g. loops, conditions). We can see that this template will render a warning that this is auto-generated code (it is always good to do that!). Then it will render a Java class named after the current entity and then, for each property in the entity (please note that we are using textX model so all attribute names come from the textX grammar) we are rendering a Java attribute. After that, we are rendering getters and setters. You could notice that for rendering proper Java types we are using |javatype expression. This is called filter in Jinja2. It works similarly to unix pipes. You have an object and you pass it to some filter. Filter will transform the given object to some other object. In this case javatype is a simple python function that will transform our types ( integer and string ) to proper Java types ( int and String ). Now, let's see how we can put this together. We need to initialize the Jinja2 engine, instantiate our meta-model, load our model and then iterate over the entities from our model and generate a Java file for each entity: from os import mkdir from os.path import exists, dirname, join import jinja2 from textx import metamodel_from_file this_folder = dirname(__file__) class SimpleType(object): def __init__(self, parent, name): self.parent = parent self.name = name def __str__(self): return self.name def get_entity_mm(): \"\"\" Builds and returns a meta-model for Entity language. \"\"\" type_builtins = { 'integer': SimpleType(None, 'integer'), 'string': SimpleType(None, 'string') } entity_mm = metamodel_from_file(join(this_folder, 'entity.tx'), classes=[SimpleType], builtins=type_builtins) return entity_mm def main(debug=False): # Instantiate the Entity meta-model entity_mm = get_entity_mm() def javatype(s): \"\"\" Maps type names from SimpleType to Java. \"\"\" return { 'integer': 'int', 'string': 'String' }.get(s.name, s.name) # Create the output folder srcgen_folder = join(this_folder, 'srcgen') if not exists(srcgen_folder): mkdir(srcgen_folder) # Initialize the template engine. jinja_env = jinja2.Environment( loader=jinja2.FileSystemLoader(this_folder), trim_blocks=True, lstrip_blocks=True) # Register the filter for mapping Entity type names to Java type names. jinja_env.filters['javatype'] = javatype # Load the Java template template = jinja_env.get_template('java.template') # Build a Person model from person.ent file person_model = entity_mm.model_from_file(join(this_folder, 'person.ent')) # Generate Java code for entity in person_model.entities: # For each entity generate java file with open(join(srcgen_folder, \"%s.java\" % entity.name.capitalize()), 'w') as f: f.write(template.render(entity=entity)) if __name__ == \"__main__\": main() And the generated code will look like this: // Autogenerated from java.template file class Person { protected String name; protected Address address; protected int age; public String getName(){ return this.name; } public void setName(String new_value){ this.name = new_value; } public Address getAddress(){ return this.address; } public void setAddress(Address new_value){ this.address = new_value; } public int getAge(){ return this.age; } public void setAge(int new_value){ this.age = new_value; } } Note The code from this tutorial can be found in the examples/Entity folder.","title":"Entity"},{"location":"tutorials/entity/#entity-tutorial","text":"A tutorial for building ER-like language and generating Java code.","title":"Entity tutorial"},{"location":"tutorials/entity/#entity-language","text":"In this example we will see how to make a simple language for data modeling. We will use this language to generate Java source code (POJO classes). Our main concept will be Entity . Each entity will have one or more properties . Each property is defined by its name and its type . Let's sketch out a model in our language. entity Person { name : string address: Address age: integer } entity Address { street : string city : string country : string }","title":"Entity language"},{"location":"tutorials/entity/#the-grammar","text":"In our example we see that each entity starts with a keyword entity . After that, we have a name that is the identifier and an open bracket. Between the brackets we have properties. In textX this is written as: Entity: 'entity' name=ID '{' properties+=Property '}' ; We can see that the Entity rule references Property rule from the assignment. Each property is defined by the name , colon ( : ) and the type's name. This can be written as: Property: name=ID ':' type=ID ; Now, grammar defined in this way will parse a single Entity . We haven't stated yet that our model consists of many Entity instances. Let's specify that. We are introducing a rule for the whole model which states that each entity model contains one or more entities . EntityModel: entities+=Entity ; This rule must be the first rule in the textX grammar file. First rule is always considered a root rule . This grammar will parse the example model from the beginning. At any time you can check and visualize entity meta-model and person model using command: $ textx visualize entity.tx person.ent Given grammar in file entity.tx and example Person model in file person.ent . This command will produce entity.tx.dot and person.ent.dot files which can be viewed by any dot viewer or converted to e.g. PNG format using command: $ dot -Tpng -O *.dot Note that GraphViz must be installed to use dot command line utility. Meta-model now looks like this: While the example (Person model) looks like this: What you see on the model diagram are actual Python objects. It looks good, but it would be even better if a reference to Address from properties was an actual Python reference, not just a value of str type. This resolving of object names to references can be done automatically by textX. To do so, we shall change our Property rule to be: Property: name=ID ':' type=[Entity] ; Now, we state that type is a reference (we are using [] ) to an object of the Entity class. This instructs textX to search for the name of the Entity after the colon and when it is found to resolve it to an Entity instance with the same name defined elsewhere in the model. But, we have a problem now. There are no entities called string and integer which we used for several properties in our model. To remedy this, we must introduce dummy entities with those names and change properties attribute assignment to be zero or more ( *= ) since our dummy entities will have no attributes. Although, this solution is possible it wouldn't be elegant at all. So let's do something better. First, let's introduce an abstract concept called Type which as the generalization of simple types (like integer and string ) and complex types (like Entity ). Type: SimpleType | Entity ; This is called abstract rule, and it means that Type is either a SimpleType or an Entity instance. Type class from the meta-model will never be instantiated. Now, we shall change our Property rule definition: Property: name=ID ':' type=[Type] ; And, at the end, there must be a way to specify our simple types. Let's do that at the beginning of our model. EntityModel: simple_types *= SimpleType entities += Entity ; And the definition of SimpleType would be: SimpleType: 'type' name=ID ; So, simple types are defined at the beginning of the model using the keyword type after which we specify the name of the type. Our person model will now begin with: type string type integer entity Person { ... Meta-model now looks like this: While the example (Person model) looks like this: But, we can make this language even better. We can define some built-in simple types so that the user does not need to specify them for every model. This has to be done from python during meta-model instantiation. We can instantiate integer and string simple types and introduce them in every model programmatically. The first problem is how to instantiate the SimpleType class. textX will dynamically create a Python class for each rule from the grammar but we do not have access to these classes in advance. Luckily, textX offers a way to override dynamically created classes with user supplied ones. So, we can create our class SimpleType and register that class during meta-model instantiation together with two of its instances ( integer and string ). class SimpleType(object): def __init__(self, parent, name): # remember to include parent param. self.parent = parent self.name = name Now, we can make a dict of builtin objects. myobjs = { 'integer': SimpleType(None, 'integer') 'string': SimpleType(None, 'string') } And register our custom class and two builtins on the meta-model: meta = metamodel_from_file('entity.tx', classes=[SimpleType], builtins=myobjs) Now, if we use meta to load our models we do not have to specify integer and string types. Furthermore, each instance of SimpleType will be an instance of our SimpleType class. We, can use this custom classes support to implement any custom behaviour in our object graph.","title":"The grammar"},{"location":"tutorials/entity/#generating-source-code","text":"textX doesn't impose any specific library or process for source code generation. You can use anything you like. From the print function to template engines. I highly recommend you to use some of the well-established template engines . Here, we will see how to use Jinja2 template engine to generate Java source code from our entity models. First, install jinja2 using pip: $ pip install Jinja2 Now, for each entity in our model we will render one Java file with a pair of getters and setters for each property. Let's write Jinja2 template (file java.template ): // Autogenerated from java.template file class {{entity.name}} { {% for property in entity.properties %} protected {{property.type|javatype}} {{property.name}}; {% endfor %} {% for property in entity.properties %} public {{property.type|javatype}} get{{property.name|capitalize}}(){ return this.{{property.name}}; } public void set{{property.name|capitalize}}({{property.type|javatype}} new_value){ this.{{property.name}} = new_value; } {% endfor %} } Templates have static parts that will be rendered as they are, and variable parts whose content depends on the model. Variable parts are written inside {{}} . For example {{entity.name}} from the second line is the name of the current entity. The logic of rendering is controlled by tags written in {%...%} (e.g. loops, conditions). We can see that this template will render a warning that this is auto-generated code (it is always good to do that!). Then it will render a Java class named after the current entity and then, for each property in the entity (please note that we are using textX model so all attribute names come from the textX grammar) we are rendering a Java attribute. After that, we are rendering getters and setters. You could notice that for rendering proper Java types we are using |javatype expression. This is called filter in Jinja2. It works similarly to unix pipes. You have an object and you pass it to some filter. Filter will transform the given object to some other object. In this case javatype is a simple python function that will transform our types ( integer and string ) to proper Java types ( int and String ). Now, let's see how we can put this together. We need to initialize the Jinja2 engine, instantiate our meta-model, load our model and then iterate over the entities from our model and generate a Java file for each entity: from os import mkdir from os.path import exists, dirname, join import jinja2 from textx import metamodel_from_file this_folder = dirname(__file__) class SimpleType(object): def __init__(self, parent, name): self.parent = parent self.name = name def __str__(self): return self.name def get_entity_mm(): \"\"\" Builds and returns a meta-model for Entity language. \"\"\" type_builtins = { 'integer': SimpleType(None, 'integer'), 'string': SimpleType(None, 'string') } entity_mm = metamodel_from_file(join(this_folder, 'entity.tx'), classes=[SimpleType], builtins=type_builtins) return entity_mm def main(debug=False): # Instantiate the Entity meta-model entity_mm = get_entity_mm() def javatype(s): \"\"\" Maps type names from SimpleType to Java. \"\"\" return { 'integer': 'int', 'string': 'String' }.get(s.name, s.name) # Create the output folder srcgen_folder = join(this_folder, 'srcgen') if not exists(srcgen_folder): mkdir(srcgen_folder) # Initialize the template engine. jinja_env = jinja2.Environment( loader=jinja2.FileSystemLoader(this_folder), trim_blocks=True, lstrip_blocks=True) # Register the filter for mapping Entity type names to Java type names. jinja_env.filters['javatype'] = javatype # Load the Java template template = jinja_env.get_template('java.template') # Build a Person model from person.ent file person_model = entity_mm.model_from_file(join(this_folder, 'person.ent')) # Generate Java code for entity in person_model.entities: # For each entity generate java file with open(join(srcgen_folder, \"%s.java\" % entity.name.capitalize()), 'w') as f: f.write(template.render(entity=entity)) if __name__ == \"__main__\": main() And the generated code will look like this: // Autogenerated from java.template file class Person { protected String name; protected Address address; protected int age; public String getName(){ return this.name; } public void setName(String new_value){ this.name = new_value; } public Address getAddress(){ return this.address; } public void setAddress(Address new_value){ this.address = new_value; } public int getAge(){ return this.age; } public void setAge(int new_value){ this.age = new_value; } } Note The code from this tutorial can be found in the examples/Entity folder.","title":"Generating source code"},{"location":"tutorials/hello_world/","text":"Hello World example \u00b6 This is an example of very simple Hello World like language. These are the steps to build a very basic Hello World - like language. Write a language description in textX (file hello.tx ): HelloWorldModel: 'hello' to_greet+=Who[','] ; Who: name = /[^,]*/ ; Description consists of a set of parsing rules which at the same time describe Python classes that will be dynamically created and used to instantiate objects of your model. This small example consists of two rules: HelloWorldModel and Who . HelloWorldModel starts with the keyword hello after which a one or more Who object must be written separated by commas. Who objects will be parsed, instantiated and stored in a to_greet list on a HelloWorldModel object. Who objects consists only of its names which must be matched the regular expression rule /[^,]*/ (match non-comma zero or more times). Please see textX grammar section for more information on writing grammar rules. At this point you can check and visualise meta-model using following command from command line: $ textx visualize hello.tx Meta-model OK. Generating 'hello.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O hello.tx.dot' You can see that for each rule from language description an appropriate Python class has been created. A BASETYPE hierarchy is built-in. Each meta-model has it. Create some content (i.e. model) in your new language ( example.hello ): hello World, Solar System, Universe Your language syntax is also described by language rules from step 1. If we break down the text of the example model it looks like this: We see that the whole line is a HelloWorldModel and the parts World , Solar System , and Universe are Who objects. Red coloured text is syntactic noise that is needed by the parser (and programmers) to recognize the boundaries of the objects in the text. To use your models from Python first create meta-model from textX language description (file hello.py ): from textx import metamodel_from_file hello_meta = metamodel_from_file('hello.tx') Than use meta-model to create models from textual description: example_hello_model = hello_meta.model_from_file('example.hello') Textual model example.hello will be parsed and transformed to a plain Python object graph. Object classes are those defined by the meta-model. You can optionally export model to dot file to visualize it. Run following from the command line: $ textx visualize hello.tx example.hello Meta-model OK. Model OK. Generating 'hello.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O hello.tx.dot' Generating 'example.hello.dot' file for model. To convert to png run 'dot -Tpng -O example.hello.dot' This is an object graph automatically constructed from example.hello file. We see that each Who object is contained in the python attribute to_greet of list type which is defined by the grammar. Use your model: interpret it, generate code \u2026 It is a plain Python graph of objects with plain attributes! Note Try out a complete tutorial for building a simple robot language.","title":"Hello World"},{"location":"tutorials/hello_world/#hello-world-example","text":"This is an example of very simple Hello World like language. These are the steps to build a very basic Hello World - like language. Write a language description in textX (file hello.tx ): HelloWorldModel: 'hello' to_greet+=Who[','] ; Who: name = /[^,]*/ ; Description consists of a set of parsing rules which at the same time describe Python classes that will be dynamically created and used to instantiate objects of your model. This small example consists of two rules: HelloWorldModel and Who . HelloWorldModel starts with the keyword hello after which a one or more Who object must be written separated by commas. Who objects will be parsed, instantiated and stored in a to_greet list on a HelloWorldModel object. Who objects consists only of its names which must be matched the regular expression rule /[^,]*/ (match non-comma zero or more times). Please see textX grammar section for more information on writing grammar rules. At this point you can check and visualise meta-model using following command from command line: $ textx visualize hello.tx Meta-model OK. Generating 'hello.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O hello.tx.dot' You can see that for each rule from language description an appropriate Python class has been created. A BASETYPE hierarchy is built-in. Each meta-model has it. Create some content (i.e. model) in your new language ( example.hello ): hello World, Solar System, Universe Your language syntax is also described by language rules from step 1. If we break down the text of the example model it looks like this: We see that the whole line is a HelloWorldModel and the parts World , Solar System , and Universe are Who objects. Red coloured text is syntactic noise that is needed by the parser (and programmers) to recognize the boundaries of the objects in the text. To use your models from Python first create meta-model from textX language description (file hello.py ): from textx import metamodel_from_file hello_meta = metamodel_from_file('hello.tx') Than use meta-model to create models from textual description: example_hello_model = hello_meta.model_from_file('example.hello') Textual model example.hello will be parsed and transformed to a plain Python object graph. Object classes are those defined by the meta-model. You can optionally export model to dot file to visualize it. Run following from the command line: $ textx visualize hello.tx example.hello Meta-model OK. Model OK. Generating 'hello.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O hello.tx.dot' Generating 'example.hello.dot' file for model. To convert to png run 'dot -Tpng -O example.hello.dot' This is an object graph automatically constructed from example.hello file. We see that each Who object is contained in the python attribute to_greet of list type which is defined by the grammar. Use your model: interpret it, generate code \u2026 It is a plain Python graph of objects with plain attributes! Note Try out a complete tutorial for building a simple robot language.","title":"Hello World example"},{"location":"tutorials/robot/","text":"Robot tutorial \u00b6 In this tutorial we will build a simple robot language to demonstrate the basic workflow when working with textX. Robot language \u00b6 When building a DSL we should first do a domain analysis, to see what concepts do we have and what are their relationships and constraints. In the following paragraph a short analysis is done. Important concepts are emphasized. In this case we want an imperative language that should define robot movement on the imaginary grid. Robot should move in four base direction . We will call these directions up, down, left and right (you could use north, south, west and east if you like). Additionally, we shall have a robot coordinate given in x, y position . For simplicity, our robot can move in discrete steps . In each movement robot can move by 1 or more steps but in the same direction. Coordinate is given as a pair of integer numbers. Robot will have an initial position . If not given explicitly it is assumed that position is (0, 0) . So, lets build a simple robot language. Grammar \u00b6 First, we need to define a grammar for the language. In textX the grammar will also define a meta-model (a.k.a. abstract syntax) for the language which can be visualized and be used as a part of the documentation. Usually we start by outlining some program in the language we are building. Here is an example program on robot language: begin initial 3, 1 up 4 left 9 down right 1 end We have begin and end keywords that define the beginning and end of the program. In this case we could do without these keywords but lets have it to make it more interesting. In between those two keywords we have a sequence of instruction. First instruction will position our robot at coordinate (3, 1) . After that robot will move up 4 steps , left 9 steps , down 1 step (1 step is the default) and finally 1 step to the right . Lets start with grammar definition. We shall start in a top-down manner so lets first define a program as a whole. Program: 'begin' commands*=Command 'end' ; Here we see that our program is defined with sequence of: string match ( 'begin' ), zero or more assignment to commands attribute, string match ( 'end' ). String matches will require literal strings given at the begin and end of program. If this is not satisfied a syntax error will be raised. This whole rule ( Program ) will create a class with the same name in the meta-model. Each program will be an instance of this class. commands assignment will result in a python attribute commands on the instance of Program class. This attribute will be of Python list type (because *= assignment is used). Each element of this list will be a specific command. Now, we see that we have different types of commands. First command has two parameters and it defines the robot initial position. Other commands has one or zero parameters and define the robot movement. To state that some textX rule is specialised in 2 or more rules we use an abstract rule. For Command we shall define two specializations: InitialCommand and MoveCommand like this: Command: InitialCommand | MoveCommand ; Abstract rule is given as ordered choice of other rules. This can be read as Each command is either a InitialCommand or MoveCommand . Lets now define command for setting initial position. InitialCommand: 'initial' x=INT ',' y=INT ; This rule specifies a class InitialCommand in the meta-model. Each initial position command will be an instance of this class. So, this command should start with the keyword initial after which we give an integer number (base type rule INT - this number will be available as attribute x on the InitialCommand instance), than a separator , is required after which we have y coordinate as integer number (this will be available as attribute y ). Using base type rule INT matched number from input string will be automatically converted to python type int . Now, lets define a movement command. We know that this command consists of direction identifier and optional number of steps (if not given the default will be 1). MoveCommand: direction=Direction (steps=INT)? ; So, the movement command model object will have two attributes. direction attribute will define one of the four possible directions and steps attribute will be an integer that will hold how many steps a robot will move in given direction. Steps are optional so if not given in the program it will still be a correct syntax. Notice, that the default of 1 is not specified in the grammar. The grammar deals with syntax constraints. Additional semantics will be handled later in model/object processors (see below). Now, the missing part is Direction rule referenced from the previous rule. This rule will define what can be written as a direction. We will define this rule like this: Direction: \"up\"|\"down\"|\"left\"|\"right\" ; This kind of rule is called a match rule . This rule does not result in a new object. It consists of ordered choice of simple matches (string, regex), base type rules (INT, STRING, BOOL...) and/or other match rule references. The result of this match will be assigned to the attribute from which it was referenced. If base type was used it will be converted in a proper python type. If not, it will be a python string that will contain the text that was matched from the input. In this case a one of 4 words will be matched and that string will be assigned to the direction attribute of the MoveCommand instance. The final touch to the grammar is a definition of the comment rule. We want to comment our robot code, right? In textX a special rule called Comment is used for that purpose. Lets define a C-style single line comments. Comment: /\\/\\/.*$/ ; Our grammar is done. Save it in robot.tx file. The content of this file should now be: Program: 'begin' commands*=Command 'end' ; Command: InitialCommand | MoveCommand ; InitialCommand: 'initial' x=INT ',' y=INT ; MoveCommand: direction=Direction (steps=INT)? ; Direction: \"up\"|\"down\"|\"left\"|\"right\" ; Comment: /\\/\\/.*$/ ; Notice that we have not constrained initial position command to be specified just once on the beginning of the program. This basically means that this command can be given multiple times throughout the program. I will leave as an exercise to the reader to implement this constraint. Next step during language design is meta-model visualization. It is usually easier to comprehend our language if rendered graphically. To do so we use excellent GraphViz software package and its DSL for graph specification called dot . It is a textual language for visual graph definition. Lets check our meta-model and export it to the dot language. $ textx visualize robot.tx Meta-model OK. Generating 'robot.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O robot.tx.dot' dot file can be opened with dot viewer (there are many to choose from) or transformed with dot tool to raster or vector graphics. For example: dot -Tpng robot_meta.dot -O robot_meta.png This command will create png image out of dot file. Instantiating meta-model \u00b6 In order to parse our models we first need to construct a meta-model. A textX meta-model is a Python object that contains all classes that can be instantiated in our model. For each grammar rule a class is created. Additionally, meta-model contains a parser that knows how to parse input strings. From parsed input (parse tree) meta-model will create a model. Meta-models are created from our grammar description, in this case robot.tx file. Open robot.py Python file and write following: from textx import metamodel_from_file robot_mm = metamodel_from_file('robot.tx') Note This meta-model can be used to parse multiple models. Instantiating model \u00b6 Now, when we have our meta-model we can parse models from strings or external textual files. Extend your robot.py with: robot_model = robot_mm.model_from_file('program.rbt') This command will parse file program.rbt and constructs our robot model. In this file does not match our language a syntax error will be raised on the first error encountered. In the same manner as meta-model visualization we can visualize our model too. $ textx visualize robot.tx program.rbt Meta-model OK. Model OK. Generating 'robot.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O robot.tx.dot' Generating 'program.rbt.dot' file for model. To convert to png run 'dot -Tpng -O program.rbt.dot' This will create program.dot file that can be visualized using proper viewer or transformed to image. $ dot -Tpng program.dot -O program.png For the robot program above we should get an image like this: Interpreting model \u00b6 When we have successfully parsed and loaded our model/program (or mogram or prodel ;) ) we can do various stuff. Usually what would you like to do is to translate your program to some other language (Java, Python, C#, Ruby,...) or you could build an interpreter that will evaluate/interpret your model directly. Or you could analyse your model, extract informations from it etc. It is up to you to decide. We will show here how to build a simple interpreter that will start the robot from the initial position and print the position of the robot after each command. Lets imagine that we have a robot that understands our language. In your robot.py file add: class Robot(object): def __init__(self): # Initial position is (0,0) self.x = 0 self.y = 0 def __str__(self): return \"Robot position is {}, {}.\".format(self.x, self.y) Now, our robot will have an interpret method that accepts our robot model and runs it. At each step this method will update the robot position and print it. def interpret(self, model): # model is an instance of Program for c in model.commands: if c.__class__.__name__ == \"InitialCommand\": print(\"Setting position to: {}, {}\".format(c.x, c.y)) self.x = c.x self.y = c.y else: dir = c.direction print(\"Going {} for {} step(s).\".format(dir, c.steps)) move = { \"up\": (0, 1), \"down\": (0, -1), \"left\": (-1, 0), \"right\": (1, 0) }[dir] # Calculate new robot position self.x += c.steps * move[0] self.y += c.steps * move[1] print(self) Now lets give our robot_model to Robot instance and see what happens. robot = Robot() robot.interpret(robot_model) You should get this output: Setting position to: 3, 1 Robot position is 3, 1. Going up for 4 step(s). Robot position is 3, 5. Going left for 9 step(s). Robot position is -6, 5. Going down for 0 step(s). Robot position is -6, 5. Going right for 1 step(s). Robot position is -5, 5. It is almost correct. We can see that down movement is for 0 steps because we have not defined the steps for down command and haven't done anything yet to implement default of 1. The best way to implement default value for step is to use so called object processor for MoveCommand . Object processor is a callable that gets called whenever textX parses and instantiates an object of particular class. Use register_obj_processors method on meta-model to register callables/processors for classes your wish to process in some way immediately after instantiation. Lets define our processor for MoveCommand in robot.py file. def move_command_processor(move_cmd): # If steps is not given, set it do default 1 value. if move_cmd.steps == 0: move_cmd.steps = 1 Now, register this processor on meta-model. After meta-model construction add a line for registration. robot_mm.register_obj_processors({'MoveCommand': move_command_processor}) register_obj_processors accepts a dictionary keyed by class name. The values are callables that should handle instances of the given class. If you run robot interpreter again you will get output like this: Setting position to: 3, 1 Robot position is 3, 1. Going up for 4 step(s). Robot position is 3, 5. Going left for 9 step(s). Robot position is -6, 5. Going down for 1 step(s). Robot position is -6, 4. Going right for 1 step(s). Robot position is -5, 4. And now our robot behaves as expected! Note The code from this tutorial can be found in the examples/robot folder. Next, you can read the Entity tutorial where you can see how to generate source code from your models.","title":"Robot"},{"location":"tutorials/robot/#robot-tutorial","text":"In this tutorial we will build a simple robot language to demonstrate the basic workflow when working with textX.","title":"Robot tutorial"},{"location":"tutorials/robot/#robot-language","text":"When building a DSL we should first do a domain analysis, to see what concepts do we have and what are their relationships and constraints. In the following paragraph a short analysis is done. Important concepts are emphasized. In this case we want an imperative language that should define robot movement on the imaginary grid. Robot should move in four base direction . We will call these directions up, down, left and right (you could use north, south, west and east if you like). Additionally, we shall have a robot coordinate given in x, y position . For simplicity, our robot can move in discrete steps . In each movement robot can move by 1 or more steps but in the same direction. Coordinate is given as a pair of integer numbers. Robot will have an initial position . If not given explicitly it is assumed that position is (0, 0) . So, lets build a simple robot language.","title":"Robot language"},{"location":"tutorials/robot/#grammar","text":"First, we need to define a grammar for the language. In textX the grammar will also define a meta-model (a.k.a. abstract syntax) for the language which can be visualized and be used as a part of the documentation. Usually we start by outlining some program in the language we are building. Here is an example program on robot language: begin initial 3, 1 up 4 left 9 down right 1 end We have begin and end keywords that define the beginning and end of the program. In this case we could do without these keywords but lets have it to make it more interesting. In between those two keywords we have a sequence of instruction. First instruction will position our robot at coordinate (3, 1) . After that robot will move up 4 steps , left 9 steps , down 1 step (1 step is the default) and finally 1 step to the right . Lets start with grammar definition. We shall start in a top-down manner so lets first define a program as a whole. Program: 'begin' commands*=Command 'end' ; Here we see that our program is defined with sequence of: string match ( 'begin' ), zero or more assignment to commands attribute, string match ( 'end' ). String matches will require literal strings given at the begin and end of program. If this is not satisfied a syntax error will be raised. This whole rule ( Program ) will create a class with the same name in the meta-model. Each program will be an instance of this class. commands assignment will result in a python attribute commands on the instance of Program class. This attribute will be of Python list type (because *= assignment is used). Each element of this list will be a specific command. Now, we see that we have different types of commands. First command has two parameters and it defines the robot initial position. Other commands has one or zero parameters and define the robot movement. To state that some textX rule is specialised in 2 or more rules we use an abstract rule. For Command we shall define two specializations: InitialCommand and MoveCommand like this: Command: InitialCommand | MoveCommand ; Abstract rule is given as ordered choice of other rules. This can be read as Each command is either a InitialCommand or MoveCommand . Lets now define command for setting initial position. InitialCommand: 'initial' x=INT ',' y=INT ; This rule specifies a class InitialCommand in the meta-model. Each initial position command will be an instance of this class. So, this command should start with the keyword initial after which we give an integer number (base type rule INT - this number will be available as attribute x on the InitialCommand instance), than a separator , is required after which we have y coordinate as integer number (this will be available as attribute y ). Using base type rule INT matched number from input string will be automatically converted to python type int . Now, lets define a movement command. We know that this command consists of direction identifier and optional number of steps (if not given the default will be 1). MoveCommand: direction=Direction (steps=INT)? ; So, the movement command model object will have two attributes. direction attribute will define one of the four possible directions and steps attribute will be an integer that will hold how many steps a robot will move in given direction. Steps are optional so if not given in the program it will still be a correct syntax. Notice, that the default of 1 is not specified in the grammar. The grammar deals with syntax constraints. Additional semantics will be handled later in model/object processors (see below). Now, the missing part is Direction rule referenced from the previous rule. This rule will define what can be written as a direction. We will define this rule like this: Direction: \"up\"|\"down\"|\"left\"|\"right\" ; This kind of rule is called a match rule . This rule does not result in a new object. It consists of ordered choice of simple matches (string, regex), base type rules (INT, STRING, BOOL...) and/or other match rule references. The result of this match will be assigned to the attribute from which it was referenced. If base type was used it will be converted in a proper python type. If not, it will be a python string that will contain the text that was matched from the input. In this case a one of 4 words will be matched and that string will be assigned to the direction attribute of the MoveCommand instance. The final touch to the grammar is a definition of the comment rule. We want to comment our robot code, right? In textX a special rule called Comment is used for that purpose. Lets define a C-style single line comments. Comment: /\\/\\/.*$/ ; Our grammar is done. Save it in robot.tx file. The content of this file should now be: Program: 'begin' commands*=Command 'end' ; Command: InitialCommand | MoveCommand ; InitialCommand: 'initial' x=INT ',' y=INT ; MoveCommand: direction=Direction (steps=INT)? ; Direction: \"up\"|\"down\"|\"left\"|\"right\" ; Comment: /\\/\\/.*$/ ; Notice that we have not constrained initial position command to be specified just once on the beginning of the program. This basically means that this command can be given multiple times throughout the program. I will leave as an exercise to the reader to implement this constraint. Next step during language design is meta-model visualization. It is usually easier to comprehend our language if rendered graphically. To do so we use excellent GraphViz software package and its DSL for graph specification called dot . It is a textual language for visual graph definition. Lets check our meta-model and export it to the dot language. $ textx visualize robot.tx Meta-model OK. Generating 'robot.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O robot.tx.dot' dot file can be opened with dot viewer (there are many to choose from) or transformed with dot tool to raster or vector graphics. For example: dot -Tpng robot_meta.dot -O robot_meta.png This command will create png image out of dot file.","title":"Grammar"},{"location":"tutorials/robot/#instantiating-meta-model","text":"In order to parse our models we first need to construct a meta-model. A textX meta-model is a Python object that contains all classes that can be instantiated in our model. For each grammar rule a class is created. Additionally, meta-model contains a parser that knows how to parse input strings. From parsed input (parse tree) meta-model will create a model. Meta-models are created from our grammar description, in this case robot.tx file. Open robot.py Python file and write following: from textx import metamodel_from_file robot_mm = metamodel_from_file('robot.tx') Note This meta-model can be used to parse multiple models.","title":"Instantiating meta-model"},{"location":"tutorials/robot/#instantiating-model","text":"Now, when we have our meta-model we can parse models from strings or external textual files. Extend your robot.py with: robot_model = robot_mm.model_from_file('program.rbt') This command will parse file program.rbt and constructs our robot model. In this file does not match our language a syntax error will be raised on the first error encountered. In the same manner as meta-model visualization we can visualize our model too. $ textx visualize robot.tx program.rbt Meta-model OK. Model OK. Generating 'robot.tx.dot' file for meta-model. To convert to png run 'dot -Tpng -O robot.tx.dot' Generating 'program.rbt.dot' file for model. To convert to png run 'dot -Tpng -O program.rbt.dot' This will create program.dot file that can be visualized using proper viewer or transformed to image. $ dot -Tpng program.dot -O program.png For the robot program above we should get an image like this:","title":"Instantiating model"},{"location":"tutorials/robot/#interpreting-model","text":"When we have successfully parsed and loaded our model/program (or mogram or prodel ;) ) we can do various stuff. Usually what would you like to do is to translate your program to some other language (Java, Python, C#, Ruby,...) or you could build an interpreter that will evaluate/interpret your model directly. Or you could analyse your model, extract informations from it etc. It is up to you to decide. We will show here how to build a simple interpreter that will start the robot from the initial position and print the position of the robot after each command. Lets imagine that we have a robot that understands our language. In your robot.py file add: class Robot(object): def __init__(self): # Initial position is (0,0) self.x = 0 self.y = 0 def __str__(self): return \"Robot position is {}, {}.\".format(self.x, self.y) Now, our robot will have an interpret method that accepts our robot model and runs it. At each step this method will update the robot position and print it. def interpret(self, model): # model is an instance of Program for c in model.commands: if c.__class__.__name__ == \"InitialCommand\": print(\"Setting position to: {}, {}\".format(c.x, c.y)) self.x = c.x self.y = c.y else: dir = c.direction print(\"Going {} for {} step(s).\".format(dir, c.steps)) move = { \"up\": (0, 1), \"down\": (0, -1), \"left\": (-1, 0), \"right\": (1, 0) }[dir] # Calculate new robot position self.x += c.steps * move[0] self.y += c.steps * move[1] print(self) Now lets give our robot_model to Robot instance and see what happens. robot = Robot() robot.interpret(robot_model) You should get this output: Setting position to: 3, 1 Robot position is 3, 1. Going up for 4 step(s). Robot position is 3, 5. Going left for 9 step(s). Robot position is -6, 5. Going down for 0 step(s). Robot position is -6, 5. Going right for 1 step(s). Robot position is -5, 5. It is almost correct. We can see that down movement is for 0 steps because we have not defined the steps for down command and haven't done anything yet to implement default of 1. The best way to implement default value for step is to use so called object processor for MoveCommand . Object processor is a callable that gets called whenever textX parses and instantiates an object of particular class. Use register_obj_processors method on meta-model to register callables/processors for classes your wish to process in some way immediately after instantiation. Lets define our processor for MoveCommand in robot.py file. def move_command_processor(move_cmd): # If steps is not given, set it do default 1 value. if move_cmd.steps == 0: move_cmd.steps = 1 Now, register this processor on meta-model. After meta-model construction add a line for registration. robot_mm.register_obj_processors({'MoveCommand': move_command_processor}) register_obj_processors accepts a dictionary keyed by class name. The values are callables that should handle instances of the given class. If you run robot interpreter again you will get output like this: Setting position to: 3, 1 Robot position is 3, 1. Going up for 4 step(s). Robot position is 3, 5. Going left for 9 step(s). Robot position is -6, 5. Going down for 1 step(s). Robot position is -6, 4. Going right for 1 step(s). Robot position is -5, 4. And now our robot behaves as expected! Note The code from this tutorial can be found in the examples/robot folder. Next, you can read the Entity tutorial where you can see how to generate source code from your models.","title":"Interpreting model"},{"location":"tutorials/state_machine/","text":"State machine language \u00b6 This is a video tutorial that explains the implementation of the StateMachine example . See the blog post about this language implementation .","title":"State Machine"},{"location":"tutorials/state_machine/#state-machine-language","text":"This is a video tutorial that explains the implementation of the StateMachine example . See the blog post about this language implementation .","title":"State machine language"},{"location":"tutorials/toylanguage/","text":"Toy language compiler \u00b6 A toy language compiler tutorial Windel Bouwman wrote an excellent tutorial for using textX and ppci to write a compiler for a simple language.","title":"Toy language compiler"},{"location":"tutorials/toylanguage/#toy-language-compiler","text":"A toy language compiler tutorial Windel Bouwman wrote an excellent tutorial for using textX and ppci to write a compiler for a simple language.","title":"Toy language compiler"},{"location":"whatsnew/release_1_5/","text":"What's new in textX 1.5 \u00b6 It's been quite a while since the last release of textX so this release brings a lot of new features and fixes. Operator precedence change \u00b6 Probably the most important/visible change is the change in the operator precendence. In the previous version, sequence had lower precedence than ordered choice which is counter-intuitive to the users that had previous experience with other tools where this is not the case. Ordered choice is now of lowest precedence which brings some backward compatibility that should be addressed for migration to the new version. For example, previously you would write: Rule: ('a' 'b') | ('c' 'd') ; To match either a b or c d . Now you can drop parentheses as the precedence of sequence is higher: Rule: 'a' 'b' | 'c' 'd' ; For the previous case there would be no problem in upgrade to 1.5 even if the grammar is not changed. But consider this: Rule: 'a' 'b' | 'c' 'd' ; In the previous version this would match a , than b or c , and then d as the | operator was of higher precedence than sequence. For your grammar to match the same language you must now write: Rule: 'a' ('b' | 'c') 'd' ; Unordered groups \u00b6 There is often a need to specify several matches that should occur in an arbitrary order. Read more here Match filters \u00b6 Match rules always return Python strings. Built-in rules return proper Python types. In order to change what is returned by match rules you can now register python callables that can additionally process returned strings. Read more here Multiple assignments to the same attribute \u00b6 textX had support for multiple assignments but it wasn't complete. When multiple assignment is detected, in the previous version, textX will decide that the multiplicity of the attribute is many but this lead to the problem if there is no way for parser to collect more than one value even if there is multiple assignments. For example, if all assignments belong to a different ordered choice alternative (see issue #33). In this version textX will correctly identify such cases. Read more here Model API \u00b6 There is now a set of handful functions for model querying. Read more here . Additional special model attributes \u00b6 In addition to _tx_position there is now _tx_position_end attribute on each model object which has the value of the end of the match in the input stream. In addition there is now _tx_metamodel attribute on the model which enables easy access to the language meta-model. Read more about it here . textX Emacs mode \u00b6 textX now has a support for Emacs . This package is also available in MELPA .","title":"Release 1.5"},{"location":"whatsnew/release_1_5/#whats-new-in-textx-15","text":"It's been quite a while since the last release of textX so this release brings a lot of new features and fixes.","title":"What's new in textX 1.5"},{"location":"whatsnew/release_1_5/#operator-precedence-change","text":"Probably the most important/visible change is the change in the operator precendence. In the previous version, sequence had lower precedence than ordered choice which is counter-intuitive to the users that had previous experience with other tools where this is not the case. Ordered choice is now of lowest precedence which brings some backward compatibility that should be addressed for migration to the new version. For example, previously you would write: Rule: ('a' 'b') | ('c' 'd') ; To match either a b or c d . Now you can drop parentheses as the precedence of sequence is higher: Rule: 'a' 'b' | 'c' 'd' ; For the previous case there would be no problem in upgrade to 1.5 even if the grammar is not changed. But consider this: Rule: 'a' 'b' | 'c' 'd' ; In the previous version this would match a , than b or c , and then d as the | operator was of higher precedence than sequence. For your grammar to match the same language you must now write: Rule: 'a' ('b' | 'c') 'd' ;","title":"Operator precedence change"},{"location":"whatsnew/release_1_5/#unordered-groups","text":"There is often a need to specify several matches that should occur in an arbitrary order. Read more here","title":"Unordered groups"},{"location":"whatsnew/release_1_5/#match-filters","text":"Match rules always return Python strings. Built-in rules return proper Python types. In order to change what is returned by match rules you can now register python callables that can additionally process returned strings. Read more here","title":"Match filters"},{"location":"whatsnew/release_1_5/#multiple-assignments-to-the-same-attribute","text":"textX had support for multiple assignments but it wasn't complete. When multiple assignment is detected, in the previous version, textX will decide that the multiplicity of the attribute is many but this lead to the problem if there is no way for parser to collect more than one value even if there is multiple assignments. For example, if all assignments belong to a different ordered choice alternative (see issue #33). In this version textX will correctly identify such cases. Read more here","title":"Multiple assignments to the same attribute"},{"location":"whatsnew/release_1_5/#model-api","text":"There is now a set of handful functions for model querying. Read more here .","title":"Model API"},{"location":"whatsnew/release_1_5/#additional-special-model-attributes","text":"In addition to _tx_position there is now _tx_position_end attribute on each model object which has the value of the end of the match in the input stream. In addition there is now _tx_metamodel attribute on the model which enables easy access to the language meta-model. Read more about it here .","title":"Additional special model attributes"},{"location":"whatsnew/release_1_5/#textx-emacs-mode","text":"textX now has a support for Emacs . This package is also available in MELPA .","title":"textX Emacs mode"},{"location":"whatsnew/release_1_7/","text":"What's new in textX 1.7 \u00b6 This version is a huge one thanks to Pierre (goto40@GitHub) who did an amazing job on scoping and multi-meta-model support. Configurable scoping support \u00b6 Multi-meta-model support \u00b6 Change in the way object processors work \u00b6 Previously object processors could do validation of the object and alter its content but could not replace the given object. This prevented processors for being used in e.g. expression evaluation or model interpretation (as a form of visitors/semantic actions). In this version it is now possible to return an arbitrary object from the processor and it will be used instead of the original. This change obsoletes match filters so in this version the support for match filters is removed which is a backward incompatible change. Luckily, upgrade is simple, just register your match filters as object processors like this: Before: match_filters = { 'MyRule': my_match_filter } mm = metamodel_from_file(..., match_filters=match_filters) Now: object_processors = { 'MyRule': my_match_filter } mm = metamodel_from_file(...) mm.register_obj_processors(object_processors)","title":"What's new in textX 1.7"},{"location":"whatsnew/release_1_7/#whats-new-in-textx-17","text":"This version is a huge one thanks to Pierre (goto40@GitHub) who did an amazing job on scoping and multi-meta-model support.","title":"What's new in textX 1.7"},{"location":"whatsnew/release_1_7/#configurable-scoping-support","text":"","title":"Configurable scoping support"},{"location":"whatsnew/release_1_7/#multi-meta-model-support","text":"","title":"Multi-meta-model support"},{"location":"whatsnew/release_1_7/#change-in-the-way-object-processors-work","text":"Previously object processors could do validation of the object and alter its content but could not replace the given object. This prevented processors for being used in e.g. expression evaluation or model interpretation (as a form of visitors/semantic actions). In this version it is now possible to return an arbitrary object from the processor and it will be used instead of the original. This change obsoletes match filters so in this version the support for match filters is removed which is a backward incompatible change. Luckily, upgrade is simple, just register your match filters as object processors like this: Before: match_filters = { 'MyRule': my_match_filter } mm = metamodel_from_file(..., match_filters=match_filters) Now: object_processors = { 'MyRule': my_match_filter } mm = metamodel_from_file(...) mm.register_obj_processors(object_processors)","title":"Change in the way object processors work"}]}