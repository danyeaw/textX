{
    "docs": [
        {
            "location": "/",
            "text": "textX\n is a meta-language (i.e. a\nlanguage for language definition) for domain-specific language (DSL)\nspecification in Python.\n\n\nIn a nutshell, textX will help you build your textual language in an easy way.\nYou can invent your own language or build a support for an already existing\ntextual language or file format.\n\n\nFrom a single grammar description, textX automatically builds a meta-model (in\nthe form of Python classes) and a parser for your language. The parser will\nparse expressions of your language and automatically build a graph of Python\nobjects (i.e. the model) corresponding to the meta-model.\n\n\ntextX is inspired by \nXtext\n - a Java based\nlanguage workbench for building DSLs with full tooling support (editors,\ndebuggers etc.) on the Eclipse platform.  If you like Java and\n\nEclipse\n check it out. It is a great tool.\n\n\nA video tutorial for textX installation and implementation of a simple data\nmodeling language is bellow.\n\n\n\n\n\nFor a not-so-basic video tutorial check out \nState Machine video\ntutorial\n.\n\n\nFeature highlights\n\u00b6\n\n\n\n\n\n\nMeta-model/parser from a single description\n\n\nA single description is used to define both language concrete syntax and its\nmeta-model (a.k.a. abstract syntax). See the description of\n\ngrammar\n and \nmetamodel\n.\n\n\n\n\n\n\nAutomatic model (AST) construction\n\n\nParse tree will automatically be transformed to a graph of python objects\n(a.k.a. the model). See the \nmodel\n section.\n\n\nPython classes will be created by textX but, if needed, user supplied\nclasses may be used. See \ncustom classes\n.\n\n\n\n\n\n\nAutomatic linking\n\n\nYou can have references to other objects in your language and the textual\nrepresentation of the reference will be resolved to the proper python reference\nautomatically.\n\n\n\n\n\n\nAutomatic parent-child relationships\n\n\ntextX will maintain a parent-child relationships imposed by the grammar.\nSee \nparent-child relationships\n.\n\n\n\n\n\n\nParser control\n\n\nParser can be configured with regard to case handling, whitespace handling,\nkeyword handling etc. See \nparser configuration\n.\n\n\n\n\n\n\nModel/object post-processing\n\n\nA callbacks (so called processors) can be registered for models and\nindividual classes.  This enables model/object postprocessing (validation,\nadditional changes etc.).  See \nprocessors\n section.\n\n\n\n\n\n\nGrammar modularization - imports\n\n\nGrammar can be split into multiple files and then files/grammars can be\nimported where needed. See \nGrammar\nmodularization\n.\n\n\n\n\n\n\nScope Providers\n\n\nScope Providers allow different types of scoping. See \nScoping\n.\n\n\n\n\n\n\nMulti-meta-model support\n\n\nDifferent meta-models can be combined. Typically some of these meta-models\nextend other meta-models (grammar modularization) and reference each other.\nSpecial scope providers support file-extension-based allocation of model\nfiles to meta models. See \nMulti meta-model support\n\n\n\n\n\n\nMeta-model/model visualization\n\n\nBoth meta-model and parsed models can be visulized using\n\nGraphViz\n software package. See\n\nvisualization\n section.\n\n\n\n\n\n\nInstallation\n\u00b6\n\n\n$ pip install textX\n\n\n\nNote: Previous command requires \npip\n to be\ninstalled.\n\n\nTo verify that textX is properly installed run:\n\n\n$ textx\n\n\n\nYou should get output like this:\n\n\nerror: the following arguments are required: cmd, metamodel\nusage: textx [-h] [-i] [-d] cmd metamodel [model]\n\ntextX checker and visualizer\n\n...\n\n\n\nPython versions\n\u00b6\n\n\ntextX works with Python 2.7, 3.3+. Other versions might work but are not\ntested.\n\n\nGetting started\n\u00b6\n\n\nSee textX \nTutorials\n to get you started:\n\n\n\n\nHello World\n\n\nRobot\n\n\nEntity\n\n\nState Machine\n - video tutorial\n\n\nToy language compiler\n\n\n\n\nFor specific information read various \nUser Guide\n sections.\n\n\nAlso, you can\ncheck out \nexamples\n.\n\n\nDiscussion and help\n\u00b6\n\n\nFor general discussion and help please\nuse \nGitHub issue tracker\n.\n\n\nProjects using textX\n\u00b6\n\n\nHere is a non-complete list of projects using textX.\n\n\n\n\n\n\nOpen-source\n\n\n\n\npyecore\n - ECore implementation in\n  Python. Vincent Aranega is doing a great work on integrating textX with\n  pyecore. The idea is that the integration eventually gets merged to the\n  main textX repo. For now, you can follow his\n  work \non his fork of textX\n.\n\n\npyTabs\n - A Domain-Specific Language\n  (DSL) for simplified music notation\n\n\napplang\n - Textual DSL for\n  generating mobile applications\n\n\npyFlies\n - DSL for cognitive\n  experiments modeling\n\n\nppci\n - Pure python\n  compiler infrastructure. \n\n\nExpremigen\n -  Expressive midi generation\n\n\nfanalyse\n - Fortran code parser/analyser\n\n\n\n\n\n\n\n\nCommercial\n\n\n\n\ntextX is used as a part of \nTyphoon-HIL's\n\n  schematic editor for the description of power electronic and DSP schemes and\n  components.\n\n\nJournaKit Followship .ows\n - A new\n  language for social media management with an interactive console to follow\n  and discover Twitter users (code released under GNU GPL 3). Read an\n  announcement\n  at\n  \nauthor's blog post\n.\n\n\n\n\n\n\n\n\nIf you are using textX to build some cool stuff drop me a line at igor dot\ndejanovic at gmail. I would like to hear from you!\n\n\nEditor/IDE support\n\u00b6\n\n\nVisual Studio Code support\n\u00b6\n\n\nThere is currently an ongoing effort to build tooling support\naround \nVisual Studio Code\n. The idea is to\nauto-generate VCS plugin with syntax highlighting, outline, InteliSense,\nnavigation, visualization. The input for the generator would be your language\ngrammar and additional information specified using various DSLs.\n\n\nYou can follow the progress\nat \nthe textX-tools GitHub organization\n.\n\n\nProjects that are currently in progress are:\n\n\n\n\nviewX\n - creating visualizators\n  for textX languages\n\n\ntextX-languageserver\n -\n  Language Server Protocol support for textX languages\n\n\ntextX-extensions\n - syntax\n  highlighting, code outline\n\n\n\n\nStay tuned ;)\n\n\nOther editors\n\u00b6\n\n\nIf you are a vim editor user check\nout \nsupport for vim\n.\n\n\nFor emacs there is \ntextx-mode\n which\nis also available in \nMELPA\n.\n\n\nYou can also check\nout \ntextX-ninja project\n. It is\ncurrently unmaintained.\n\n\nCiting textX\n\u00b6\n\n\nIf you are using textX in your research project we would be very grateful if you\ncite our paper:\n\n\nDejanovi\u0107 I., Vaderna R., Milosavljevi\u0107 G., Vukovi\u0107 \u017d. (2017).\nTextX: A Python tool for Domain-Specific Languages implementation.\nKnowledge-Based Systems, 115, 1-4.\n\n\n@article{Dejanovic2017,\n    author = {Dejanovi\\'{c}, I. and Vaderna, R. and Milosavljevi\\'{c}, G. and Vukovi\\'{c}, \\v{Z}.},\n    doi = {10.1016/j.knosys.2016.10.023},\n    issn = {0950-7051},\n    journal = {Knowledge-Based Systems},\n    keywords = {Domain-Specific Language; Meta-model; Model; Model-Driven software development; Parser; Python },\n    note = {},\n    pages = {1--4},\n    title = {{TextX: A Python tool for Domain-Specific Languages implementation}},\n    url = {http://www.sciencedirect.com/science/article/pii/S0950705116304178},\n    volume = {115},\n    year = {2017}\n}",
            "title": "Home"
        },
        {
            "location": "/#feature-highlights",
            "text": "Meta-model/parser from a single description  A single description is used to define both language concrete syntax and its\nmeta-model (a.k.a. abstract syntax). See the description of grammar  and  metamodel .    Automatic model (AST) construction  Parse tree will automatically be transformed to a graph of python objects\n(a.k.a. the model). See the  model  section.  Python classes will be created by textX but, if needed, user supplied\nclasses may be used. See  custom classes .    Automatic linking  You can have references to other objects in your language and the textual\nrepresentation of the reference will be resolved to the proper python reference\nautomatically.    Automatic parent-child relationships  textX will maintain a parent-child relationships imposed by the grammar.\nSee  parent-child relationships .    Parser control  Parser can be configured with regard to case handling, whitespace handling,\nkeyword handling etc. See  parser configuration .    Model/object post-processing  A callbacks (so called processors) can be registered for models and\nindividual classes.  This enables model/object postprocessing (validation,\nadditional changes etc.).  See  processors  section.    Grammar modularization - imports  Grammar can be split into multiple files and then files/grammars can be\nimported where needed. See  Grammar\nmodularization .    Scope Providers  Scope Providers allow different types of scoping. See  Scoping .    Multi-meta-model support  Different meta-models can be combined. Typically some of these meta-models\nextend other meta-models (grammar modularization) and reference each other.\nSpecial scope providers support file-extension-based allocation of model\nfiles to meta models. See  Multi meta-model support    Meta-model/model visualization  Both meta-model and parsed models can be visulized using GraphViz  software package. See visualization  section.",
            "title": "Feature highlights"
        },
        {
            "location": "/#installation",
            "text": "$ pip install textX  Note: Previous command requires  pip  to be\ninstalled.  To verify that textX is properly installed run:  $ textx  You should get output like this:  error: the following arguments are required: cmd, metamodel\nusage: textx [-h] [-i] [-d] cmd metamodel [model]\n\ntextX checker and visualizer\n\n...",
            "title": "Installation"
        },
        {
            "location": "/#python-versions",
            "text": "textX works with Python 2.7, 3.3+. Other versions might work but are not\ntested.",
            "title": "Python versions"
        },
        {
            "location": "/#getting-started",
            "text": "See textX  Tutorials  to get you started:   Hello World  Robot  Entity  State Machine  - video tutorial  Toy language compiler   For specific information read various  User Guide  sections.  Also, you can\ncheck out  examples .",
            "title": "Getting started"
        },
        {
            "location": "/#discussion-and-help",
            "text": "For general discussion and help please\nuse  GitHub issue tracker .",
            "title": "Discussion and help"
        },
        {
            "location": "/#projects-using-textx",
            "text": "Here is a non-complete list of projects using textX.    Open-source   pyecore  - ECore implementation in\n  Python. Vincent Aranega is doing a great work on integrating textX with\n  pyecore. The idea is that the integration eventually gets merged to the\n  main textX repo. For now, you can follow his\n  work  on his fork of textX .  pyTabs  - A Domain-Specific Language\n  (DSL) for simplified music notation  applang  - Textual DSL for\n  generating mobile applications  pyFlies  - DSL for cognitive\n  experiments modeling  ppci  - Pure python\n  compiler infrastructure.   Expremigen  -  Expressive midi generation  fanalyse  - Fortran code parser/analyser     Commercial   textX is used as a part of  Typhoon-HIL's \n  schematic editor for the description of power electronic and DSP schemes and\n  components.  JournaKit Followship .ows  - A new\n  language for social media management with an interactive console to follow\n  and discover Twitter users (code released under GNU GPL 3). Read an\n  announcement\n  at\n   author's blog post .     If you are using textX to build some cool stuff drop me a line at igor dot\ndejanovic at gmail. I would like to hear from you!",
            "title": "Projects using textX"
        },
        {
            "location": "/#editoride-support",
            "text": "",
            "title": "Editor/IDE support"
        },
        {
            "location": "/#visual-studio-code-support",
            "text": "There is currently an ongoing effort to build tooling support\naround  Visual Studio Code . The idea is to\nauto-generate VCS plugin with syntax highlighting, outline, InteliSense,\nnavigation, visualization. The input for the generator would be your language\ngrammar and additional information specified using various DSLs.  You can follow the progress\nat  the textX-tools GitHub organization .  Projects that are currently in progress are:   viewX  - creating visualizators\n  for textX languages  textX-languageserver  -\n  Language Server Protocol support for textX languages  textX-extensions  - syntax\n  highlighting, code outline   Stay tuned ;)",
            "title": "Visual Studio Code support"
        },
        {
            "location": "/#other-editors",
            "text": "If you are a vim editor user check\nout  support for vim .  For emacs there is  textx-mode  which\nis also available in  MELPA .  You can also check\nout  textX-ninja project . It is\ncurrently unmaintained.",
            "title": "Other editors"
        },
        {
            "location": "/#citing-textx",
            "text": "If you are using textX in your research project we would be very grateful if you\ncite our paper:  Dejanovi\u0107 I., Vaderna R., Milosavljevi\u0107 G., Vukovi\u0107 \u017d. (2017).\nTextX: A Python tool for Domain-Specific Languages implementation.\nKnowledge-Based Systems, 115, 1-4.  @article{Dejanovic2017,\n    author = {Dejanovi\\'{c}, I. and Vaderna, R. and Milosavljevi\\'{c}, G. and Vukovi\\'{c}, \\v{Z}.},\n    doi = {10.1016/j.knosys.2016.10.023},\n    issn = {0950-7051},\n    journal = {Knowledge-Based Systems},\n    keywords = {Domain-Specific Language; Meta-model; Model; Model-Driven software development; Parser; Python },\n    note = {},\n    pages = {1--4},\n    title = {{TextX: A Python tool for Domain-Specific Languages implementation}},\n    url = {http://www.sciencedirect.com/science/article/pii/S0950705116304178},\n    volume = {115},\n    year = {2017}\n}",
            "title": "Citing textX"
        },
        {
            "location": "/grammar/",
            "text": "textX grammar\n\u00b6\n\n\nThe language syntax and the meta-model are defined by the textX grammar in the\nform of a set of textX rules.\n\n\nRules\n\u00b6\n\n\nThe basic building blocks of the textX language are rules. Each rule is written\nin the following form:\n\n\nHello:\n  'hello' who=ID\n;\n\n\n\nThis rule is called \nHello\n. After the rule name, there is a colon. The body of\nthe rule is given as a textX expression, starting at the colon and ending with a\nsemicolon. This rule tells us that the pattern of \nHello\n objects in input\nstrings consists of the string literal \nhello\n, followed by the ID rule (here ID\nis a reference to a built-in rule, more about this in a moment).\n\n\nThese are valid \nHello\n objects:\n\n\nhello Alice\nhello Bob\nhello foo1234\n\n\n\nRule \nHello\n at the same time defines a Python class \nHello\n. When the rule is\nrecognized in the input stream, an object of this class will get created and the\nattribute \nwho\n will be set to whatever the rule \nID\n has matched after the word\n\nhello\n (this is specified by the assignment \nwho=ID\n).\n\n\nOf course, there are many more rule expressions than those shown in this small\nexample. In the next section, a detailed description of each textX expression is\ngiven.\n\n\ntextX base types\n\u00b6\n\n\nIn the previous example you have seen an \nID\n rule. This rule is one of the\nbuilt-in rules that form the base of textX's type system. Base types/rules are\ndepicted in the following figure:\n\n\n\n\n\n\nID\n rule: matches a common identifier consisting of letters, digits\n  and underscores. The regex pattern that describe this rule is \n'[^\\d\\W]\\w*\\b'\n.\n  This match will be converted to a Python string.\n\n\nINT\n rule: matches an integer number. This match will be converted to\n  a Python \nint\n instance.\n\n\nFLOAT\n rule: will match a floating point number. This match will be converted\n  to a Python \nfloat\n instance.\n\n\nBOOL\n rule: matches the words \ntrue\n or \nfalse\n. This match\n  will be converted to a Python \nbool\n instance.\n\n\nSTRING\n rule: matches a quoted string. This match will be converted\n  to a Python \nstr\n instance.\n\n\n\n\ntextX base types are automatically converted to python types during object\ninstantiation.\nSee \nauto-initialization\n for\nmore information.\n\n\nRule expressions\n\u00b6\n\n\nRule expressions represent the body of a rule. They is specified using basic\nexpressions and operators.\n\n\nThe basic expressions are:\n\n\n\n\nMatches\n\n\nString match (\n'...'\n or \n\"...\"\n)\n\n\nRegex match (\n/.../\n)\n\n\n\n\n\n\nSequence\n\n\nOrdered choice\n (\n|\n)\n\n\nOptional\n (\n?\n)\n\n\nRepetitions\n\n\nZero or more (\n*\n)\n\n\nOne or more (\n+\n)\n\n\nUnordered group (\n#\n)\n\n\n\n\n\n\nReferences\n\n\nMatch reference\n\n\nLink reference (\n[..]\n)\n\n\n\n\n\n\nAssignments\n\n\nPlain (\n=\n)\n\n\nBoolean (\n?=\n)\n\n\nZero or more (\n*=\n)\n\n\nOne or more (\n+=\n)\n\n\n\n\n\n\nSyntactic predicates\n\n\nNot (\n!\n) - negative lookahead\n\n\nAnd (\n&\n) - positive lookahead\n\n\n\n\n\n\nMatch suppression\n\n\n\n\nMatches\n\u00b6\n\n\nMatch expressions are, besides base type rules, the expressions at the lowest\nlevel. They are the basic building blocks for more complex expressions. These\nexpressions will consume input on success.\n\n\nThere are two types of match expressions:\n\n\n\n\n\n\nString match\n - is written as a single quoted string. It will match a literal\n  string on the input.\n\n\nHere are a few examples of string matches:\n\n\n'blue'\n'zero'\n'person'\n\n\n\n\n\n\n\nRegex match\n - uses regular expression defined inside \n/ /\n to match the\n  input. Therefore, it defines a whole class of strings that can be matched.\n  Internally a python \nre\n module is used.\n\n\nHere are few example of regex matches:\n\n\n/\\d*/\n/\\d{3,4}-\\d{3}/\n/[-\\w]*\\b/\n/[^}]*/\n\n\n\n\n\n\n\nFor more information on Regular Expression in Python see \nRegular Expression\n  HOWTO\n.\n\n\nSequence\n\u00b6\n\n\nSequence is a textX expression that is given by just writing contained\nsub-expressions one after another. For example,the following rule:\n\n\nColors:\n  \"red\" \"green\" \"blue\"\n;\n\n\n\nis defined as a sequence consisting of three string matches (\nred\n \ngreen\n and\n\nblue\n). Contained expressions will be matched in the exact order they are\ngiven. If some of the expressions do not match, the sequence as a whole will\nfail. The above rule defined by the sequence will match only the following\nstring:\n\n\nred green blue\n\n\n\n\n\nNote\n\n\nIf whitespace skipping is enabled (it is by default), arbitrary whitespaces\ncan occur between matched words.\n\n\n\n\nOrdered choice\n\u00b6\n\n\nOrdered choice is given as a set of expression separated by the\n|\n operator.\nThis operator will try to match contained expression from left to right and the\nfirst match that succeeds will be used.\n\n\nExample:\n\n\nColor:\n  \"red\" | \"green\" | \"blue\"\n;\n\n\n\nThis will match either \nred\n or \ngreen\n or \nblue\n and the parser will try to\nmatch the expressions in that order.\n\n\n\n\nNote\n\n\nIn most classic parsing technologies an unordered match (alternative) is used.\nThis may lead to ambiguous grammars where multiple parse tree may exist for\nthe same input string.\n\n\n\n\nUnderlying parsing technology of textX\nis \nArpeggio\n which is a parser based\non PEG grammars and thus the \n|\n operator directly translates to Arpeggio's PEG\nordered choice. Using ordered choice yields unambiguous parsing. If the text\nparses there is only one possible parse tree.\n\n\nOptional\n\u00b6\n\n\nOptional\n is an expression that will match the contained expression if that is\npossible, but will not fail otherwise. Thus, optional expression always\nsucceeds.\n\n\nExample:\n\n\nMoveUp:\n  'up' INT?\n;\n\n\n\nINT\n match is optional in this example. This means that the \nup\n keyword is\nrequired, but the following integer may or may not be found.\n\n\nFollowing lines will match:\n\n\nup 45\nup 1\nup\n\n\n\nOptional expressions can be more complex. For example:\n\n\nMoveUp:\n  'up' ( INT | FLOAT )?\n\n\n\nNow, an ordered choice in the parentheses is optional.\n\n\nRepetitions\n\u00b6\n\n\n\n\n\n\nZero or more\n repetition is specified by the \n*\n operator and will match\n  the contained expression zero or more times. Here is an example:\n\n\nColors:\n  (\"red\"|\"green\"|\"blue\")*\n;\n\n\n\nIn this example \nzero or more\n repetition is applied on the \nordered\nchoice\n. In each repeated match one color will be matched, trying from left\nto right. Thus, \nColors\n rule will match color as many times as possible,\nbut will not fail if no color exists in the input string. The following\nwould be matched by the \nColors\n rule:\n\n\nred blue green\n\n\n\nbut also:\n\n\nred blue blue red red green\n\n\n\nor an empty string.\n\n\n\n\n\n\nOne or more\n repetition is specified by \n+\n operator and will match the\n  contained expression one or more times. Thus, everything that is written for\n  \nzero or more\n applies here except that at least one match must be found for\n  this expression to succeed. Here is an above example modified to match at\n  least one color:\n\n\nColors:\n  (\"red\"|\"green\"|\"blue\")+\n;\n\n\n\n\n\n\n\nUnordered group\n is a special kind of a sequence. Syntactically it is\n  similar to a repetition. It is specified by the \n#\n operator and must be\n  applied to sequences. This operator will match each element of the sequence in\n  an arbitrary order:\n\n\nColors:\n  (\"red\" \"green\" \"blue\")#\n;\n\n\n\nFor the previous example all following lines are valid:\n\n\nred blue green\nred green blue\nblue green red\n...\n\n\n\nBut, the following lines are not valid:\n\n\nred blue red green\nblue green\n\n\n\nConsider this example:\n\n\nModifier: \n    (static?='static' final?='final' visibility=Visibility)#\n;\n\nVisibility:\n    'public' | 'private' | 'protected';\n\n\n\nWe want to provide modifiers to the type declarations in our language.\nFurthermore, we want modifiers to be written in any order.\n\n\nThe following lines will match (thanks to \n?=\n operator, only visibility\nmust be specified):\n\n\npublic\npublic static\nfinal protected static\n...\n\n\n\n\n\nNote\n\n\nUnordered group may also have\n\nrepetition modifiers\n defined.\n\n\n\n\n\n\n\n\nAssignments\n\u00b6\n\n\nAssignments are used as a part of the meta-model deduction process. Each\nassignment will result in an attribute of the meta-class created by the rule.\n\n\nEach assignment consists of the LHS (left-hand side) and the RHS (right-hand\nside). The LHS is always a name of the meta-class attribute while the RHS can be\na reference to other rules (either a match or a link reference) or a simple\nmatch (string or regex match). For example:\n\n\nPerson:\n  name=Name ',' surename=Surename ',' age=INT ',' height=INT ';'\n;\n\n\n\nThe \nName\n and \nSurename\n rules referenced in the RHS of the first two\nassignments are not given in this example.\n\n\nThis example describes the rule and meta-class \nPerson\n, that will parse and\ninstantiate the \nPerson\n objects with  these four attributes:\n\n\n\n\nname\n - which will use the rule \nName\n to match the input, it \n  will be a reference to the instance of the \nName\n class,\n\n\nsurename\n - will use \nSurename\n rule to match the input,\n\n\nage\n - will use the built-in type \nINT\n to match a number from the\n  input string. \nage\n will be converted to the python \nint\n type.\n\n\nheight\n - the same as \nage\n, but the matched number will be\n  assigned to the \nheight\n attribute of the \nPerson\n instance.\n\n\n\n\nNotice the comma as the separator between matches and the semicolon match at the\nend of the rule. Those matches must be found in the input but the matched\nstrings will be discarded. They represent a syntactic noise.\n\n\nIf the RHS is one of textX BASETYPEs, then the matched string will be converted\nto some of the plain python types (e.g. \nint\n, \nstring\n, \nboolean\n).\n\n\nIf the RHS is a string or regex match like in this example:\n\n\nColor:\n  color=/\\w+/\n;\n\n\n\nthen the attribute given by the LHS will be set as the string matched by the RHS\nregular expression or string.\n\n\nIf the RHS is a reference to some other rule, then the attribute given by the\nLHS will be set to refer to the object created by the RHS rule.\n\n\nFollowing strings are matched by the \nPerson\n rule from above:\n\n\nPetar, Petrovic, 27, 185;\nJohn, Doe, 34, 178;\n\n\n\nThere are four types of assignments:\n\n\n\n\n\n\nPlain assignment\n (\n=\n) will match its RHS once and assign what is\n  matched to the attribute given by the LHS. The above example uses plain\n  assignments.\n\n\nExamples:\n\n\na=INT\nb=FLOAT\nc=/[a-Z0-9]+/\ndir=Direction\n\n\n\n\n\n\n\nBoolean assignment\n (\n?=\n) will set the attribute to \nTrue\n if\n  the RHS match succeeds and to \nFalse\n otherwise.\n\n\nExamples::\n\n\ncold ?= 'cold'\nnumber_given ?= INT\n\n\n\n\n\n\n\nZero or more assignment\n (\n*=\n) - LHS attribute will be a \nlist\n. This\n  assignment will keep matching the RHS as long as the match succeeds and each\n  matched object will be appended to the attribute. If no match succeeds, the\n  attribute will be an empty list.\n\n\nExamples::\n\n\ncommands*=Command\nnumbers*=INT\n\n\n\n\n\n\n\nOne or more assignment\n (\n+=\n) - same as the previous assignment, but it must match the RHS\n  at least once. If no match succeeds, this assignment does not succeed.\n\n\n\n\n\n\nMultiple assignment to the same attribute\n\u00b6\n\n\ntextX allows for multiple assignments to the same attribute. For example:\n\n\n  MyRule:\n      a=INT b=FLOAT a*=ID\n  ;\n\n\n\nHere \na\n attribute will always be a Python list. The type of \na\n will be\n\nOBJECT\n as the two assignments have declared different types for \na\n (\nINT\n and\n\nID\n).\n\n\nConsider this example:\n\n\n  Method:\n      'func(' (params+=Parameter[','])? ')'\n  ;\n  Parameter:\n      type=ID name=ID | name=ID\n  ;\n\n\n\nIn \nParameter\n rule, the \nname\n attribute assignments are part of different\nordered choice alternatives and thus name will never have more than one value\nand thus should not be a list. The type of \nname\n is consistent in both\nassignments so it will be \nID\n.\n\n\nThe rule of the thumb for multiple assignments is that if there is no\npossibility for an attribute to collect more than one value during parsing it\nwill be a single value object, otherwise it will be a list.\n\n\nReferences\n\u00b6\n\n\nRules can reference each other. References are usually used as a RHS of the\nassignments. There are two types of rule references:\n\n\n\n\n\n\nMatch rule reference\n - will \ncall\n another rule. When instance of the called\n  rule is created, it will be assigned to the attribute on the LHS. We say that the\n  referred object is contained inside the referring object (i.e. they form a\n  \nparent-child relationship\n).\n\n\nExample::\n\n\nStructure:\n  'structure' '{'\n    elements*=StructureElement\n  '}'\n;\n\n\n\nStructureElement\n will be matched zero or more times. With each match, a new\ninstance of the \nStructureElement\n will be created and appended to the \nelements\n\npython list. A \nparent\n attribute of each \nStructureElement\n will be set to\nthe containing \nStructure\n.\n\n\n\n\n\n\nLink rule reference\n - will match an identifier of some class object at the\n  given place and convert that identifier to a python reference to the target\n  object. This reference resolving is done automatically by textX. By default, a\n  \nname\n attribute is used as the identifier of the object. Currently, there is\n  no automatic support for namespaces in textX. All objects of the same class\n  are in a single namespace.\n\n\nExample:\n\n\nScreenType:\n  'screen' name=ID \"{\"\n  '}'\n;\n\nScreenInstance:\n  'screen' type=[ScreenType]\n;\n\n\n\nThe \ntype\n attribute is a link to the \nScreenType\n object. This is a valid\nusage:\n\n\n// This is a definition of the ScreenType object\nscreen Introduction {\n\n}\n\n// And this is a reference link to the ScreenType object defined above\n// ScreenInstance instance\nscreen Introduction\n\n\n\nIntroduction\n will be matched, the \nScreenType\n object with that name will\nbe found and the \ntype\n attribute of \nScreenInstance\n instance will be set to\nit.\n\n\nID\n rule is used by default to match the link identifier. If you want to change\nthat, you can use the following syntax:\n\n\nScreenInstance:\n  'screen' type=[ScreenType|WORD]\n;\n\n\n\nHere, instead of \nID\n a \nWORD\n rule is used to match the object's identifier.\n\n\n\n\n\n\n\n\nNote\n\n\nAttributes with \nname\n names are used for reference auto-resolving. A dict\nlookup is used, thus they must be of a hashable type. See \n\nissue #40\n.\n\n\n\n\nSyntactic predicates\n\u00b6\n\n\nSyntactic predicates are operators that are used to implement lookahead. The\nlookahead is used to do parsing decision based on the part of the input ahead of\nthe current position. Syntactic predicates are written as a prefix of some textX\nrule expression. The rule expression will be used to match input ahead of the\ncurrent location in the input string. It will either fail or succeed but will\nnever consume any input.\n\n\nThere are two type of syntactic predicates:\n\n\n\n\n\n\nNot - negative lookahead\n (\n!\n) - will succeed if the current input doesn't\n  match the expression given after the \n!\n operator.\n\n\nExample problem:\n\n\nExpression: Let | ID | NUMBER;\nLet:\n    'let'\n        expr+=Expression\n    'end'\n;\n\n\n\nIn this example we have nested expressions built with indirectly recurssive\n\nLet\n rule. The problem is that the \nID\n rule from \nExpression\n will match\nkeyword \nend\n and thus will consume end of \nLet\n rule, so the parser will\nhit EOF without completing any \nLet\n rules. To fix this, we can specify that\n\nID\n will match any identifier except keywords \nlet\n and \nend\n like this:\n\n\nExpression: Let | MyID | NUMBER;\nLet:\n    'let'\n        expr+=Expression\n    'end'\n;\nKeyword: 'let' | 'end';\nMyID: !Keyword ID;\n\n\n\nNow, \nMyID\n will match \nID\n only if it is not a keyword.\n\n\n\n\n\n\nAnd - positive lookahead\n (\n&\n) - will succeed if the current input starts\n  with the string matched by the expression given after the \n&\n operator.\n\n\nExample:\n\n\nModel:\n    elements+=Element\n;\nElement:\n    AbeforeB | A | B\n;\nAbeforeB: \n    a='a' &'b'      // this succeeds only if 'b' follows 'a'\n;\nA: a='a';\nB: a='b';\n\n\n\nGiven the input string \na a a b\n first two \na\n chars will be matched by the\nrule \nA\n, but the third \na\n will be matched by the rule \nAbeforeB\n.  So,\neven when \nAbeforeB\n matches only \na\n and is tried before any other match,\nit will not succeed for the first two \na\n chars because they are not\nfollowed by \nb\n.\n\n\n\n\n\n\nMatch suppression\n\u00b6\n\n\nSometimes it is necessary to define match rules that should return only parts of\nthe match. For that we use match the suppression operator (\n-\n) after the\nexpression you want to suppress.\n\n\nFor example:\n\n\nFullyQualifiedID[noskipws]:\n    /\\s*/-\n    QuotedID+['.']\n    /\\s*/-\n;\nQuotedID:\n    '\"'?- ID '\"'?-\n;\n\n\n\nBecause we use \nnoskipws\n rule modifier, \nFullyQualifiedID\n does not skip\nwhitespaces automatically. Thus, we have to match whitespaces ourself, but we\ndon't want those whitespaces in the resulting string. You might wonder why we\nare using \nnoskipws\n. It is because we do not want whitespaces in between each\n\nQuotedID\n match. So, for example, \nfirst. second\n shouldn't match but\n\nfirst.second\n should.\n\n\nIn the rule \nFullyQualifiedID\n we are suppressing whitespace matches \n/\\s*/-\n.\nWe also state in \nQuotedID\n that there are optional quotation marks around each\nID, but we don't want those either \n'\"'?-\n.\n\n\nGiven this input:\n\n\nfirst.\"second\".third.\"fourth\"\n\n\n\nFullyQualifiedID\n will return:\n\n\nfirst.second.third.fourth\n\n\n\nRepetition modifiers\n\u00b6\n\n\nRepetition modifiers are used for the modification of the repetition expressions\n(\n*\n, \n+\n, \n#\n, \n*=\n, \n+=\n). They are specified in brackets \n[ ]\n. If there are\nmore modifiers, they are separated by a comma.\n\n\nCurrently, there are two modifiers defined:\n\n\n\n\n\n\nSeparator modifier\n - is used to define separator on multiple matches.\n  Separator is a simple match (string match or regex match).\n\n\nExample:\n\n\nnumbers*=INT[',']\n\n\n\nHere, a separator string match is defined (\n','\n). This will match zero\nor more integers separated by commas.\n\n\n45, 47, 3, 78\n\n\n\nA regex can also be specified as a separator.\n\n\nfields += ID[/;|,|:/]\n\n\n\nThis will match IDs separated by either \n;\n or \n,\n or \n:\n.\n\n\nfirst, second; third, fourth: fifth\n\n\n\n\n\n\n\nEnd-of-line terminate modifier\n (\neolterm\n) - used to terminate repetition\n  on end-of-line. By default the repetition match will span lines. When this\n  modifier is specified, repetition will work inside the current line only.\n\n\nExample:\n\n\nSTRING*[',', eolterm]\n\n\n\nHere we have a separator as well as the \neolterm\n defined. This will match\nzero or more strings separated by commas inside one line.\n\n\n\"first\", \"second\", \"third\"\n\"fourth\"\n\n\n\nIf we run the example expression once on this string, it will match the\nfirst line only. \n\"fourth\"\n in the second line will not be matched.\n\n\n\n\n\n\n\n\nWarning\n\n\nBe aware that when \neolterm\n modifier is used, its effect starts from the\nprevious match. For example:\n\n\nConditions:\n  'conditions' '{'\n    varNames+=WORD[eolterm]    // match var names until end of line\n  '}'\n\n\n\nIn this example \nvarNames\n must be matched in the same line as\n\nconditions {\n because \neolterm\n effect start immediately.\nIn this example we wanted to give the user the freedom to specify var names on\nthe next line, even to put some empty lines if he/she wishes. In order to do\nthat, we should modify the example like this::\n\n\nConditions:\n  'conditions' '{'\n    /\\s*/\n    varNames+=WORD[eolterm]    // match var names until end of line\n  '}'\n\n\n\nRegex match \n/\\s*/\n will collect whitespaces (spaces and new-lines)\nbefore the \nWORD\n match begins. Afterwards, repeated matches will work\ninside one line only.\n\n\n\n\nRule types\n\u00b6\n\n\nThere are three kinds of rules in textX:\n\n\n\n\nCommon rules (or just rules)\n\n\nAbstract rules\n\n\nMatch rules\n\n\n\n\nCommon rules\n are rules that contain at least one assignment, i.e., they\nhave attributes defined. For example:\n\n\nInitialCommand:\n  'initial' x=INT ',' y=INT\n;\n\n\n\nThis rule has two defined attributes: \nx\n and \ny\n.\n\n\nAbstract rules\n are rules that have no assignments and reference at least one\nabstract or common rule. They are usually given as an ordered choice of other\nrules and they are used to generalize other rules. For example:\n\n\nProgram:\n  'begin'\n    commands*=Command\n  'end'\n;\n\nCommand:\n  MoveCommand | InitialCommand\n;\n\n\n\nIn this example, Python objects in the \ncommands\n list will either contain\ninstances of \nMoveCommand\n or \nInitialCommand\n. \nCommand\n rule is abstract. A\nmeta-class of this rule will never be instantiated. Abstract rule can also be\nused in link rule references:\n\n\nListOfCommands:\n  commands*=[Command][',']\n;\n\n\n\nAbstract rules may reference match rules and base types. For example:\n\n\nValue:\n    STRING | FLOAT | BOOL | Object | Array | \"null\"\n;\n\n\n\nIn this example, the base types as well as the string match \n\"null\"\n are all\nmatch rules, but \nObject\n and \nArray\n are common rules and therefore \nValue\n is\nabstract.\n\n\nAbstract rules can be a complex mix of rule references and match expressions as\nlong as there is at least one abstract or common reference. For example:\n\n\nValue:\n  'id' /\\d+-\\d+/ | FLOAT | Object\n;\n\n\n\nA rule with a single reference to an abstract or common rule is also abstract:\n\n\nValue:\n  OtherRule\n;\n\n\n\nMatch rules\n are rules that have no assignments either direct or indirect,\ni.e. all referenced rules are match rules too. They are usually used to specify\nenumerated values or some complex string matches that can't be done with regular\nexpressions.\n\n\nExamples:\n\n\nWidget:\n  \"edit\"|\"combo\"|\"checkbox\"|\"togglebutton\"\n;\n\nName:\n  STRING|/(\\w|\\+|-)+/\n;\n\nValue:\n  /(\\w|\\+|-)+/ | FLOAT | INT\n;\n\n\n\nThese rules can be used in match references only (i.e., you can't link to these\nrules as they don't exists as objects), and they produce objects of the base\npython types (\nstr\n, \nint\n, \nbool\n, \nfloat\n).\n\n\nAll base type rules (e.g., \nINT\n, \nSTRING\n, \nBASETYPE\n) are match rules.\n\n\nRule modifiers\n\u00b6\n\n\nRule modifiers are used for the modification of the rule's expression. They are\nspecified in brackets (\n[ ]\n) at the beginning of the rule's definition after\nthe rule's name. Currently, they are used to alter parser configuration for\nwhitespace handling on the rule level.\n\n\nThere are two rule modifiers at the moment:\n\n\n\n\n\n\nskipws, noskipws\n - are used to enable/disable whitespace skipping during\n  parsing. This will change the global parser's \nskipws\n setting given during\n  the meta-model instantiation.\n\n\nExample:\n\n\nRule:\n    'entity' name=ID /\\s*/ call=Rule2;\nRule2[noskipws]:\n    'first' 'second';\n\n\n\nIn this example \nRule\n rule will use default parser behaviour set during the\nmeta-model instantiation, while \nRule2\n rule will disable whitespace\nskipping. This will change \nRule2\n to match the word \nfirstsecond\n, but not\nwords \nfirst second\n with whitespaces in between.\n\n\n\n\nNote\n\n\nRemember that whitespace handling modification will start immediately\nafter the previous match. In the above example, additional \n/\\s*/\n is\ngiven before the \nRule2\n call to consume all whitespaces before trying\nto match \nRule2\n.\n\n\n\n\n\n\n\n\nws\n - used to redefine what is considered to be a whitespaces on the rule\n  level. textX by default treats space, tab and new-line as a whitespace\n  characters. This can be changed globally during the meta-model instantiation\n  (see \nWhitespace handling\n) or per rule\n  using this modifier.\n\n\nExample:\n\n\nRule:\n    'entity' name=ID /\\s*/ call=Rule2;\nRule2[ws='\\n']:\n    'first' 'second';\n\n\n\nIn this example \nRule\n will use the default parser behavior but the \nRule2\n\nwill alter the white-space definition to be new-line only. This means that\nthe words \nfirst\n and \nsecond\n will get matched only if they are on separate\nlines or in the same line but without other characters in between (even tabs\nand spaces).\n\n\n\n\nNote\n\n\nAs in the previous example, the modification will start immediately, so\nif you want to consume preceding spaces you must do that explicitly, as\ngiven with \n/\\s*/\n in the :\nRule\n.\n\n\n\n\n\n\n\n\nGrammar comments\n\u00b6\n\n\nSyntax for comments inside a grammar is \n//\n for line comments and \n/* ... */\n\nfor block comments.\n\n\nLanguage comments\n\u00b6\n\n\nTo support comments in your DSL use a special grammar rule \nComment\n. textX will\ntry to match this rule in between each other normal grammar match (similarly to\nthe whitespace matching). If the match succeeds, the matched content will be\ndiscarded.\n\n\nFor example, in the \nrobot language example\n comments are\ndefined like this:\n\n\nComment:\n  /\\/\\/.*$/\n;\n\n\n\nWhich states that everything starting with \n//\n and continuing until the end of\nline is a comment.\n\n\nGrammar modularization\n\u00b6\n\n\nGrammars can be defined in multiple files and than imported. Rules used in the\nreferences are first searched for in the current file and then in the imported\nfiles, in the order of the import.\n\n\nExample:\n\n\nimport scheme\n\nLibrary:\n  'library' name=Name '{'\n    attributes*=LibraryAttribute\n\n    scheme=Scheme\n\n  '}'\n;\n\n\n\nScheme\n rule is defined in \nscheme.tx\n grammar file imported at the beginning.\n\n\nGrammar files may be located in folders. In that case, dot notation is used.\n\n\nExample:\n\n\nimport component.types\n\n\n\ntypes.tx\n grammar is located in the \ncomponent\n folder relatively to the\ncurrent grammar file.\n\n\nIf you want to override the default search order, you can specify a fully\nqualified name of the rule using dot notation when giving the name of the\nreferring object.\n\n\nExample:\n\n\nimport component.types\n\nMyRule:\n  a = component.types.List\n;\n\nList:\n  '[' values+=BASETYPE[','] ']'\n;\n\n\n\nList\n from \ncomponent.types\n is matched/instantiated and set to \na\n attribute.",
            "title": "Grammar"
        },
        {
            "location": "/grammar/#textx-grammar",
            "text": "The language syntax and the meta-model are defined by the textX grammar in the\nform of a set of textX rules.",
            "title": "textX grammar"
        },
        {
            "location": "/grammar/#rules",
            "text": "The basic building blocks of the textX language are rules. Each rule is written\nin the following form:  Hello:\n  'hello' who=ID\n;  This rule is called  Hello . After the rule name, there is a colon. The body of\nthe rule is given as a textX expression, starting at the colon and ending with a\nsemicolon. This rule tells us that the pattern of  Hello  objects in input\nstrings consists of the string literal  hello , followed by the ID rule (here ID\nis a reference to a built-in rule, more about this in a moment).  These are valid  Hello  objects:  hello Alice\nhello Bob\nhello foo1234  Rule  Hello  at the same time defines a Python class  Hello . When the rule is\nrecognized in the input stream, an object of this class will get created and the\nattribute  who  will be set to whatever the rule  ID  has matched after the word hello  (this is specified by the assignment  who=ID ).  Of course, there are many more rule expressions than those shown in this small\nexample. In the next section, a detailed description of each textX expression is\ngiven.",
            "title": "Rules"
        },
        {
            "location": "/grammar/#textx-base-types",
            "text": "In the previous example you have seen an  ID  rule. This rule is one of the\nbuilt-in rules that form the base of textX's type system. Base types/rules are\ndepicted in the following figure:    ID  rule: matches a common identifier consisting of letters, digits\n  and underscores. The regex pattern that describe this rule is  '[^\\d\\W]\\w*\\b' .\n  This match will be converted to a Python string.  INT  rule: matches an integer number. This match will be converted to\n  a Python  int  instance.  FLOAT  rule: will match a floating point number. This match will be converted\n  to a Python  float  instance.  BOOL  rule: matches the words  true  or  false . This match\n  will be converted to a Python  bool  instance.  STRING  rule: matches a quoted string. This match will be converted\n  to a Python  str  instance.   textX base types are automatically converted to python types during object\ninstantiation.\nSee  auto-initialization  for\nmore information.",
            "title": "textX base types"
        },
        {
            "location": "/grammar/#rule-expressions",
            "text": "Rule expressions represent the body of a rule. They is specified using basic\nexpressions and operators.  The basic expressions are:   Matches  String match ( '...'  or  \"...\" )  Regex match ( /.../ )    Sequence  Ordered choice  ( | )  Optional  ( ? )  Repetitions  Zero or more ( * )  One or more ( + )  Unordered group ( # )    References  Match reference  Link reference ( [..] )    Assignments  Plain ( = )  Boolean ( ?= )  Zero or more ( *= )  One or more ( += )    Syntactic predicates  Not ( ! ) - negative lookahead  And ( & ) - positive lookahead    Match suppression",
            "title": "Rule expressions"
        },
        {
            "location": "/grammar/#matches",
            "text": "Match expressions are, besides base type rules, the expressions at the lowest\nlevel. They are the basic building blocks for more complex expressions. These\nexpressions will consume input on success.  There are two types of match expressions:    String match  - is written as a single quoted string. It will match a literal\n  string on the input.  Here are a few examples of string matches:  'blue'\n'zero'\n'person'    Regex match  - uses regular expression defined inside  / /  to match the\n  input. Therefore, it defines a whole class of strings that can be matched.\n  Internally a python  re  module is used.  Here are few example of regex matches:  /\\d*/\n/\\d{3,4}-\\d{3}/\n/[-\\w]*\\b/\n/[^}]*/    For more information on Regular Expression in Python see  Regular Expression\n  HOWTO .",
            "title": "Matches"
        },
        {
            "location": "/grammar/#sequence",
            "text": "Sequence is a textX expression that is given by just writing contained\nsub-expressions one after another. For example,the following rule:  Colors:\n  \"red\" \"green\" \"blue\"\n;  is defined as a sequence consisting of three string matches ( red   green  and blue ). Contained expressions will be matched in the exact order they are\ngiven. If some of the expressions do not match, the sequence as a whole will\nfail. The above rule defined by the sequence will match only the following\nstring:  red green blue   Note  If whitespace skipping is enabled (it is by default), arbitrary whitespaces\ncan occur between matched words.",
            "title": "Sequence"
        },
        {
            "location": "/grammar/#ordered-choice",
            "text": "Ordered choice is given as a set of expression separated by the |  operator.\nThis operator will try to match contained expression from left to right and the\nfirst match that succeeds will be used.  Example:  Color:\n  \"red\" | \"green\" | \"blue\"\n;  This will match either  red  or  green  or  blue  and the parser will try to\nmatch the expressions in that order.   Note  In most classic parsing technologies an unordered match (alternative) is used.\nThis may lead to ambiguous grammars where multiple parse tree may exist for\nthe same input string.   Underlying parsing technology of textX\nis  Arpeggio  which is a parser based\non PEG grammars and thus the  |  operator directly translates to Arpeggio's PEG\nordered choice. Using ordered choice yields unambiguous parsing. If the text\nparses there is only one possible parse tree.",
            "title": "Ordered choice"
        },
        {
            "location": "/grammar/#optional",
            "text": "Optional  is an expression that will match the contained expression if that is\npossible, but will not fail otherwise. Thus, optional expression always\nsucceeds.  Example:  MoveUp:\n  'up' INT?\n;  INT  match is optional in this example. This means that the  up  keyword is\nrequired, but the following integer may or may not be found.  Following lines will match:  up 45\nup 1\nup  Optional expressions can be more complex. For example:  MoveUp:\n  'up' ( INT | FLOAT )?  Now, an ordered choice in the parentheses is optional.",
            "title": "Optional"
        },
        {
            "location": "/grammar/#repetitions",
            "text": "Zero or more  repetition is specified by the  *  operator and will match\n  the contained expression zero or more times. Here is an example:  Colors:\n  (\"red\"|\"green\"|\"blue\")*\n;  In this example  zero or more  repetition is applied on the  ordered\nchoice . In each repeated match one color will be matched, trying from left\nto right. Thus,  Colors  rule will match color as many times as possible,\nbut will not fail if no color exists in the input string. The following\nwould be matched by the  Colors  rule:  red blue green  but also:  red blue blue red red green  or an empty string.    One or more  repetition is specified by  +  operator and will match the\n  contained expression one or more times. Thus, everything that is written for\n   zero or more  applies here except that at least one match must be found for\n  this expression to succeed. Here is an above example modified to match at\n  least one color:  Colors:\n  (\"red\"|\"green\"|\"blue\")+\n;    Unordered group  is a special kind of a sequence. Syntactically it is\n  similar to a repetition. It is specified by the  #  operator and must be\n  applied to sequences. This operator will match each element of the sequence in\n  an arbitrary order:  Colors:\n  (\"red\" \"green\" \"blue\")#\n;  For the previous example all following lines are valid:  red blue green\nred green blue\nblue green red\n...  But, the following lines are not valid:  red blue red green\nblue green  Consider this example:  Modifier: \n    (static?='static' final?='final' visibility=Visibility)#\n;\n\nVisibility:\n    'public' | 'private' | 'protected';  We want to provide modifiers to the type declarations in our language.\nFurthermore, we want modifiers to be written in any order.  The following lines will match (thanks to  ?=  operator, only visibility\nmust be specified):  public\npublic static\nfinal protected static\n...   Note  Unordered group may also have repetition modifiers  defined.",
            "title": "Repetitions"
        },
        {
            "location": "/grammar/#assignments",
            "text": "Assignments are used as a part of the meta-model deduction process. Each\nassignment will result in an attribute of the meta-class created by the rule.  Each assignment consists of the LHS (left-hand side) and the RHS (right-hand\nside). The LHS is always a name of the meta-class attribute while the RHS can be\na reference to other rules (either a match or a link reference) or a simple\nmatch (string or regex match). For example:  Person:\n  name=Name ',' surename=Surename ',' age=INT ',' height=INT ';'\n;  The  Name  and  Surename  rules referenced in the RHS of the first two\nassignments are not given in this example.  This example describes the rule and meta-class  Person , that will parse and\ninstantiate the  Person  objects with  these four attributes:   name  - which will use the rule  Name  to match the input, it \n  will be a reference to the instance of the  Name  class,  surename  - will use  Surename  rule to match the input,  age  - will use the built-in type  INT  to match a number from the\n  input string.  age  will be converted to the python  int  type.  height  - the same as  age , but the matched number will be\n  assigned to the  height  attribute of the  Person  instance.   Notice the comma as the separator between matches and the semicolon match at the\nend of the rule. Those matches must be found in the input but the matched\nstrings will be discarded. They represent a syntactic noise.  If the RHS is one of textX BASETYPEs, then the matched string will be converted\nto some of the plain python types (e.g.  int ,  string ,  boolean ).  If the RHS is a string or regex match like in this example:  Color:\n  color=/\\w+/\n;  then the attribute given by the LHS will be set as the string matched by the RHS\nregular expression or string.  If the RHS is a reference to some other rule, then the attribute given by the\nLHS will be set to refer to the object created by the RHS rule.  Following strings are matched by the  Person  rule from above:  Petar, Petrovic, 27, 185;\nJohn, Doe, 34, 178;  There are four types of assignments:    Plain assignment  ( = ) will match its RHS once and assign what is\n  matched to the attribute given by the LHS. The above example uses plain\n  assignments.  Examples:  a=INT\nb=FLOAT\nc=/[a-Z0-9]+/\ndir=Direction    Boolean assignment  ( ?= ) will set the attribute to  True  if\n  the RHS match succeeds and to  False  otherwise.  Examples::  cold ?= 'cold'\nnumber_given ?= INT    Zero or more assignment  ( *= ) - LHS attribute will be a  list . This\n  assignment will keep matching the RHS as long as the match succeeds and each\n  matched object will be appended to the attribute. If no match succeeds, the\n  attribute will be an empty list.  Examples::  commands*=Command\nnumbers*=INT    One or more assignment  ( += ) - same as the previous assignment, but it must match the RHS\n  at least once. If no match succeeds, this assignment does not succeed.",
            "title": "Assignments"
        },
        {
            "location": "/grammar/#multiple-assignment-to-the-same-attribute",
            "text": "textX allows for multiple assignments to the same attribute. For example:    MyRule:\n      a=INT b=FLOAT a*=ID\n  ;  Here  a  attribute will always be a Python list. The type of  a  will be OBJECT  as the two assignments have declared different types for  a  ( INT  and ID ).  Consider this example:    Method:\n      'func(' (params+=Parameter[','])? ')'\n  ;\n  Parameter:\n      type=ID name=ID | name=ID\n  ;  In  Parameter  rule, the  name  attribute assignments are part of different\nordered choice alternatives and thus name will never have more than one value\nand thus should not be a list. The type of  name  is consistent in both\nassignments so it will be  ID .  The rule of the thumb for multiple assignments is that if there is no\npossibility for an attribute to collect more than one value during parsing it\nwill be a single value object, otherwise it will be a list.",
            "title": "Multiple assignment to the same attribute"
        },
        {
            "location": "/grammar/#references",
            "text": "Rules can reference each other. References are usually used as a RHS of the\nassignments. There are two types of rule references:    Match rule reference  - will  call  another rule. When instance of the called\n  rule is created, it will be assigned to the attribute on the LHS. We say that the\n  referred object is contained inside the referring object (i.e. they form a\n   parent-child relationship ).  Example::  Structure:\n  'structure' '{'\n    elements*=StructureElement\n  '}'\n;  StructureElement  will be matched zero or more times. With each match, a new\ninstance of the  StructureElement  will be created and appended to the  elements \npython list. A  parent  attribute of each  StructureElement  will be set to\nthe containing  Structure .    Link rule reference  - will match an identifier of some class object at the\n  given place and convert that identifier to a python reference to the target\n  object. This reference resolving is done automatically by textX. By default, a\n   name  attribute is used as the identifier of the object. Currently, there is\n  no automatic support for namespaces in textX. All objects of the same class\n  are in a single namespace.  Example:  ScreenType:\n  'screen' name=ID \"{\"\n  '}'\n;\n\nScreenInstance:\n  'screen' type=[ScreenType]\n;  The  type  attribute is a link to the  ScreenType  object. This is a valid\nusage:  // This is a definition of the ScreenType object\nscreen Introduction {\n\n}\n\n// And this is a reference link to the ScreenType object defined above\n// ScreenInstance instance\nscreen Introduction  Introduction  will be matched, the  ScreenType  object with that name will\nbe found and the  type  attribute of  ScreenInstance  instance will be set to\nit.  ID  rule is used by default to match the link identifier. If you want to change\nthat, you can use the following syntax:  ScreenInstance:\n  'screen' type=[ScreenType|WORD]\n;  Here, instead of  ID  a  WORD  rule is used to match the object's identifier.     Note  Attributes with  name  names are used for reference auto-resolving. A dict\nlookup is used, thus they must be of a hashable type. See  issue #40 .",
            "title": "References"
        },
        {
            "location": "/grammar/#syntactic-predicates",
            "text": "Syntactic predicates are operators that are used to implement lookahead. The\nlookahead is used to do parsing decision based on the part of the input ahead of\nthe current position. Syntactic predicates are written as a prefix of some textX\nrule expression. The rule expression will be used to match input ahead of the\ncurrent location in the input string. It will either fail or succeed but will\nnever consume any input.  There are two type of syntactic predicates:    Not - negative lookahead  ( ! ) - will succeed if the current input doesn't\n  match the expression given after the  !  operator.  Example problem:  Expression: Let | ID | NUMBER;\nLet:\n    'let'\n        expr+=Expression\n    'end'\n;  In this example we have nested expressions built with indirectly recurssive Let  rule. The problem is that the  ID  rule from  Expression  will match\nkeyword  end  and thus will consume end of  Let  rule, so the parser will\nhit EOF without completing any  Let  rules. To fix this, we can specify that ID  will match any identifier except keywords  let  and  end  like this:  Expression: Let | MyID | NUMBER;\nLet:\n    'let'\n        expr+=Expression\n    'end'\n;\nKeyword: 'let' | 'end';\nMyID: !Keyword ID;  Now,  MyID  will match  ID  only if it is not a keyword.    And - positive lookahead  ( & ) - will succeed if the current input starts\n  with the string matched by the expression given after the  &  operator.  Example:  Model:\n    elements+=Element\n;\nElement:\n    AbeforeB | A | B\n;\nAbeforeB: \n    a='a' &'b'      // this succeeds only if 'b' follows 'a'\n;\nA: a='a';\nB: a='b';  Given the input string  a a a b  first two  a  chars will be matched by the\nrule  A , but the third  a  will be matched by the rule  AbeforeB .  So,\neven when  AbeforeB  matches only  a  and is tried before any other match,\nit will not succeed for the first two  a  chars because they are not\nfollowed by  b .",
            "title": "Syntactic predicates"
        },
        {
            "location": "/grammar/#match-suppression",
            "text": "Sometimes it is necessary to define match rules that should return only parts of\nthe match. For that we use match the suppression operator ( - ) after the\nexpression you want to suppress.  For example:  FullyQualifiedID[noskipws]:\n    /\\s*/-\n    QuotedID+['.']\n    /\\s*/-\n;\nQuotedID:\n    '\"'?- ID '\"'?-\n;  Because we use  noskipws  rule modifier,  FullyQualifiedID  does not skip\nwhitespaces automatically. Thus, we have to match whitespaces ourself, but we\ndon't want those whitespaces in the resulting string. You might wonder why we\nare using  noskipws . It is because we do not want whitespaces in between each QuotedID  match. So, for example,  first. second  shouldn't match but first.second  should.  In the rule  FullyQualifiedID  we are suppressing whitespace matches  /\\s*/- .\nWe also state in  QuotedID  that there are optional quotation marks around each\nID, but we don't want those either  '\"'?- .  Given this input:  first.\"second\".third.\"fourth\"  FullyQualifiedID  will return:  first.second.third.fourth",
            "title": "Match suppression"
        },
        {
            "location": "/grammar/#repetition-modifiers",
            "text": "Repetition modifiers are used for the modification of the repetition expressions\n( * ,  + ,  # ,  *= ,  += ). They are specified in brackets  [ ] . If there are\nmore modifiers, they are separated by a comma.  Currently, there are two modifiers defined:    Separator modifier  - is used to define separator on multiple matches.\n  Separator is a simple match (string match or regex match).  Example:  numbers*=INT[',']  Here, a separator string match is defined ( ',' ). This will match zero\nor more integers separated by commas.  45, 47, 3, 78  A regex can also be specified as a separator.  fields += ID[/;|,|:/]  This will match IDs separated by either  ;  or  ,  or  : .  first, second; third, fourth: fifth    End-of-line terminate modifier  ( eolterm ) - used to terminate repetition\n  on end-of-line. By default the repetition match will span lines. When this\n  modifier is specified, repetition will work inside the current line only.  Example:  STRING*[',', eolterm]  Here we have a separator as well as the  eolterm  defined. This will match\nzero or more strings separated by commas inside one line.  \"first\", \"second\", \"third\"\n\"fourth\"  If we run the example expression once on this string, it will match the\nfirst line only.  \"fourth\"  in the second line will not be matched.     Warning  Be aware that when  eolterm  modifier is used, its effect starts from the\nprevious match. For example:  Conditions:\n  'conditions' '{'\n    varNames+=WORD[eolterm]    // match var names until end of line\n  '}'  In this example  varNames  must be matched in the same line as conditions {  because  eolterm  effect start immediately.\nIn this example we wanted to give the user the freedom to specify var names on\nthe next line, even to put some empty lines if he/she wishes. In order to do\nthat, we should modify the example like this::  Conditions:\n  'conditions' '{'\n    /\\s*/\n    varNames+=WORD[eolterm]    // match var names until end of line\n  '}'  Regex match  /\\s*/  will collect whitespaces (spaces and new-lines)\nbefore the  WORD  match begins. Afterwards, repeated matches will work\ninside one line only.",
            "title": "Repetition modifiers"
        },
        {
            "location": "/grammar/#rule-types",
            "text": "There are three kinds of rules in textX:   Common rules (or just rules)  Abstract rules  Match rules   Common rules  are rules that contain at least one assignment, i.e., they\nhave attributes defined. For example:  InitialCommand:\n  'initial' x=INT ',' y=INT\n;  This rule has two defined attributes:  x  and  y .  Abstract rules  are rules that have no assignments and reference at least one\nabstract or common rule. They are usually given as an ordered choice of other\nrules and they are used to generalize other rules. For example:  Program:\n  'begin'\n    commands*=Command\n  'end'\n;\n\nCommand:\n  MoveCommand | InitialCommand\n;  In this example, Python objects in the  commands  list will either contain\ninstances of  MoveCommand  or  InitialCommand .  Command  rule is abstract. A\nmeta-class of this rule will never be instantiated. Abstract rule can also be\nused in link rule references:  ListOfCommands:\n  commands*=[Command][',']\n;  Abstract rules may reference match rules and base types. For example:  Value:\n    STRING | FLOAT | BOOL | Object | Array | \"null\"\n;  In this example, the base types as well as the string match  \"null\"  are all\nmatch rules, but  Object  and  Array  are common rules and therefore  Value  is\nabstract.  Abstract rules can be a complex mix of rule references and match expressions as\nlong as there is at least one abstract or common reference. For example:  Value:\n  'id' /\\d+-\\d+/ | FLOAT | Object\n;  A rule with a single reference to an abstract or common rule is also abstract:  Value:\n  OtherRule\n;  Match rules  are rules that have no assignments either direct or indirect,\ni.e. all referenced rules are match rules too. They are usually used to specify\nenumerated values or some complex string matches that can't be done with regular\nexpressions.  Examples:  Widget:\n  \"edit\"|\"combo\"|\"checkbox\"|\"togglebutton\"\n;\n\nName:\n  STRING|/(\\w|\\+|-)+/\n;\n\nValue:\n  /(\\w|\\+|-)+/ | FLOAT | INT\n;  These rules can be used in match references only (i.e., you can't link to these\nrules as they don't exists as objects), and they produce objects of the base\npython types ( str ,  int ,  bool ,  float ).  All base type rules (e.g.,  INT ,  STRING ,  BASETYPE ) are match rules.",
            "title": "Rule types"
        },
        {
            "location": "/grammar/#rule-modifiers",
            "text": "Rule modifiers are used for the modification of the rule's expression. They are\nspecified in brackets ( [ ] ) at the beginning of the rule's definition after\nthe rule's name. Currently, they are used to alter parser configuration for\nwhitespace handling on the rule level.  There are two rule modifiers at the moment:    skipws, noskipws  - are used to enable/disable whitespace skipping during\n  parsing. This will change the global parser's  skipws  setting given during\n  the meta-model instantiation.  Example:  Rule:\n    'entity' name=ID /\\s*/ call=Rule2;\nRule2[noskipws]:\n    'first' 'second';  In this example  Rule  rule will use default parser behaviour set during the\nmeta-model instantiation, while  Rule2  rule will disable whitespace\nskipping. This will change  Rule2  to match the word  firstsecond , but not\nwords  first second  with whitespaces in between.   Note  Remember that whitespace handling modification will start immediately\nafter the previous match. In the above example, additional  /\\s*/  is\ngiven before the  Rule2  call to consume all whitespaces before trying\nto match  Rule2 .     ws  - used to redefine what is considered to be a whitespaces on the rule\n  level. textX by default treats space, tab and new-line as a whitespace\n  characters. This can be changed globally during the meta-model instantiation\n  (see  Whitespace handling ) or per rule\n  using this modifier.  Example:  Rule:\n    'entity' name=ID /\\s*/ call=Rule2;\nRule2[ws='\\n']:\n    'first' 'second';  In this example  Rule  will use the default parser behavior but the  Rule2 \nwill alter the white-space definition to be new-line only. This means that\nthe words  first  and  second  will get matched only if they are on separate\nlines or in the same line but without other characters in between (even tabs\nand spaces).   Note  As in the previous example, the modification will start immediately, so\nif you want to consume preceding spaces you must do that explicitly, as\ngiven with  /\\s*/  in the : Rule .",
            "title": "Rule modifiers"
        },
        {
            "location": "/grammar/#grammar-comments",
            "text": "Syntax for comments inside a grammar is  //  for line comments and  /* ... */ \nfor block comments.",
            "title": "Grammar comments"
        },
        {
            "location": "/grammar/#language-comments",
            "text": "To support comments in your DSL use a special grammar rule  Comment . textX will\ntry to match this rule in between each other normal grammar match (similarly to\nthe whitespace matching). If the match succeeds, the matched content will be\ndiscarded.  For example, in the  robot language example  comments are\ndefined like this:  Comment:\n  /\\/\\/.*$/\n;  Which states that everything starting with  //  and continuing until the end of\nline is a comment.",
            "title": "Language comments"
        },
        {
            "location": "/grammar/#grammar-modularization",
            "text": "Grammars can be defined in multiple files and than imported. Rules used in the\nreferences are first searched for in the current file and then in the imported\nfiles, in the order of the import.  Example:  import scheme\n\nLibrary:\n  'library' name=Name '{'\n    attributes*=LibraryAttribute\n\n    scheme=Scheme\n\n  '}'\n;  Scheme  rule is defined in  scheme.tx  grammar file imported at the beginning.  Grammar files may be located in folders. In that case, dot notation is used.  Example:  import component.types  types.tx  grammar is located in the  component  folder relatively to the\ncurrent grammar file.  If you want to override the default search order, you can specify a fully\nqualified name of the rule using dot notation when giving the name of the\nreferring object.  Example:  import component.types\n\nMyRule:\n  a = component.types.List\n;\n\nList:\n  '[' values+=BASETYPE[','] ']'\n;  List  from  component.types  is matched/instantiated and set to  a  attribute.",
            "title": "Grammar modularization"
        },
        {
            "location": "/metamodel/",
            "text": "textX meta-models\n\u00b6\n\n\ntextX meta-model is a Python object that knows about all classes that can be\ninstantiated while parsing the input. A meta-model is built from the grammar by\nthe functions \nmetamodel_from_file\n or \nmetamodel_from_str\n in the\n\ntextx.metamodel\n module.\n\n\nfrom textx import metamodel_from_file\nmy_metamodel = metamodel_from_file('my_grammar.tx')\n\n\n\nEach rule from the grammar will result in a Python class kept in the meta-model.\nBesides, meta-model knows how to parse the input strings and convert them\nto \nmodel\n.\n\n\nParsing the input and creating the model is done by \nmodel_from_file\n and\n\nmodel_from_str\n methods of the meta-model object:\n\n\nmy_model = my_metamodel.model_from_file('some_input.md')\n\n\n\nCustom classes\n\u00b6\n\n\nFor each grammar rule a Python class with the same name is created dynamically.\nThese classes are instantiated during the parsing of the input string/file to\ncreate a graph of python objects, a.k.a. \nmodel\n or Abstract-Syntax Tree (AST).\n\n\nMost of the time dynamically created classes will be sufficient, but sometimes\nyou will want to use your own classes instead. To do so use parameter \nclasses\n\nduring the meta-model instantiation. This parameter is a list of your classes\nthat should be named the same as the rules from the grammar which they\nrepresent.\n\n\nfrom textx import metamodel_from_str\n\ngrammar = '''\nEntityModel:\n  entities+=Entity    // each model has one or more entities\n;\n\nEntity:\n  'entity' name=ID '{'\n    attributes+=Attribute     // each entity has one or more attributes\n  '}'\n;\n\nAttribute:\n  name=ID ':' type=[Entity]   // type is a reference to an entity. There are\n                              // built-in entities registered on the meta-model\n                              // for primitive types (integer, string)\n;\n'''\n\nclass Entity(object):\n  def __init__(self, parent, name, attributes):\n    self.parent = parent\n    self.name = name\n    self.attributes = attributes\n\n\n# Use our Entity class. \"Attribute\" class will be created dynamically.\nentity_mm = metamodel_from_str(grammar, classes=[Entity])\n\n\n\nNow \nentity_mm\n can be used to parse the input models where our \nEntity\n class\nwill be instantiated to represent each \nEntity\n rule from the grammar.\n\n\n\n\nNote\n\n\nConstructor of the user-defined classes should accept all attributes defined\nby the corresponding rule from the grammar. In the previous example, we have\nprovided \nname\n and \nattributes\n attributes from the \nEntity\n rule. If the\nclass is a child in a parent-child relationship (see the next section), then\nthe \nparent\n constructor parameter should also be given.\n\n\n\n\nParent-child relationships\n\u00b6\n\n\nThere is often an intrinsic parent-child relationship between object in the\nmodel. In the previous example, each \nAttribute\n instance will always be a child\nof some \nEntity\n object.\n\n\ntextX gives automatic support for these relationships by providing the \nparent\n\nattribute on each child object.\n\n\nWhen you navigate \nmodel\n each child instance will have a \nparent\n\nattribute.\n\n\n\n\nNote\n\n\nAlways provide the parent parameter in user-defined classes for each class\nthat is a child in a parent-child relationship.\n\n\n\n\nProcessors\n\u00b6\n\n\nTo specify static semantics of the language textX uses a concept called\n\nprocessor\n. Processors are python callables that can modify the model\nelements during model parsing/instantiation or do some additional checks that\nare not possible to do by the grammar alone.\n\n\nThere are two types of processors:\n\n\n\n\nmodel processors\n - are callables that are called at the end of the parsing\n  when the whole model is instantiated. These processors accept the model \n  and meta-model as parameters.\n\n\nobject processors\n - are registered for particular classes (grammar rules)\n  and are called when the objects of the given class is instantiated.\n\n\n\n\nProcessors can modify model/objects or raise exception (\nTextXSemanticError\n) if\nsome constraint is not met. User code that calls the model instantiation/parsing\ncan catch and handle this exception.\n\n\nModel processors\n\u00b6\n\n\nTo register a model processor call \nregister_model_processor\n on the meta-model\ninstance.\n\n\nfrom textx import metamodel_from_file\n\n# Model processor is a callable that will accept meta-model and model as its\n# parameters.\ndef check_some_semantics(model, metamodel):\n  ...\n  ... Do some check on the model and raise TextXSemanticError if the semantics\n  ... rules are violated.\n\nmy_metamodel = metamodel_from_file('mygrammar.tx')\n\n# Register model processor on the meta-model instance\nmy_metamodel.register_model_processor(check_some_semantics)\n\n# Parse the model. check_some_semantics will be called automatically after\n# a successful parse to do further checks. If the rules are not met,\n# an instance of TextXSemanticError will be raised.\nmy_metamodel.model_from_file('some_model.ext')\n\n\n\nObject processors\n\u00b6\n\n\nThe purpose of the object processors is to validate or alter the object being\nconstructed. They are registered per class/rule.\n\n\nLet's do some additional checks for the above Entity DSL example.\n\n\ndef entity_obj_processor(entity):\n  '''\n  Check that Ethe ntity names are capitalized. This could also be specified\n  in the grammar using regex match but we will do that check here just\n  as an example.\n  '''\n\n  if entity.name != entity.name.capitalize():\n    raise TextXSemanticError('Entity name \"%s\" must be capitalized.' %\n                            entity.name)\n\ndef attribute_obj_processor(attribute):\n  '''\n  Obj. processors can also introduce changes in the objects they process.\n  Here we set \"primitive\" attribute based on the Entity they refer to.\n  '''\n  attribute.primitive = attribute.type.name in ['integer', 'string']\n\n\n# Object processors are registered by defining a map between a rule name\n# and the callable that will process the instances of that rule/class.\nobj_processors = {\n    'Entity': entity_obj_processor,\n    'Attribute': attribute_obj_processor,\n    }\n\n# This map/dict is registered on a meta-model by the \"register_obj_processors\"\n# call.\nentity_mm.register_obj_processors(obj_processors)\n\n# Parse the model. At each successful parse of Entity or Attribute, the registered\n# processor will be called and the semantics error will be raised if the\n# check does not pass.\nentity_mm.model_from_file('my_entity_model.ent')\n\n\n\nFor another example of the usage of an object processor that modifies the\nobjects, see object processor\n\nmove_command_processor\n \nrobot example\n.\n\n\nIf object processor returns a value that value will be used instead of the\noriginal object. This can be used to implement e.g. expression evaluators or\non-the-fly model interpretation. For more information\n\n\n\n\nNote\n\n\nFor more information please take a look at \nthe tests\n.\n\n\n\n\nBuilt-in objects\n\u00b6\n\n\nOften, you will need objects that should be a part of each model and you do not\nwant users to specify them in every model they create. The most notable example\nare primitive types (e.g. \ninteger\n, \nstring\n, \nbool\n).\n\n\nLet's provide \ninteger\n and \nstring\n Entities to our \nEntity\n meta-model in\norder to simplify the model creation so that the users can use the names of\nthese two entities as the \nAttribute\n types.\n\n\nclass Entity(object):\n    def __init__(self, parent, name, attributes):\n        self.parent = parent\n        self.name = name\n        self.attributes = attributes\n\nentity_builtins = {\n        'integer': Entity(None, 'integer', []),\n        'string': Entity(None, 'string', [])\n}\nentity_mm = metamodel_from_file(\n  'entity.tx',\n  classes=[Entity]            # Register Entity user class,\n  builtins=entity_builtins    # Register integer and string built-in objs\n)\n\n\n\nNow an \ninteger\n and \nstring\n \nAttribute\n types can be used.\nSee \nmodel\n\nand\n\nEntitiy\n\nexample for more.\n\n\nCreating your own base type\n\u00b6\n\n\nMatch rules\n by default return Python \nstring\n type.\nBuilt-in match rules (i.e. \nBASETYPEs\n) on the other hand return Python base\ntypes.\n\n\nYou can use object processors to create your type by specifying match rule in\nthe grammar and object processor for that rule that will create an object of a\nproper Python type.\n\n\nExample:\n\n\nModel:\n  'begin' some_number=MyFloat 'end'\n;\nMyFloat:\n  /\\d+\\.(\\d+)?/\n;\n\n\n\n\nIn this example \nMyFloat\n rule is a match rule and by default will return Python\n\nstring\n, so attribute \nsome_number\n will be of \nstring\n type. To change that,\nregister object processor for \nMyFloat\n rule:\n\n\nmm = metamodel_from_str(grammar)\nmm.register_obj_processors({'MyFloat': lambda x: float(x)}))\n\n\n\n\nNow, \nMyFloat\n will always be converted to Python \nfloat\n type.\n\n\nUsing match filters you can override built-in rule's conversions like this:\n\n\nModel:\n  some_float=INT\n;\n\n\n\n\nmm = metamodel_from_str(grammar)\nmm.register_obj_processors({'INT': lambda x: float(x)}))\n\n\n\n\nIn this example we use built-in rule \nINT\n that returns Python \nint\n type.\nRegistering object processor with the key \nINT\n we can change default behaviour\nand convert what is matched by this rule to some other object (\nfloat\n in this\ncase).\n\n\nAuto-initialization of the attributes\n\u00b6\n\n\nEach object that is recognized in the input string will be instantiated and its\nattributes will be set to the values parsed from the input. In the event that a\ndefined attribute is optional, it will nevertheless be created on the instance\nand set to the default value.\n\n\nHere is a list of the default values for each base textX type:\n\n\n\n\nID - empty string - ''\n\n\nINT - int - 0\n\n\nFLOAT - float - 0.0\n\n\nBOOL - bool - False\n\n\nSTRING - empty string - ''\n\n\n\n\nEach attribute with zero or more multiplicity (\n*=\n) that does not match any\nobject from the input will be initialized to an empty list.\n\n\nAn attribute declared with one or more multiplicity (\n+=\n) must match at least\none object from the input and will therefore be transformed to python list\ncontaining all matched objects.\n\n\nThe drawback of this auto-initialization system is that we can't be sure if the\nattribute was missing from the input or was matched, but the given value was the\nsame as the default value.\n\n\nIn some applications it is important to distinguish between those two\nsituations. For that purpose, there is a parameter \nauto_init_attributes\n of the\nmeta-model constructor that is \nTrue\n by default, but can be set to \nFalse\n to\nprevent auto-initialization from taking place.\n\n\nIf auto-initialization is disabled, then each optional attribute that was not\nmatched on the input will be set to \nNone\n. This is true for the plain\nassignments (\n=\n). An optional assignment (\n?=\n) will always be \nFalse\n if the\nRHS object is not matched in the input. The multiplicity assignments (\n*=\n and\n\n+=\n) will always be python lists.",
            "title": "Meta-model"
        },
        {
            "location": "/metamodel/#textx-meta-models",
            "text": "textX meta-model is a Python object that knows about all classes that can be\ninstantiated while parsing the input. A meta-model is built from the grammar by\nthe functions  metamodel_from_file  or  metamodel_from_str  in the textx.metamodel  module.  from textx import metamodel_from_file\nmy_metamodel = metamodel_from_file('my_grammar.tx')  Each rule from the grammar will result in a Python class kept in the meta-model.\nBesides, meta-model knows how to parse the input strings and convert them\nto  model .  Parsing the input and creating the model is done by  model_from_file  and model_from_str  methods of the meta-model object:  my_model = my_metamodel.model_from_file('some_input.md')",
            "title": "textX meta-models"
        },
        {
            "location": "/metamodel/#custom-classes",
            "text": "For each grammar rule a Python class with the same name is created dynamically.\nThese classes are instantiated during the parsing of the input string/file to\ncreate a graph of python objects, a.k.a.  model  or Abstract-Syntax Tree (AST).  Most of the time dynamically created classes will be sufficient, but sometimes\nyou will want to use your own classes instead. To do so use parameter  classes \nduring the meta-model instantiation. This parameter is a list of your classes\nthat should be named the same as the rules from the grammar which they\nrepresent.  from textx import metamodel_from_str\n\ngrammar = '''\nEntityModel:\n  entities+=Entity    // each model has one or more entities\n;\n\nEntity:\n  'entity' name=ID '{'\n    attributes+=Attribute     // each entity has one or more attributes\n  '}'\n;\n\nAttribute:\n  name=ID ':' type=[Entity]   // type is a reference to an entity. There are\n                              // built-in entities registered on the meta-model\n                              // for primitive types (integer, string)\n;\n'''\n\nclass Entity(object):\n  def __init__(self, parent, name, attributes):\n    self.parent = parent\n    self.name = name\n    self.attributes = attributes\n\n\n# Use our Entity class. \"Attribute\" class will be created dynamically.\nentity_mm = metamodel_from_str(grammar, classes=[Entity])  Now  entity_mm  can be used to parse the input models where our  Entity  class\nwill be instantiated to represent each  Entity  rule from the grammar.   Note  Constructor of the user-defined classes should accept all attributes defined\nby the corresponding rule from the grammar. In the previous example, we have\nprovided  name  and  attributes  attributes from the  Entity  rule. If the\nclass is a child in a parent-child relationship (see the next section), then\nthe  parent  constructor parameter should also be given.",
            "title": "Custom classes"
        },
        {
            "location": "/metamodel/#parent-child-relationships",
            "text": "There is often an intrinsic parent-child relationship between object in the\nmodel. In the previous example, each  Attribute  instance will always be a child\nof some  Entity  object.  textX gives automatic support for these relationships by providing the  parent \nattribute on each child object.  When you navigate  model  each child instance will have a  parent \nattribute.   Note  Always provide the parent parameter in user-defined classes for each class\nthat is a child in a parent-child relationship.",
            "title": "Parent-child relationships"
        },
        {
            "location": "/metamodel/#processors",
            "text": "To specify static semantics of the language textX uses a concept called processor . Processors are python callables that can modify the model\nelements during model parsing/instantiation or do some additional checks that\nare not possible to do by the grammar alone.  There are two types of processors:   model processors  - are callables that are called at the end of the parsing\n  when the whole model is instantiated. These processors accept the model \n  and meta-model as parameters.  object processors  - are registered for particular classes (grammar rules)\n  and are called when the objects of the given class is instantiated.   Processors can modify model/objects or raise exception ( TextXSemanticError ) if\nsome constraint is not met. User code that calls the model instantiation/parsing\ncan catch and handle this exception.",
            "title": "Processors"
        },
        {
            "location": "/metamodel/#model-processors",
            "text": "To register a model processor call  register_model_processor  on the meta-model\ninstance.  from textx import metamodel_from_file\n\n# Model processor is a callable that will accept meta-model and model as its\n# parameters.\ndef check_some_semantics(model, metamodel):\n  ...\n  ... Do some check on the model and raise TextXSemanticError if the semantics\n  ... rules are violated.\n\nmy_metamodel = metamodel_from_file('mygrammar.tx')\n\n# Register model processor on the meta-model instance\nmy_metamodel.register_model_processor(check_some_semantics)\n\n# Parse the model. check_some_semantics will be called automatically after\n# a successful parse to do further checks. If the rules are not met,\n# an instance of TextXSemanticError will be raised.\nmy_metamodel.model_from_file('some_model.ext')",
            "title": "Model processors"
        },
        {
            "location": "/metamodel/#object-processors",
            "text": "The purpose of the object processors is to validate or alter the object being\nconstructed. They are registered per class/rule.  Let's do some additional checks for the above Entity DSL example.  def entity_obj_processor(entity):\n  '''\n  Check that Ethe ntity names are capitalized. This could also be specified\n  in the grammar using regex match but we will do that check here just\n  as an example.\n  '''\n\n  if entity.name != entity.name.capitalize():\n    raise TextXSemanticError('Entity name \"%s\" must be capitalized.' %\n                            entity.name)\n\ndef attribute_obj_processor(attribute):\n  '''\n  Obj. processors can also introduce changes in the objects they process.\n  Here we set \"primitive\" attribute based on the Entity they refer to.\n  '''\n  attribute.primitive = attribute.type.name in ['integer', 'string']\n\n\n# Object processors are registered by defining a map between a rule name\n# and the callable that will process the instances of that rule/class.\nobj_processors = {\n    'Entity': entity_obj_processor,\n    'Attribute': attribute_obj_processor,\n    }\n\n# This map/dict is registered on a meta-model by the \"register_obj_processors\"\n# call.\nentity_mm.register_obj_processors(obj_processors)\n\n# Parse the model. At each successful parse of Entity or Attribute, the registered\n# processor will be called and the semantics error will be raised if the\n# check does not pass.\nentity_mm.model_from_file('my_entity_model.ent')  For another example of the usage of an object processor that modifies the\nobjects, see object processor move_command_processor   robot example .  If object processor returns a value that value will be used instead of the\noriginal object. This can be used to implement e.g. expression evaluators or\non-the-fly model interpretation. For more information   Note  For more information please take a look at  the tests .",
            "title": "Object processors"
        },
        {
            "location": "/metamodel/#built-in-objects",
            "text": "Often, you will need objects that should be a part of each model and you do not\nwant users to specify them in every model they create. The most notable example\nare primitive types (e.g.  integer ,  string ,  bool ).  Let's provide  integer  and  string  Entities to our  Entity  meta-model in\norder to simplify the model creation so that the users can use the names of\nthese two entities as the  Attribute  types.  class Entity(object):\n    def __init__(self, parent, name, attributes):\n        self.parent = parent\n        self.name = name\n        self.attributes = attributes\n\nentity_builtins = {\n        'integer': Entity(None, 'integer', []),\n        'string': Entity(None, 'string', [])\n}\nentity_mm = metamodel_from_file(\n  'entity.tx',\n  classes=[Entity]            # Register Entity user class,\n  builtins=entity_builtins    # Register integer and string built-in objs\n)  Now an  integer  and  string   Attribute  types can be used.\nSee  model \nand Entitiy \nexample for more.",
            "title": "Built-in objects"
        },
        {
            "location": "/metamodel/#creating-your-own-base-type",
            "text": "Match rules  by default return Python  string  type.\nBuilt-in match rules (i.e.  BASETYPEs ) on the other hand return Python base\ntypes.  You can use object processors to create your type by specifying match rule in\nthe grammar and object processor for that rule that will create an object of a\nproper Python type.  Example:  Model:\n  'begin' some_number=MyFloat 'end'\n;\nMyFloat:\n  /\\d+\\.(\\d+)?/\n;  In this example  MyFloat  rule is a match rule and by default will return Python string , so attribute  some_number  will be of  string  type. To change that,\nregister object processor for  MyFloat  rule:  mm = metamodel_from_str(grammar)\nmm.register_obj_processors({'MyFloat': lambda x: float(x)}))  Now,  MyFloat  will always be converted to Python  float  type.  Using match filters you can override built-in rule's conversions like this:  Model:\n  some_float=INT\n;  mm = metamodel_from_str(grammar)\nmm.register_obj_processors({'INT': lambda x: float(x)}))  In this example we use built-in rule  INT  that returns Python  int  type.\nRegistering object processor with the key  INT  we can change default behaviour\nand convert what is matched by this rule to some other object ( float  in this\ncase).",
            "title": "Creating your own base type"
        },
        {
            "location": "/metamodel/#auto-initialization-of-the-attributes",
            "text": "Each object that is recognized in the input string will be instantiated and its\nattributes will be set to the values parsed from the input. In the event that a\ndefined attribute is optional, it will nevertheless be created on the instance\nand set to the default value.  Here is a list of the default values for each base textX type:   ID - empty string - ''  INT - int - 0  FLOAT - float - 0.0  BOOL - bool - False  STRING - empty string - ''   Each attribute with zero or more multiplicity ( *= ) that does not match any\nobject from the input will be initialized to an empty list.  An attribute declared with one or more multiplicity ( += ) must match at least\none object from the input and will therefore be transformed to python list\ncontaining all matched objects.  The drawback of this auto-initialization system is that we can't be sure if the\nattribute was missing from the input or was matched, but the given value was the\nsame as the default value.  In some applications it is important to distinguish between those two\nsituations. For that purpose, there is a parameter  auto_init_attributes  of the\nmeta-model constructor that is  True  by default, but can be set to  False  to\nprevent auto-initialization from taking place.  If auto-initialization is disabled, then each optional attribute that was not\nmatched on the input will be set to  None . This is true for the plain\nassignments ( = ). An optional assignment ( ?= ) will always be  False  if the\nRHS object is not matched in the input. The multiplicity assignments ( *=  and += ) will always be python lists.",
            "title": "Auto-initialization of the attributes"
        },
        {
            "location": "/model/",
            "text": "textX models\n\u00b6\n\n\nModel is a python object graph consisting of POPOs (Plain Old Python Objects)\nconstructed from the input string that conforms to your DSL defined by the\ngrammar and additional \nmodel and object processors\n.\n\n\nIn a sense, this structure is an Abstract Syntax Tree (AST) known from classic\nparsing theory, but it is actually a graph structure where each reference is\nresolved to a proper python reference.\n\n\nEach object is an instance of a class from the meta-model. Classes are created\non-the-fly from the grammar rules or are \nsupplied by the\nuser\n.\n\n\nA model is created from the input string using the \nmodel_from_file\n and \nmodel_from_str\n\nmethods of the meta-model instance.\n\n\nfrom textx import metamodel_from_file\n\nmy_mm = metamodel_from_file('mygrammar.tx')\n\n# Create model\nmy_model = my_mm.model_from_file('some_model.ext')\n\n\n\nLet's take the Entity language used in \nCustom\nClasses\n section.\n\n\nContent of \nentity.tx\n file:\n\n\nEntityModel:\n  entities+=Entity    // each model has one or more entities\n;\n\nEntity:\n  'entity' name=ID '{'\n    attributes+=Attribute     // each entity has one or more attributes\n  '}'\n;\n\nAttribute:\n  name=ID ':' type=[Entity]   // type is a reference to an entity. There are\n                              // built-in entities registered on the meta-model\n                              // for the primitive types (integer, string)\n;\n\n\n\nFor the meta-model construction and built-in registration see \nCustom\nClasses\n and\n\nBuiltins\n sections.\n\n\nNow, we can use the \nentity_mm\n meta-model to parse and create Entity models.\n\n\nperson_model = entity_mm.model_from_file('person.ent')\n\n\n\n\nWhere \nperson.ent\n file might contain this:\n\n\nentity Person {\n  name : string\n  address: Address\n  age: integer\n}\n\nentity Address {\n  street : string\n  city : string\n  country : string\n}\n\n\n\nModel API\n\u00b6\n\n\nFunctions given in this section can be imported from \ntextx.model\n module.\n\n\nget_model(obj)\n\u00b6\n\n\nobj (model object)\n\n\nFinds the root of the model following \nparent\n references.\n\n\nget_metamodel(obj)\n\u00b6\n\n\nReturns metamodel for the model given \nobj\n belongs to.\n\n\nget_parent_of_type(typ, obj)\n\u00b6\n\n\n\n\ntyp (str or class)\n: the name of type of the type itself of the model object\nsearched for.\n\n\nobj (model object)\n: model object to start search from.\n\n\n\n\nFinds first object up the parent chain of the given type. If no parent of the\ngiven type exists \nNone\n is returned.\n\n\nget_children_of_type(typ, root)\n\u00b6\n\n\n\n\ntyp (str or python class)\n: The type of the model object we are looking for.\n\n\nroot (model object)\n: Python model object which is the start of the search\n    process.\n\n\n\n\nReturns a list of all model elements of type \ntyp\n starting from model element\n\nroot\n. The search process will follow containment links only. Non-containing\nreferences shall not be followed.\n\n\nSpecial model object's attributes\n\u00b6\n\n\nBeside attributes specified by the grammar, there are several special\nattributes on model objects created by textX. All special attributes' names\nstart with prefix \n_tx\n.\n\n\nThese special attributes don't exist if the type of the resulting model object\ndon't allow dynamic attribute creation (e.g. for Python base builtin types -\nstr, int).\n\n\n_tx_position and _tx_position_end\n\u00b6\n\n\n_tx_position\n attribute holds the position in the input string where the\nobject has been matched by the parser. Each object from the model object graph\nhas this attribute.\n\n\nThis is an absolute position in the input stream. To convert it to line/column\nformat use \npos_to_linecol\n method of the parser.\n\n\nline, col = entity_mm.parser.pos_to_linecol(\n    person_model.entities[0]._tx_position)\n\n\n\n\nWhere \nentity_mm\n is a meta-model of the language.\n\n\nPrevious example will give the line/column position of the first entity.\n\n\n_tx_position_end\n is the position in the input stream where the object ends.\nThis position is one char past the last char belonging to the object. Thus,\n\n_tx_position_end - _tx_position == length of the object str representation\n.\n\n\n_tx_filename\n\u00b6\n\n\nThis attribute exists only on the root of the model. If the model is loaded\nfrom a file, this attribute will be the full path of the source file. If the\nmodel is created from a string this attribute will be \nNone\n.\n\n\n_tx_parser\n\u00b6\n\n\nThis attribute represents the concrete parser instance used for this model\n(the parser of the _tx_metamodel is only a blueprint for the parser).\n\n\n_tx_metamodel\n\u00b6\n\n\nThis attribute exists only on the root of the model. It is a reference to the\nmeta-model object used for creating the model.\n\n\nThis attribute can be useful to access the parser given the reference to the\nmodel.\n\n\nparser = model._tx_metamodel.parser\nline, col = parser.pos_to_linecol(some_model_object)\n\n\n\n\n_tx_fqn\n\u00b6\n\n\nIs the fully qualified name of the grammar rule/Python class in regard to the\nimport path of the grammar file where the rule is defined. This attribute is\nused in \n__repr__\n of auto-generated Python classes.\n\n\n_tx_model_repository\n\u00b6\n\n\nThe model may have a model repository (initiated by some scope provider or by\nthe metamodel). This object is responsible to provide and cache other model\ninstances (see textx.scoping.providers).",
            "title": "Model"
        },
        {
            "location": "/model/#textx-models",
            "text": "Model is a python object graph consisting of POPOs (Plain Old Python Objects)\nconstructed from the input string that conforms to your DSL defined by the\ngrammar and additional  model and object processors .  In a sense, this structure is an Abstract Syntax Tree (AST) known from classic\nparsing theory, but it is actually a graph structure where each reference is\nresolved to a proper python reference.  Each object is an instance of a class from the meta-model. Classes are created\non-the-fly from the grammar rules or are  supplied by the\nuser .  A model is created from the input string using the  model_from_file  and  model_from_str \nmethods of the meta-model instance.  from textx import metamodel_from_file\n\nmy_mm = metamodel_from_file('mygrammar.tx')\n\n# Create model\nmy_model = my_mm.model_from_file('some_model.ext')  Let's take the Entity language used in  Custom\nClasses  section.  Content of  entity.tx  file:  EntityModel:\n  entities+=Entity    // each model has one or more entities\n;\n\nEntity:\n  'entity' name=ID '{'\n    attributes+=Attribute     // each entity has one or more attributes\n  '}'\n;\n\nAttribute:\n  name=ID ':' type=[Entity]   // type is a reference to an entity. There are\n                              // built-in entities registered on the meta-model\n                              // for the primitive types (integer, string)\n;  For the meta-model construction and built-in registration see  Custom\nClasses  and Builtins  sections.  Now, we can use the  entity_mm  meta-model to parse and create Entity models.  person_model = entity_mm.model_from_file('person.ent')  Where  person.ent  file might contain this:  entity Person {\n  name : string\n  address: Address\n  age: integer\n}\n\nentity Address {\n  street : string\n  city : string\n  country : string\n}",
            "title": "textX models"
        },
        {
            "location": "/model/#model-api",
            "text": "Functions given in this section can be imported from  textx.model  module.",
            "title": "Model API"
        },
        {
            "location": "/model/#get_modelobj",
            "text": "obj (model object)  Finds the root of the model following  parent  references.",
            "title": "get_model(obj)"
        },
        {
            "location": "/model/#get_metamodelobj",
            "text": "Returns metamodel for the model given  obj  belongs to.",
            "title": "get_metamodel(obj)"
        },
        {
            "location": "/model/#get_parent_of_typetyp-obj",
            "text": "typ (str or class) : the name of type of the type itself of the model object\nsearched for.  obj (model object) : model object to start search from.   Finds first object up the parent chain of the given type. If no parent of the\ngiven type exists  None  is returned.",
            "title": "get_parent_of_type(typ, obj)"
        },
        {
            "location": "/model/#get_children_of_typetyp-root",
            "text": "typ (str or python class) : The type of the model object we are looking for.  root (model object) : Python model object which is the start of the search\n    process.   Returns a list of all model elements of type  typ  starting from model element root . The search process will follow containment links only. Non-containing\nreferences shall not be followed.",
            "title": "get_children_of_type(typ, root)"
        },
        {
            "location": "/model/#special-model-objects-attributes",
            "text": "Beside attributes specified by the grammar, there are several special\nattributes on model objects created by textX. All special attributes' names\nstart with prefix  _tx .  These special attributes don't exist if the type of the resulting model object\ndon't allow dynamic attribute creation (e.g. for Python base builtin types -\nstr, int).",
            "title": "Special model object's attributes"
        },
        {
            "location": "/model/#_tx_position-and-_tx_position_end",
            "text": "_tx_position  attribute holds the position in the input string where the\nobject has been matched by the parser. Each object from the model object graph\nhas this attribute.  This is an absolute position in the input stream. To convert it to line/column\nformat use  pos_to_linecol  method of the parser.  line, col = entity_mm.parser.pos_to_linecol(\n    person_model.entities[0]._tx_position)  Where  entity_mm  is a meta-model of the language.  Previous example will give the line/column position of the first entity.  _tx_position_end  is the position in the input stream where the object ends.\nThis position is one char past the last char belonging to the object. Thus, _tx_position_end - _tx_position == length of the object str representation .",
            "title": "_tx_position and _tx_position_end"
        },
        {
            "location": "/model/#_tx_filename",
            "text": "This attribute exists only on the root of the model. If the model is loaded\nfrom a file, this attribute will be the full path of the source file. If the\nmodel is created from a string this attribute will be  None .",
            "title": "_tx_filename"
        },
        {
            "location": "/model/#_tx_parser",
            "text": "This attribute represents the concrete parser instance used for this model\n(the parser of the _tx_metamodel is only a blueprint for the parser).",
            "title": "_tx_parser"
        },
        {
            "location": "/model/#_tx_metamodel",
            "text": "This attribute exists only on the root of the model. It is a reference to the\nmeta-model object used for creating the model.  This attribute can be useful to access the parser given the reference to the\nmodel.  parser = model._tx_metamodel.parser\nline, col = parser.pos_to_linecol(some_model_object)",
            "title": "_tx_metamodel"
        },
        {
            "location": "/model/#_tx_fqn",
            "text": "Is the fully qualified name of the grammar rule/Python class in regard to the\nimport path of the grammar file where the rule is defined. This attribute is\nused in  __repr__  of auto-generated Python classes.",
            "title": "_tx_fqn"
        },
        {
            "location": "/model/#_tx_model_repository",
            "text": "The model may have a model repository (initiated by some scope provider or by\nthe metamodel). This object is responsible to provide and cache other model\ninstances (see textx.scoping.providers).",
            "title": "_tx_model_repository"
        },
        {
            "location": "/scoping/",
            "text": "textX Scoping\n\u00b6\n\n\nMotivation and Introduction to Scoping\n\u00b6\n\n\nAssume a grammar with references as in the following example (grammar snippet).\n\n\nMyAttribute:\n        ref=[MyInterface|FQN] name=ID ';'\n;\n\n\n\nThe scope provider is responsible for the reference resolution of such a\nreference.\n\n\nThe default behavior (default scope provider) is looking for the referenced name\nglobally. Other scope providers will take namespaces into account, support\nreferences to parts of the model stored in different files or even models\ndefined by other metamodels (imported into the current metamodel). Moreover,\nscope providers exist allowing to reference model elements relative to other\nreferenced model elements: This can be a referenced method defined in a\nreferenced class of an instance (with a meta-model defining classes, methods and\ninstances of classes).\n\n\nUsage\n\u00b6\n\n\nThe scope providers are registered to the metamodel and can be bound to specific\nparts of rules:\n\n\n\n\ne.g., \nmy_meta_model.register_scope_providers({\"*.*\": scoping.providers.FQN()})\n\n\nor: \nmy_meta_model.register_scope_providers({\"MyAttribute.ref\": scoping.providers.FQN()})\n\n\nor: \nmy_meta_model.register_scope_providers({\"*.ref\": scoping.providers.FQN()})\n\n\nor: \nmy_meta_model.register_scope_providers({\"MyAttribute.*\": scoping.providers.FQN()})\n\n\n\n\nExample (from \ntests/test_scoping/test_local_scope.py\n):\n\n\n# Grammar snippet (Components.tx)\nComponent:\n    'component' name=ID ('extends' extends+=[Component|FQN][','])? '{'\n        slots*=Slot\n    '}'\n;\nSlot: SlotIn|SlotOut;\n# ...\nInstance:\n    'instance' name=ID ':' component=[Component|FQN] ;\nConnection:\n    'connect'\n      from_inst=[Instance|ID] '.' from_port=[SlotOut|ID]\n    'to'\n      to_inst=[Instance|ID] '.' to_port=[SlotIn|ID]\n;\n\n# Python snippet\nmy_meta_model = metamodel_from_file(abspath(dirname(__file__)) + '/components_model1/Components.tx')\nmy_meta_model.register_scope_providers({\n    \"*.*\": scoping_providers.FQN(),\n    \"Connection.from_port\": scoping_providers.RelativeName(\"from_inst.component.slots\"),\n    \"Connection.to_port\": scoping_providers.RelativeName(\"to_inst.component.slots\"),\n})\n\n\n\nThis example selects the fully qualified name provider as default provider\n(\n\"*.*\"\n). Moreover, for special attributes of a \nConnection\n a relative name\nlookup is specified: Here the \npath\n from the rule \nConnection\n containing the\nattribute of interest (e.g. \nConnection.from_port\n) to the referenced element is\nspecified (the slot contained in \nfrom_inst.component.slots\n). Since this\nattribute is a list, the list is searched to find the referenced name.\n\n\n\n\nNote\n\n\nSpecial rule selections (e.g., \nConnection.from_port\n) are preferred\nto wildcard selection (e.e, \n\"*.*\"\n).\n\n\n\n\nScope Providers defined in Module \"textx.scoping.providers\"\n\u00b6\n\n\nWe provide some standard scope providers:\n\n\n\n\ntextx.scoping.providers.PlainName\n: This is the \ndefault provider\n of\n   textX.\n\n\ntextx.scoping.providers.FQN\n: This is a \nprovider similar to Java or Xtext\n   name loopup\n.\n   Example: see \ntests/test_scoping/test_full_qualified_name.py\n.\n\n\ntextx.scoping.providers.ImportURI\n: This a provider which \nallows to load\n   additional modules\n for lookup.\n   You need to define a rule with an attribute \nimportURI\n as string (like in\n   Xtext). This string is then used to load other models. Moreover, you need\n   to provide another scope provider to manage the concrete lookup, e.g., the\n   \nscope_provider_plain_names\n or the \nscope_provider_fully_qualified_names\n.\n   Model objects formed by the rules with an \nimportURI\n attribute get an\n   additional attribute \n_tx_loaded_models\n which is a list of the loaded\n   models by this rule instance.\n   Example: see \ntests/test_scoping/test_import_module.py\n.\n\n\nFQNImportURI\n (decorated scope provider)\n\n\nPlainNameImportURI\n (decorated scope provider)\n\n\n\n\n\n\ntextx.scoping.providers.GlobalRepo\n: This is a provider where \nyou initially\n   need to specifiy the model files to be loaded and used for lookup\n. Like\n   for \nImportURI\n you need to provide another scope provider for the concrete\n   lookup.\n   Example: see \ntests/test_scoping/test_global_import_modules.py\n.\n\n\ntextx.scoping.providers.FQNGlobalRepo\n (decorated scope provider)\n\n\ntextx.scoping.providers.PlainNameGlobalRepo\n (decorated scope provider)\n\n\n\n\n\n\ntextx.scoping.providers.RelativeName\n: This is a scope provider to \nresolve\n   relative lookups\n: e.g., model-methods of a model-instance, defined by the\n   class associated with the model-instance. Typically, another reference (the\n   reference to the model-class of a model-instance) is used to determine the\n   concrete referenced object (e.g. the model-method, owned by a model-class).\n   Example: see \ntests/test_scoping/test_local_scope.py\n.\n\n\ntextx.scoping.providers.ExtRelativeName\n: The same as \nRelativeName\n \nallowing\n   to model inheritance or chained lookups\n.\n   Example: see \ntests/test_scoping/test_local_scope.py\n.\n\n\n\n\nNote on Uniqueness of Model Elements\n\u00b6\n\n\nTwo different models created using one single meta model (not using a scope\nprovider like \nGlobalRepo\n, but by directly loading the models from file) have\ndifferent instances of the same model elements. If you need two such models to\nshare their model element instances, you can specify this, while creating the\nmeta model (\nglobal_repository=True\n). Then, the meta model will store an own\ninstance of a \nGlobalModelRepository\n as a base for all loaded models.\n\n\nModel elements in models including other parts of the model (possibly circular)\nhave unique model elements (no double instances).\n\n\nExamples see \ntests/test_scoping/test_import_module.py\n.\n\n\nTechnical aspects and implementation details\n\u00b6\n\n\nThe scope providers are python callables accepting \nobj, attr, obj_ref\n:\n\n\n\n\nobj\n     : the object representing the start of the search (e.g., a rule\n             (e.g. \nMyAttribute\n in the example above, or the model)\n\n\nattr\n    : a reference to the attribute \nref\n\n\nobj_ref\n : a \ntextx.model.ObjCrossRef\n - the reference to be resolved\n\n\n\n\nThe scope provides return the referenced object (e.g. a \nMyInterface\n object in\nthe example illustrated in the \nMotivation and Introduction\n above (or \nNone\n if\nnothing is found; or a \nPostponed\n object, see below).\n\n\nScope providers shall be stateless or have unmodifiable state after\nconstruction: this means they should allow to be reused for different models\n(created using the same meta-model) without interacting with each other. This\nmeans, they must save their state in the corresponding model, if they need to\nstore data (e.g., if they load additional models from files \nduring name\nresolution\n, they are not allowed to store them inside the scope provider.\n\n\nModels with references being resolved have a temporary attribute\n\n_tx_reference_resolver\n of type \nReferenceResolver\n. This object can be used to\nresolve the object. It contains information, such as the parser in charge for\nthe model (file) being processed.\n\n\n\n\nNote\n\n\nScope providers as normal functions (\ndef <name>(...):...\n), not\naccessing global data, are safe per se. The reason to be stateless, is that\nno side effects (beside, e.g., loading other models) should influence the\nname lookup.\n\n\n\n\nThe state of model resolution should mainly consist of models already loaded.\nThese models are stored in a \nGlobalModelRepository\n class. This class (if\nrequired) is stored in the model. An included model loaded from another\nincluding model \"inherits\" the part of the \nGlobalModelRepository\n representing\nall loaded models. This is done to (a) cache already loaded models and (b)\nguarantee, that every referenced model element is instantiated exactly once.\nEven in the case of circular inclusions.\n\n\nScope providers may return an object of type \nPostponed\n, if they depend on\nanother event to happen first. This event is typically the resolution of another\nreference. The resolution process will repeat multiple times over all unresolved\nreferences to be resolved until all references are resolved or no progress\nregarding the resolution is observed. In the latter case an error is raised. The\ncontrol flow responsibility of the resolution process is allocated to the\n\nmodel.py\n module.",
            "title": "Scoping"
        },
        {
            "location": "/scoping/#textx-scoping",
            "text": "",
            "title": "textX Scoping"
        },
        {
            "location": "/scoping/#motivation-and-introduction-to-scoping",
            "text": "Assume a grammar with references as in the following example (grammar snippet).  MyAttribute:\n        ref=[MyInterface|FQN] name=ID ';'\n;  The scope provider is responsible for the reference resolution of such a\nreference.  The default behavior (default scope provider) is looking for the referenced name\nglobally. Other scope providers will take namespaces into account, support\nreferences to parts of the model stored in different files or even models\ndefined by other metamodels (imported into the current metamodel). Moreover,\nscope providers exist allowing to reference model elements relative to other\nreferenced model elements: This can be a referenced method defined in a\nreferenced class of an instance (with a meta-model defining classes, methods and\ninstances of classes).",
            "title": "Motivation and Introduction to Scoping"
        },
        {
            "location": "/scoping/#usage",
            "text": "The scope providers are registered to the metamodel and can be bound to specific\nparts of rules:   e.g.,  my_meta_model.register_scope_providers({\"*.*\": scoping.providers.FQN()})  or:  my_meta_model.register_scope_providers({\"MyAttribute.ref\": scoping.providers.FQN()})  or:  my_meta_model.register_scope_providers({\"*.ref\": scoping.providers.FQN()})  or:  my_meta_model.register_scope_providers({\"MyAttribute.*\": scoping.providers.FQN()})   Example (from  tests/test_scoping/test_local_scope.py ):  # Grammar snippet (Components.tx)\nComponent:\n    'component' name=ID ('extends' extends+=[Component|FQN][','])? '{'\n        slots*=Slot\n    '}'\n;\nSlot: SlotIn|SlotOut;\n# ...\nInstance:\n    'instance' name=ID ':' component=[Component|FQN] ;\nConnection:\n    'connect'\n      from_inst=[Instance|ID] '.' from_port=[SlotOut|ID]\n    'to'\n      to_inst=[Instance|ID] '.' to_port=[SlotIn|ID]\n;\n\n# Python snippet\nmy_meta_model = metamodel_from_file(abspath(dirname(__file__)) + '/components_model1/Components.tx')\nmy_meta_model.register_scope_providers({\n    \"*.*\": scoping_providers.FQN(),\n    \"Connection.from_port\": scoping_providers.RelativeName(\"from_inst.component.slots\"),\n    \"Connection.to_port\": scoping_providers.RelativeName(\"to_inst.component.slots\"),\n})  This example selects the fully qualified name provider as default provider\n( \"*.*\" ). Moreover, for special attributes of a  Connection  a relative name\nlookup is specified: Here the  path  from the rule  Connection  containing the\nattribute of interest (e.g.  Connection.from_port ) to the referenced element is\nspecified (the slot contained in  from_inst.component.slots ). Since this\nattribute is a list, the list is searched to find the referenced name.   Note  Special rule selections (e.g.,  Connection.from_port ) are preferred\nto wildcard selection (e.e,  \"*.*\" ).",
            "title": "Usage"
        },
        {
            "location": "/scoping/#scope-providers-defined-in-module-textxscopingproviders",
            "text": "We provide some standard scope providers:   textx.scoping.providers.PlainName : This is the  default provider  of\n   textX.  textx.scoping.providers.FQN : This is a  provider similar to Java or Xtext\n   name loopup .\n   Example: see  tests/test_scoping/test_full_qualified_name.py .  textx.scoping.providers.ImportURI : This a provider which  allows to load\n   additional modules  for lookup.\n   You need to define a rule with an attribute  importURI  as string (like in\n   Xtext). This string is then used to load other models. Moreover, you need\n   to provide another scope provider to manage the concrete lookup, e.g., the\n    scope_provider_plain_names  or the  scope_provider_fully_qualified_names .\n   Model objects formed by the rules with an  importURI  attribute get an\n   additional attribute  _tx_loaded_models  which is a list of the loaded\n   models by this rule instance.\n   Example: see  tests/test_scoping/test_import_module.py .  FQNImportURI  (decorated scope provider)  PlainNameImportURI  (decorated scope provider)    textx.scoping.providers.GlobalRepo : This is a provider where  you initially\n   need to specifiy the model files to be loaded and used for lookup . Like\n   for  ImportURI  you need to provide another scope provider for the concrete\n   lookup.\n   Example: see  tests/test_scoping/test_global_import_modules.py .  textx.scoping.providers.FQNGlobalRepo  (decorated scope provider)  textx.scoping.providers.PlainNameGlobalRepo  (decorated scope provider)    textx.scoping.providers.RelativeName : This is a scope provider to  resolve\n   relative lookups : e.g., model-methods of a model-instance, defined by the\n   class associated with the model-instance. Typically, another reference (the\n   reference to the model-class of a model-instance) is used to determine the\n   concrete referenced object (e.g. the model-method, owned by a model-class).\n   Example: see  tests/test_scoping/test_local_scope.py .  textx.scoping.providers.ExtRelativeName : The same as  RelativeName   allowing\n   to model inheritance or chained lookups .\n   Example: see  tests/test_scoping/test_local_scope.py .",
            "title": "Scope Providers defined in Module \"textx.scoping.providers\""
        },
        {
            "location": "/scoping/#note-on-uniqueness-of-model-elements",
            "text": "Two different models created using one single meta model (not using a scope\nprovider like  GlobalRepo , but by directly loading the models from file) have\ndifferent instances of the same model elements. If you need two such models to\nshare their model element instances, you can specify this, while creating the\nmeta model ( global_repository=True ). Then, the meta model will store an own\ninstance of a  GlobalModelRepository  as a base for all loaded models.  Model elements in models including other parts of the model (possibly circular)\nhave unique model elements (no double instances).  Examples see  tests/test_scoping/test_import_module.py .",
            "title": "Note on Uniqueness of Model Elements"
        },
        {
            "location": "/scoping/#technical-aspects-and-implementation-details",
            "text": "The scope providers are python callables accepting  obj, attr, obj_ref :   obj      : the object representing the start of the search (e.g., a rule\n             (e.g.  MyAttribute  in the example above, or the model)  attr     : a reference to the attribute  ref  obj_ref  : a  textx.model.ObjCrossRef  - the reference to be resolved   The scope provides return the referenced object (e.g. a  MyInterface  object in\nthe example illustrated in the  Motivation and Introduction  above (or  None  if\nnothing is found; or a  Postponed  object, see below).  Scope providers shall be stateless or have unmodifiable state after\nconstruction: this means they should allow to be reused for different models\n(created using the same meta-model) without interacting with each other. This\nmeans, they must save their state in the corresponding model, if they need to\nstore data (e.g., if they load additional models from files  during name\nresolution , they are not allowed to store them inside the scope provider.  Models with references being resolved have a temporary attribute _tx_reference_resolver  of type  ReferenceResolver . This object can be used to\nresolve the object. It contains information, such as the parser in charge for\nthe model (file) being processed.   Note  Scope providers as normal functions ( def <name>(...):... ), not\naccessing global data, are safe per se. The reason to be stateless, is that\nno side effects (beside, e.g., loading other models) should influence the\nname lookup.   The state of model resolution should mainly consist of models already loaded.\nThese models are stored in a  GlobalModelRepository  class. This class (if\nrequired) is stored in the model. An included model loaded from another\nincluding model \"inherits\" the part of the  GlobalModelRepository  representing\nall loaded models. This is done to (a) cache already loaded models and (b)\nguarantee, that every referenced model element is instantiated exactly once.\nEven in the case of circular inclusions.  Scope providers may return an object of type  Postponed , if they depend on\nanother event to happen first. This event is typically the resolution of another\nreference. The resolution process will repeat multiple times over all unresolved\nreferences to be resolved until all references are resolved or no progress\nregarding the resolution is observed. In the latter case an error is raised. The\ncontrol flow responsibility of the resolution process is allocated to the model.py  module.",
            "title": "Technical aspects and implementation details"
        },
        {
            "location": "/multimetamodel/",
            "text": "Multi meta-model support\n\u00b6\n\n\nAs a feature of some \nscope providers\n, there is the possibility to\ndefine a file extension based allocation of model files to meta models. This\nglobal data is stored in the class \ntextx.scoping.MetaModelProvider\n: Here, you\ncan register meta models associated to files patterns. Thus, you can control\nwhich meta model to use when loading a file in a scope provider using the\n\nImportURI\n-feature (e.g., \nFQNImportURI\n) or with global scope providers (e.g.,\n\nPlainNameGlobalRepo\n). If no file pattern matches, the meta model of the current\nmodel is utilized.\n\n\nSimple examples see \ntests/test_scoping/test_metamodel_provider*.py\n.\n\n\nUse Case: Recipes and Ingredients with global model sharing\n\u00b6\n\n\nIn this use case we define recipes (food preparation) including a list of\ningredients. The ingredients of a recipe model element are defined by\n\n\n\n\na count (e.g. 100),\n\n\na unit (e.g. gram),\n\n\nand an ingredient reference (e.g. sugar).\n\n\n\n\nIn a separate model the ingredients are defined: Here we can define multiple\nunits to be used for each ingerdient (e.g. \n60 gram of sugar\n or \n1 cup of\nsugar\n). Moreover some ingredients may inherit features of other ingredients\n(e.g. salt may have the same units as sugar).\n\n\nHere, two meta models are defined to handle ingredient definitions (e.g.\n\nfruits.ingredient\n) and recipes (e.g. \nfruit_salad.recipe\n).\n\n\nThe \nMetaModelProvider\n is utilized to allocate the file extensions to the meta\nmodels\n(see\n\ntest_metamodel_provider2.py\n).\nImportantly, a common model repository (\nglobal_repo\n) is defined to share all\nmodel elements among both meta models:\n\n\ni_mm = get_meta_model(\n    global_repo, this_folder + \"/metamodel_provider2/Ingredient.tx\")\nr_mm = get_meta_model(\n    global_repo, this_folder + \"/metamodel_provider2/Recipe.tx\")\n\nscoping.MetaModelProvider.add_metamodel(\"*.recipe\", r_mm)\nscoping.MetaModelProvider.add_metamodel(\"*.ingredient\", i_mm)\n\n\n\nUse Case: meta-model sharing with the ImportURI-feature\n\u00b6\n\n\nIn this use case we have a given meta model to define components and instances\nof components. A second model is added to define users to use instances of such\ncomponents defned in the first model.\n\n\nThe grammar for the user meta-model is given as follows (including the ability\nto import a component model file).\n\n\nimport Components\n\nModel:\n    imports+=Import\n    users+=User\n;\n\nUser:\n    \"user\" name=ID \"uses\" instance=[Instance|FQN] // Instance, FQN from other grammar\n;\n\nImport: 'import' importURI=STRING;\n\n\n\nThe global \nMetaModelProvider\n class is utilized to allocate the file extension to\nthe corresponding meta model:\n\n\n    scoping.MetaModelProvider.add_metamodel(\"*.components\", mm_components)\n    scoping.MetaModelProvider.add_metamodel(\"*.users\", mm_users)\n\n\n\nWith this construct we can define a user model referencing a component model:\n\n\nimport \"example.components\"\nuser pi uses usage.action1\n\n\n\nUse Case: referencing non-textx meta-models/models\n\u00b6\n\n\nYou can reference an arbitrary python object using the \nOBJECT\n rule (see:\n\ntest_reference_to_buildin_attribute.py\n)\n\n\nAccess:\n    'access' name=ID pyobj=[OBJECT] ('.' pyattr=[OBJECT])?\n\n\n\nIn this case the referenced object is a python dictionary (\npyobj\n) and the\nentry of such a dictionary (\npyattr\n). An example model will look like:\n\n\naccess AccessName1 foreign_model.name_of_entry\n\n\n\nA custom scope provider is used to achieve this mapping:\n\n\ndef my_scope_provider(obj, attr, attr_ref):\n    pyobj = obj.pyobj\n    if attr_ref.obj_name in pyobj:\n        return pyobj[attr_ref.obj_name]\n    else:\n        raise Exception(\"{} not found\".format(attr_ref.obj_name))\n\n\n\nThe scope provider is linked to the \npyattr\n attribute of the rule \nAccess\n:\n\n\nmy_metamodel.register_scope_providers({\n    \"Access.pyattr\": my_scope_provider,\n})\n\n\n\nWith this, we can reference non-textx data elements from within our language.\nThis can be used to, e.g., use a non-textx AST object and reference it from a\ntextx model.\n\n\nReferencing non-textx meta-models/models with a json file\n\u00b6\n\n\nIn\n\ntest_reference_to_nontextx_attribute.py\n we\nalso demonstrate how such an external model can be loaded with our own language\n(using a json file as external model).\n\n\nimport \"test_reference_to_nontextx_attribute/othermodel.json\" as data\naccess A1 data.name\naccess A2 data.gender\n\n\n\nWhere the json file \nothermodel.json\n consists of:\n\n\n{\n  \"name\": \"pierre\",\n  \"gender\": \"male\"\n}\n\n\n\nConclusion\n\u00b6\n\n\nWe provide a pragmatic way to define meta-models using other meta models.\nMostly, we focus on textx meta-models using other textx meta-models. But scope\nproviders may be used to also link a textx meta model to an arbitrary non-textx\ndata structure.",
            "title": "Multi meta-model"
        },
        {
            "location": "/multimetamodel/#multi-meta-model-support",
            "text": "As a feature of some  scope providers , there is the possibility to\ndefine a file extension based allocation of model files to meta models. This\nglobal data is stored in the class  textx.scoping.MetaModelProvider : Here, you\ncan register meta models associated to files patterns. Thus, you can control\nwhich meta model to use when loading a file in a scope provider using the ImportURI -feature (e.g.,  FQNImportURI ) or with global scope providers (e.g., PlainNameGlobalRepo ). If no file pattern matches, the meta model of the current\nmodel is utilized.  Simple examples see  tests/test_scoping/test_metamodel_provider*.py .",
            "title": "Multi meta-model support"
        },
        {
            "location": "/multimetamodel/#use-case-recipes-and-ingredients-with-global-model-sharing",
            "text": "In this use case we define recipes (food preparation) including a list of\ningredients. The ingredients of a recipe model element are defined by   a count (e.g. 100),  a unit (e.g. gram),  and an ingredient reference (e.g. sugar).   In a separate model the ingredients are defined: Here we can define multiple\nunits to be used for each ingerdient (e.g.  60 gram of sugar  or  1 cup of\nsugar ). Moreover some ingredients may inherit features of other ingredients\n(e.g. salt may have the same units as sugar).  Here, two meta models are defined to handle ingredient definitions (e.g. fruits.ingredient ) and recipes (e.g.  fruit_salad.recipe ).  The  MetaModelProvider  is utilized to allocate the file extensions to the meta\nmodels\n(see test_metamodel_provider2.py ).\nImportantly, a common model repository ( global_repo ) is defined to share all\nmodel elements among both meta models:  i_mm = get_meta_model(\n    global_repo, this_folder + \"/metamodel_provider2/Ingredient.tx\")\nr_mm = get_meta_model(\n    global_repo, this_folder + \"/metamodel_provider2/Recipe.tx\")\n\nscoping.MetaModelProvider.add_metamodel(\"*.recipe\", r_mm)\nscoping.MetaModelProvider.add_metamodel(\"*.ingredient\", i_mm)",
            "title": "Use Case: Recipes and Ingredients with global model sharing"
        },
        {
            "location": "/multimetamodel/#use-case-meta-model-sharing-with-the-importuri-feature",
            "text": "In this use case we have a given meta model to define components and instances\nof components. A second model is added to define users to use instances of such\ncomponents defned in the first model.  The grammar for the user meta-model is given as follows (including the ability\nto import a component model file).  import Components\n\nModel:\n    imports+=Import\n    users+=User\n;\n\nUser:\n    \"user\" name=ID \"uses\" instance=[Instance|FQN] // Instance, FQN from other grammar\n;\n\nImport: 'import' importURI=STRING;  The global  MetaModelProvider  class is utilized to allocate the file extension to\nthe corresponding meta model:      scoping.MetaModelProvider.add_metamodel(\"*.components\", mm_components)\n    scoping.MetaModelProvider.add_metamodel(\"*.users\", mm_users)  With this construct we can define a user model referencing a component model:  import \"example.components\"\nuser pi uses usage.action1",
            "title": "Use Case: meta-model sharing with the ImportURI-feature"
        },
        {
            "location": "/multimetamodel/#use-case-referencing-non-textx-meta-modelsmodels",
            "text": "You can reference an arbitrary python object using the  OBJECT  rule (see: test_reference_to_buildin_attribute.py )  Access:\n    'access' name=ID pyobj=[OBJECT] ('.' pyattr=[OBJECT])?  In this case the referenced object is a python dictionary ( pyobj ) and the\nentry of such a dictionary ( pyattr ). An example model will look like:  access AccessName1 foreign_model.name_of_entry  A custom scope provider is used to achieve this mapping:  def my_scope_provider(obj, attr, attr_ref):\n    pyobj = obj.pyobj\n    if attr_ref.obj_name in pyobj:\n        return pyobj[attr_ref.obj_name]\n    else:\n        raise Exception(\"{} not found\".format(attr_ref.obj_name))  The scope provider is linked to the  pyattr  attribute of the rule  Access :  my_metamodel.register_scope_providers({\n    \"Access.pyattr\": my_scope_provider,\n})  With this, we can reference non-textx data elements from within our language.\nThis can be used to, e.g., use a non-textx AST object and reference it from a\ntextx model.",
            "title": "Use Case: referencing non-textx meta-models/models"
        },
        {
            "location": "/multimetamodel/#referencing-non-textx-meta-modelsmodels-with-a-json-file",
            "text": "In test_reference_to_nontextx_attribute.py  we\nalso demonstrate how such an external model can be loaded with our own language\n(using a json file as external model).  import \"test_reference_to_nontextx_attribute/othermodel.json\" as data\naccess A1 data.name\naccess A2 data.gender  Where the json file  othermodel.json  consists of:  {\n  \"name\": \"pierre\",\n  \"gender\": \"male\"\n}",
            "title": "Referencing non-textx meta-models/models with a json file"
        },
        {
            "location": "/multimetamodel/#conclusion",
            "text": "We provide a pragmatic way to define meta-models using other meta models.\nMostly, we focus on textx meta-models using other textx meta-models. But scope\nproviders may be used to also link a textx meta model to an arbitrary non-textx\ndata structure.",
            "title": "Conclusion"
        },
        {
            "location": "/parser_config/",
            "text": "Parser configuration\n\u00b6\n\n\nCase sensitivity\n\u00b6\n\n\nParser is by default case sensitive. For DSLs that should be case insensitive\nuse \nignore_case\n parameter of the meta-model constructor call.\n\n\nfrom textx import metamodel_from_file\n\nmy_metamodel = metamodel_from_file('mygrammar.tx', ignore_case=True)\n\n\n\n\nWhitespace handling\n\u00b6\n\n\nThe parser will skip whitespaces by default. Whitespaces are spaces, tabs and\nnewlines by default. Skipping of the whitespaces can be disabled by \nskipws\n bool\nparameter in the constructor call. Also, what is a whitespace can be redefined by\nthe \nws\n string parameter.\n\n\nfrom textx import metamodel_from_file\nmy_metamodel = metamodel_from_file('mygrammar.tx', skipws=False, ws='\\s\\n')\n\n\n\n\nWhitespaces and whitespace skipping can be defined in the grammar on the level\nof a single rule by \nrule modifiers\n.\n\n\nAutomatic keywords\n\u00b6\n\n\nWhen designing a DSL, it is usually desirable to match keywords on word\nboundaries. For example, if we have Entity grammar from the above, then a word\n\nentity\n will be considered a keyword and should be matched on word boundaries\nonly. If we have a word \nentity2\n in the input string at the place where\n\nentity\n should be matched, the match should not succeed.\n\n\nWe could achieve this by using a regular expression match and the word\nboundaries regular expression rule for each keyword-like match.\n\n\nEnitity:\n  /\\bentity\\b/ name=ID ...\n\n\n\nBut the grammar will be cumbersome to read.\n\n\ntextX can do automatic word boundary match for all keyword-like string matches.\nTo enable this feature set parameter \nautokwd\n to \nTrue\n in the constructor\ncall.\n\n\nfrom textx import metamodel_from_file\nmy_metamodel = metamodel_from_file('mygrammar.tx', autokwd=True)\n\n\n\n\nAny simple match from the grammar that is matched by the\nregular expression \n[^\\d\\W]\\w*\n is considered to be a keyword.\n\n\nMemoization (a.k.a. packrat parsing)\n\u00b6\n\n\nThis technique is based on memoizing result on each parsing expression rule. For\nsome grammars with a lot of backtracking this can yield a significant speed\nincrease at the expense of some memory used for the memoization cache.\n\n\nStarting with textX 1.4 this feature is disabled by default. If you think that\nparsing is slow, try to enable memoization by setting \nmemoization\n parameter to\n\nTrue\n during meta-model instantiation.\n\n\nfrom textx import metamodel_from_file\nmy_metamodel = metamodel_from_file('mygrammar.tx', memoization=True)",
            "title": "Parser configuration"
        },
        {
            "location": "/parser_config/#parser-configuration",
            "text": "",
            "title": "Parser configuration"
        },
        {
            "location": "/parser_config/#case-sensitivity",
            "text": "Parser is by default case sensitive. For DSLs that should be case insensitive\nuse  ignore_case  parameter of the meta-model constructor call.  from textx import metamodel_from_file\n\nmy_metamodel = metamodel_from_file('mygrammar.tx', ignore_case=True)",
            "title": "Case sensitivity"
        },
        {
            "location": "/parser_config/#whitespace-handling",
            "text": "The parser will skip whitespaces by default. Whitespaces are spaces, tabs and\nnewlines by default. Skipping of the whitespaces can be disabled by  skipws  bool\nparameter in the constructor call. Also, what is a whitespace can be redefined by\nthe  ws  string parameter.  from textx import metamodel_from_file\nmy_metamodel = metamodel_from_file('mygrammar.tx', skipws=False, ws='\\s\\n')  Whitespaces and whitespace skipping can be defined in the grammar on the level\nof a single rule by  rule modifiers .",
            "title": "Whitespace handling"
        },
        {
            "location": "/parser_config/#automatic-keywords",
            "text": "When designing a DSL, it is usually desirable to match keywords on word\nboundaries. For example, if we have Entity grammar from the above, then a word entity  will be considered a keyword and should be matched on word boundaries\nonly. If we have a word  entity2  in the input string at the place where entity  should be matched, the match should not succeed.  We could achieve this by using a regular expression match and the word\nboundaries regular expression rule for each keyword-like match.  Enitity:\n  /\\bentity\\b/ name=ID ...  But the grammar will be cumbersome to read.  textX can do automatic word boundary match for all keyword-like string matches.\nTo enable this feature set parameter  autokwd  to  True  in the constructor\ncall.  from textx import metamodel_from_file\nmy_metamodel = metamodel_from_file('mygrammar.tx', autokwd=True)  Any simple match from the grammar that is matched by the\nregular expression  [^\\d\\W]\\w*  is considered to be a keyword.",
            "title": "Automatic keywords"
        },
        {
            "location": "/parser_config/#memoization-aka-packrat-parsing",
            "text": "This technique is based on memoizing result on each parsing expression rule. For\nsome grammars with a lot of backtracking this can yield a significant speed\nincrease at the expense of some memory used for the memoization cache.  Starting with textX 1.4 this feature is disabled by default. If you think that\nparsing is slow, try to enable memoization by setting  memoization  parameter to True  during meta-model instantiation.  from textx import metamodel_from_file\nmy_metamodel = metamodel_from_file('mygrammar.tx', memoization=True)",
            "title": "Memoization (a.k.a. packrat parsing)"
        },
        {
            "location": "/visualization/",
            "text": "Visualization\n\u00b6\n\n\nA meta-model, model and parse-tree can be exported to dot files\n(\nGraphViz\n) for visualization. Module \ntextx.export\n\ncontains functions \nmetamodel_export\n and \nmodel_export\n that can export\nmeta-model and model to dot files respectively.\n\n\nIf \ndebugging\n is enabled, meta-model, model and parse trees will\nautomatically get exported to dot.\n\n\nDot files can be viewed by dot viewers.  There are quite a few dot viewers\nfreely available (e.g. \nxdot\n,\n\nZGRViewer\n).\n\n\nAlternatively, dot files can be converted to image formats using \ndot\n command.\nFor more info see \nthis SO thread\n.\n\n\nMeta-model visualization\n\u00b6\n\n\nTo visualize a meta-model (see \nEntity\nexample\n)\ndo:\n\n\nfrom textx import metamodel_from_file\nfrom textx.export import metamodel_export\n\nentity_mm = metamodel_from_file('entity.tx')\n\nmetamodel_export(entity_mm, 'entity.dot')\n\n\n\n\nentity.dot\n file will be created. You can visualize this file by using various\ndot viewers or convert it to various image formats using the 'dot' tool.\n\n\n$ dot -Tpng -O entity.dot\n\n\n\nThe following image is generated:\n\n\n\n\nModel visualization\n\u00b6\n\n\nSimilarly to meta-model visualization, you can also visualize your models (see \nEntity\nexample\n).\n\n\nfrom textx.export import model_export\n\nperson_model = entity_mm.model_from_file('person.ent')\n\nmodel_export(person_model, 'person.dot')\n\n\n\n\nConvert this \ndot\n file to \npng\n with:\n\n\n$ dot -Tpng -O person.dot\n\n\n\nThe following image is generated:\n\n\n\n\n\n\nNote\n\n\nAlso, see \ntextx command/tool\n for model visualization\nfrom the command line.",
            "title": "Visualization"
        },
        {
            "location": "/visualization/#visualization",
            "text": "A meta-model, model and parse-tree can be exported to dot files\n( GraphViz ) for visualization. Module  textx.export \ncontains functions  metamodel_export  and  model_export  that can export\nmeta-model and model to dot files respectively.  If  debugging  is enabled, meta-model, model and parse trees will\nautomatically get exported to dot.  Dot files can be viewed by dot viewers.  There are quite a few dot viewers\nfreely available (e.g.  xdot , ZGRViewer ).  Alternatively, dot files can be converted to image formats using  dot  command.\nFor more info see  this SO thread .",
            "title": "Visualization"
        },
        {
            "location": "/visualization/#meta-model-visualization",
            "text": "To visualize a meta-model (see  Entity\nexample )\ndo:  from textx import metamodel_from_file\nfrom textx.export import metamodel_export\n\nentity_mm = metamodel_from_file('entity.tx')\n\nmetamodel_export(entity_mm, 'entity.dot')  entity.dot  file will be created. You can visualize this file by using various\ndot viewers or convert it to various image formats using the 'dot' tool.  $ dot -Tpng -O entity.dot  The following image is generated:",
            "title": "Meta-model visualization"
        },
        {
            "location": "/visualization/#model-visualization",
            "text": "Similarly to meta-model visualization, you can also visualize your models (see  Entity\nexample ).  from textx.export import model_export\n\nperson_model = entity_mm.model_from_file('person.ent')\n\nmodel_export(person_model, 'person.dot')  Convert this  dot  file to  png  with:  $ dot -Tpng -O person.dot  The following image is generated:    Note  Also, see  textx command/tool  for model visualization\nfrom the command line.",
            "title": "Model visualization"
        },
        {
            "location": "/error_handling/",
            "text": "Error handling\n\u00b6\n\n\ntextX will raise an error if a syntax or semantic error is detected during\nmeta-model or model parsing/construction.\n\n\nFor a syntax error \nTextXSyntaxError\n is raised. For a semantic error\n\nTextXSemanticError\n is raised. Both exceptions inherit from \nTextXError\n. These\nexceptions are located in \ntextx.exceptions\n module.\n\n\nAll exceptions have \nmessage\n attribute with the error message, and \nline\n and\n\ncol\n attributes which represent line and column where the error was found.\n\n\n\n\nNote\n\n\nSee also \ntextx command/tool\n for (meta)model checking \nfrom command line.",
            "title": "Error handling"
        },
        {
            "location": "/error_handling/#error-handling",
            "text": "textX will raise an error if a syntax or semantic error is detected during\nmeta-model or model parsing/construction.  For a syntax error  TextXSyntaxError  is raised. For a semantic error TextXSemanticError  is raised. Both exceptions inherit from  TextXError . These\nexceptions are located in  textx.exceptions  module.  All exceptions have  message  attribute with the error message, and  line  and col  attributes which represent line and column where the error was found.   Note  See also  textx command/tool  for (meta)model checking \nfrom command line.",
            "title": "Error handling"
        },
        {
            "location": "/debugging/",
            "text": "Debugging\n\u00b6\n\n\ntextX supports debugging on the meta-model (grammar) and model levels. If\ndebugging is enabled, textX will print various debugging messages.\n\n\nIf the \ndebug\n parameter of the meta-model construction is set to \nTrue\n, debug\nmessages during grammar parsing and meta-model construction will be printed.\nAdditionally, a parse tree created during the grammar parsing as well as\nmeta-model (if constructed successfully) dot files will be generated:\n\n\nform textx.metamodel import metamodel_from_file\n\nrobot_metamodel = metamodel_from_file('robot.tx', debug=True)\n\n\n\n\nIf \ndebug\n is set in the \nmodel_from_file/str\n calls, various messages during\nthe model parsing and construction will be printed. Additionally, parse tree\ncreated from the input as well as the model will be exported to dot files.\n\n\n  robot_program = robot_metamodel.model_from_file('program.rbt', debug=True)\n\n\n\n\nAlternatively, you can use \ntextx check or visualize command\n\nin debug mode.\n\n\n$ textx -d visualize robot.tx program.rbt\n\n*** PARSING LANGUAGE DEFINITION ***\nNew rule: grammar_to_import -> RegExMatch\nNew rule: import_stm -> Sequence\nNew rule: rule_name -> RegExMatch\nNew rule: param_name -> RegExMatch\nNew rule: string_value -> OrderedChoice\nNew rule: rule_param -> Sequence\nRule rule_param founded in cache.\nNew rule: rule_params -> Sequence\n...\n\n>> Matching rule textx_model=Sequence at position 0 => */* This ex\n  >> Matching rule ZeroOrMore in textx_model at position 0 => */* This ex\n      >> Matching rule import_stm=Sequence in textx_model at position 0 => */* This ex\n        ?? Try match rule StrMatch(import) in import_stm at position 0 => */* This ex\n        >> Matching rule comment=OrderedChoice in import_stm at position 0 => */* This ex\n            ?? Try match rule comment_line=RegExMatch(//.*?$) in comment at position 0 => */* This ex\n            -- NoMatch at 0\n            ?? Try match rule comment_block=RegExMatch(/\\*(.|\\n)*?\\*/) in comment at position 0 => */* This ex\n\n...\n\n\nGenerating 'robot.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O robot.tx.dot'\nGenerating 'program.rbt.dot' file for model.\nTo convert to png run 'dot -Tpng -O program.rbt.dot'\n\n\n\nThis command renders parse trees and parser models of both textX and your\nlanguage to dot files. Also, a meta-model and model of the language will be\nrendered if parsed correctly.",
            "title": "Debugging"
        },
        {
            "location": "/debugging/#debugging",
            "text": "textX supports debugging on the meta-model (grammar) and model levels. If\ndebugging is enabled, textX will print various debugging messages.  If the  debug  parameter of the meta-model construction is set to  True , debug\nmessages during grammar parsing and meta-model construction will be printed.\nAdditionally, a parse tree created during the grammar parsing as well as\nmeta-model (if constructed successfully) dot files will be generated:  form textx.metamodel import metamodel_from_file\n\nrobot_metamodel = metamodel_from_file('robot.tx', debug=True)  If  debug  is set in the  model_from_file/str  calls, various messages during\nthe model parsing and construction will be printed. Additionally, parse tree\ncreated from the input as well as the model will be exported to dot files.    robot_program = robot_metamodel.model_from_file('program.rbt', debug=True)  Alternatively, you can use  textx check or visualize command \nin debug mode.  $ textx -d visualize robot.tx program.rbt\n\n*** PARSING LANGUAGE DEFINITION ***\nNew rule: grammar_to_import -> RegExMatch\nNew rule: import_stm -> Sequence\nNew rule: rule_name -> RegExMatch\nNew rule: param_name -> RegExMatch\nNew rule: string_value -> OrderedChoice\nNew rule: rule_param -> Sequence\nRule rule_param founded in cache.\nNew rule: rule_params -> Sequence\n...\n\n>> Matching rule textx_model=Sequence at position 0 => */* This ex\n  >> Matching rule ZeroOrMore in textx_model at position 0 => */* This ex\n      >> Matching rule import_stm=Sequence in textx_model at position 0 => */* This ex\n        ?? Try match rule StrMatch(import) in import_stm at position 0 => */* This ex\n        >> Matching rule comment=OrderedChoice in import_stm at position 0 => */* This ex\n            ?? Try match rule comment_line=RegExMatch(//.*?$) in comment at position 0 => */* This ex\n            -- NoMatch at 0\n            ?? Try match rule comment_block=RegExMatch(/\\*(.|\\n)*?\\*/) in comment at position 0 => */* This ex\n\n...\n\n\nGenerating 'robot.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O robot.tx.dot'\nGenerating 'program.rbt.dot' file for model.\nTo convert to png run 'dot -Tpng -O program.rbt.dot'  This command renders parse trees and parser models of both textX and your\nlanguage to dot files. Also, a meta-model and model of the language will be\nrendered if parsed correctly.",
            "title": "Debugging"
        },
        {
            "location": "/textx_command/",
            "text": "textx command/tool\n\u00b6\n\n\nTo check and visualize (meta)models from the command line.\n\n\n\n\nUsing the tool\n\u00b6\n\n\nTo get basic help:\n\n\n$ textx --help\nusage: textx [-h] [-i] [-d] cmd metamodel [model]\n\ntextX checker and visualizer\n\npositional arguments:\n  cmd         Command - \"check\" or \"visualize\"\n  metamodel   Meta-model file name\n  model       Model file name\n\noptional arguments:\n  -h, --help  show this help message and exit\n  -i          case-insensitive parsing\n  -d          run in debug mode\n\n\n\nYou can check and visualize (generate a .dot file) your meta-model or model using\nthis tool.\n\n\nFor example, to check and visualize a metamodel you could issue:\n\n\n$ textx visualize robot.tx\nMeta-model OK.\nGenerating 'robot.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O robot.tx.dot'\n\n\n\nCreate an image from the .dot file:\n\n\n$ dot -Tpng -O robot.tx.dot\n\n\n\nOr use some \ndot\n viewer. For example:\n\n\n$ xdot robot.tx.dot\n\n\n\nVisualize model:\n\n\n$ textx visualize robot.tx program.rbt\nMeta-model OK.\nModel OK.\nGenerating 'robot.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O robot.tx.dot'\nGenerating 'program.rbt.dot' file for model.\nTo convert to png run 'dot -Tpng -O program.rbt.dot'\n\n\n\nTo only check (meta)models use \ncheck\n command:\n\n\n$ textx check robot.tx program.rbt\n\n\n\nIf there is an error you will get a nice error report:\n\n\n$ textx check robot.tx program.rbt\nMeta-model OK.\nError in model file.\nExpected 'initial' or 'up' or 'down' or 'left' or \n  'right' or 'end' at program.rbt:(3, 3) => 'al 3, 1   *gore 4    '.",
            "title": "textx command"
        },
        {
            "location": "/textx_command/#textx-commandtool",
            "text": "To check and visualize (meta)models from the command line.",
            "title": "textx command/tool"
        },
        {
            "location": "/textx_command/#using-the-tool",
            "text": "To get basic help:  $ textx --help\nusage: textx [-h] [-i] [-d] cmd metamodel [model]\n\ntextX checker and visualizer\n\npositional arguments:\n  cmd         Command - \"check\" or \"visualize\"\n  metamodel   Meta-model file name\n  model       Model file name\n\noptional arguments:\n  -h, --help  show this help message and exit\n  -i          case-insensitive parsing\n  -d          run in debug mode  You can check and visualize (generate a .dot file) your meta-model or model using\nthis tool.  For example, to check and visualize a metamodel you could issue:  $ textx visualize robot.tx\nMeta-model OK.\nGenerating 'robot.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O robot.tx.dot'  Create an image from the .dot file:  $ dot -Tpng -O robot.tx.dot  Or use some  dot  viewer. For example:  $ xdot robot.tx.dot  Visualize model:  $ textx visualize robot.tx program.rbt\nMeta-model OK.\nModel OK.\nGenerating 'robot.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O robot.tx.dot'\nGenerating 'program.rbt.dot' file for model.\nTo convert to png run 'dot -Tpng -O program.rbt.dot'  To only check (meta)models use  check  command:  $ textx check robot.tx program.rbt  If there is an error you will get a nice error report:  $ textx check robot.tx program.rbt\nMeta-model OK.\nError in model file.\nExpected 'initial' or 'up' or 'down' or 'left' or \n  'right' or 'end' at program.rbt:(3, 3) => 'al 3, 1   *gore 4    '.",
            "title": "Using the tool"
        },
        {
            "location": "/tutorials/hello_world/",
            "text": "Hello World example\n\u00b6\n\n\nThis is an example of very simple Hello World like language.\n\n\n\n\nThese are the steps to build a very basic Hello World - like language.\n\n\n\n\n\n\nWrite a language description in textX (file \nhello.tx\n):\n\n\nHelloWorldModel:\n  'hello' to_greet+=Who[',']\n;\n\nWho:\n  name = /[^,]*/\n;\n\n\n\nDescription consists of a set of parsing rules which at the same time\ndescribe Python classes that will be dynamically created and used to\ninstantiate objects of your model.  This small example consists of two\nrules: \nHelloWorldModel\n and \nWho\n.  \nHelloWorldModel\n starts with the\nkeyword \nhello\n after which a one or more \nWho\n object must be written\nseparated by commas. \nWho\n objects will be parsed, instantiated and stored\nin a \nto_greet\n list on a \nHelloWorldModel\n object. \nWho\n objects consists\nonly of its names which must be matched the regular expression rule\n\n/[^,]*/\n (match non-comma zero or more times). Please see \ntextX\ngrammar\n section for more information on writing grammar\nrules.\n\n\n\n\n\n\nAt this point you can check and visualise meta-model using following command\n   from command line:\n\n\n$ textx visualize hello.tx\nMeta-model OK.\nGenerating 'hello.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O hello.tx.dot'\n\n\n\n\n\nYou can see that for each rule from language description an appropriate\nPython class has been created. A BASETYPE hierarchy is built-in. Each\nmeta-model has it.\n\n\n\n\n\n\nCreate some content (i.e. model) in your new language (\nexample.hello\n):\n\n\nhello World, Solar System, Universe\n\n\n\nYour language syntax is also described by language rules from step 1.\n\n\nIf we break down the text of the example model it looks like this:\n\n\n\n\nWe see that the whole line is a \nHelloWorldModel\n and the parts \nWorld\n, \n\nSolar System\n, and \nUniverse\n are \nWho\n objects. Red coloured text is\nsyntactic noise that is needed by the parser (and programmers) to recognize\nthe boundaries of the objects in the text.\n\n\n\n\n\n\nTo use your models from Python first create meta-model from textX language\n   description (file \nhello.py\n):\n\n\nfrom textx import metamodel_from_file\nhello_meta = metamodel_from_file('hello.tx')\n\n\n\n\n\n\n\nThan use meta-model to create models from textual description:\n\n\nexample_hello_model = hello_meta.model_from_file('example.hello')\n\n\n\nTextual model \nexample.hello\n will be parsed and transformed to a plain\nPython object graph. Object classes are those defined by the meta-model.\n\n\n\n\n\n\nYou can optionally export model to \ndot\n file to visualize it. Run following\n   from the command line:\n\n\n$ textx visualize hello.tx example.hello\nMeta-model OK.\nModel OK.\nGenerating 'hello.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O hello.tx.dot'\nGenerating 'example.hello.dot' file for model.\nTo convert to png run 'dot -Tpng -O example.hello.dot'\n\n\n\n\n\nThis is an object graph automatically constructed from \nexample.hello\n\nfile.\n\n\nWe see that each \nWho\n object is contained in the python attribute\n\nto_greet\n of list type which is defined by the grammar.\n\n\n\n\n\n\nUse your model: interpret it, generate code \u2026 It is a plain Python\n   graph of objects with plain attributes!\n\n\n\n\n\n\n\n\nNote\n\n\nTry out a \ncomplete tutorial\n for building a simple robot language.",
            "title": "Hello World"
        },
        {
            "location": "/tutorials/hello_world/#hello-world-example",
            "text": "This is an example of very simple Hello World like language.   These are the steps to build a very basic Hello World - like language.    Write a language description in textX (file  hello.tx ):  HelloWorldModel:\n  'hello' to_greet+=Who[',']\n;\n\nWho:\n  name = /[^,]*/\n;  Description consists of a set of parsing rules which at the same time\ndescribe Python classes that will be dynamically created and used to\ninstantiate objects of your model.  This small example consists of two\nrules:  HelloWorldModel  and  Who .   HelloWorldModel  starts with the\nkeyword  hello  after which a one or more  Who  object must be written\nseparated by commas.  Who  objects will be parsed, instantiated and stored\nin a  to_greet  list on a  HelloWorldModel  object.  Who  objects consists\nonly of its names which must be matched the regular expression rule /[^,]*/  (match non-comma zero or more times). Please see  textX\ngrammar  section for more information on writing grammar\nrules.    At this point you can check and visualise meta-model using following command\n   from command line:  $ textx visualize hello.tx\nMeta-model OK.\nGenerating 'hello.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O hello.tx.dot'   You can see that for each rule from language description an appropriate\nPython class has been created. A BASETYPE hierarchy is built-in. Each\nmeta-model has it.    Create some content (i.e. model) in your new language ( example.hello ):  hello World, Solar System, Universe  Your language syntax is also described by language rules from step 1.  If we break down the text of the example model it looks like this:   We see that the whole line is a  HelloWorldModel  and the parts  World ,  Solar System , and  Universe  are  Who  objects. Red coloured text is\nsyntactic noise that is needed by the parser (and programmers) to recognize\nthe boundaries of the objects in the text.    To use your models from Python first create meta-model from textX language\n   description (file  hello.py ):  from textx import metamodel_from_file\nhello_meta = metamodel_from_file('hello.tx')    Than use meta-model to create models from textual description:  example_hello_model = hello_meta.model_from_file('example.hello')  Textual model  example.hello  will be parsed and transformed to a plain\nPython object graph. Object classes are those defined by the meta-model.    You can optionally export model to  dot  file to visualize it. Run following\n   from the command line:  $ textx visualize hello.tx example.hello\nMeta-model OK.\nModel OK.\nGenerating 'hello.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O hello.tx.dot'\nGenerating 'example.hello.dot' file for model.\nTo convert to png run 'dot -Tpng -O example.hello.dot'   This is an object graph automatically constructed from  example.hello \nfile.  We see that each  Who  object is contained in the python attribute to_greet  of list type which is defined by the grammar.    Use your model: interpret it, generate code \u2026 It is a plain Python\n   graph of objects with plain attributes!     Note  Try out a  complete tutorial  for building a simple robot language.",
            "title": "Hello World example"
        },
        {
            "location": "/tutorials/robot/",
            "text": "Robot tutorial\n\u00b6\n\n\nIn this tutorial we will build a simple robot language to demonstrate\nthe basic workflow when working with textX.\n\n\n\n\nRobot language\n\u00b6\n\n\nWhen building a DSL we should first do a domain analysis, to see what concepts\ndo we have and what are their relationships and constraints. In the following\nparagraph a short analysis is done. Important concepts are emphasized.\n\n\nIn this case we want an imperative language that should define \nrobot\n movement\non the imaginary grid.  Robot should \nmove\n in four base \ndirection\n. We will\ncall these directions \nup, down, left\n and \nright\n (you could use north, south,\nwest and east if you like).  Additionally, we shall have a robot coordinate\ngiven in x, y \nposition\n.  For simplicity, our robot can move in discrete\n\nsteps\n. In each movement robot can move by 1 or more steps but in the same\ndirection. Coordinate is given as a pair of integer numbers. Robot will have an\n\ninitial position\n. If not given explicitly it is assumed that position is \n(0,\n0)\n.\n\n\nSo, lets build a simple robot language.\n\n\nGrammar\n\u00b6\n\n\nFirst, we need to define a grammar for the language. In textX the grammar will\nalso define a meta-model (a.k.a. abstract syntax) for the language which can be\nvisualized and be used as a part of the documentation.\n\n\nUsually we start by outlining some program in the language we are building.\n\n\nHere is an example \nprogram\n on robot language:\n\n\nbegin\n    initial 3, 1\n    up 4\n    left 9\n    down\n    right 1\nend\n\n\n\nWe have \nbegin\n and \nend\n keywords that define the beginning and end of the\nprogram. In this case we could do without these keywords but lets have it to\nmake it more interesting.\n\n\nIn between those two keywords we have a sequence of instruction. First\ninstruction will position our robot at coordinate \n(3, 1)\n. After that robot\nwill move \nup 4 steps\n, \nleft 9 steps\n, \ndown 1 step\n (1 step is the default)\nand finally \n1 step to the right\n.\n\n\nLets start with grammar definition. We shall start in a top-down manner so lets\nfirst define a program as a whole.\n\n\nProgram:\n  'begin'\n    commands*=Command\n  'end'\n;\n\n\n\nHere we see that our program is defined with sequence of:\n\n\n\n\nstring match (\n'begin'\n),\n\n\nzero or more assignment to \ncommands\n attribute,\n\n\nstring match (\n'end'\n).\n\n\n\n\nString matches will require literal strings given at the begin and end of\nprogram. If this is not satisfied a syntax error will be raised. This whole rule\n(\nProgram\n) will create a class with the same name in the meta-model. Each\nprogram will be an instance of this class. \ncommands\n assignment will result in\na python attribute \ncommands\n on the instance of \nProgram\n class. This attribute\nwill be of Python \nlist\n type (because \n*=\n assignment is used).  Each element\nof this list will be a specific command.\n\n\nNow, we see that we have different types of commands. First command has two\nparameters and it defines the robot initial position. Other commands has one or\nzero parameters and define the robot movement.\n\n\nTo state that some textX rule is specialised in 2 or more rules we use an\nabstract rule. For \nCommand\n we shall define two specializations:\n\nInitialCommand\n and \nMoveCommand\n like this:\n\n\nCommand:\n  InitialCommand | MoveCommand\n;\n\n\n\nAbstract rule is given as ordered choice of other rules. This can be read as\n\nEach command is either a InitialCommand or MoveCommand\n.\n\n\nLets now define command for setting initial position.\n\n\nInitialCommand:\n  'initial' x=INT ',' y=INT\n;\n\n\n\nThis rule specifies a class \nInitialCommand\n in the meta-model. Each initial\nposition command will be an instance of this class.\n\n\nSo, this command should start with the keyword \ninitial\n after which we give an\ninteger number (base type rule \nINT\n - this number will be available as\nattribute \nx\n on the \nInitialCommand\n instance), than a separator \n,\n is\nrequired after which we have \ny\n coordinate as integer number (this will be\navailable as attribute \ny\n). Using base type rule \nINT\n matched number from\ninput string will be automatically converted to python type \nint\n.\n\n\nNow, lets define a movement command. We know that this command consists of\ndirection identifier and optional number of steps (if not given the default will\nbe 1).\n\n\nMoveCommand:\n  direction=Direction (steps=INT)?\n;\n\n\n\nSo, the movement command model object will have two attributes.\n\ndirection\n attribute will define one of the four possible directions and\n\nsteps\n attribute will be an integer that will hold how many steps a robot\nwill move in given direction. Steps are optional so if not given in the program\nit will still be a correct syntax. Notice, that the default of 1 is not\nspecified in the grammar. The grammar deals with syntax constraints. Additional\nsemantics will be handled later in model/object processors (see below).\n\n\nNow, the missing part is \nDirection\n rule referenced from the previous rule.\nThis rule will define what can be written as a direction.  We will define this\nrule like this:\n\n\nDirection:\n  \"up\"|\"down\"|\"left\"|\"right\"\n;\n\n\n\nThis kind of rule is called a \nmatch rule\n. This rule does not result in a new\nobject. It consists of ordered choice of simple matches (string, regex), base\ntype rules (INT, STRING, BOOL...) and/or other match rule references.\n\n\nThe result of this match will be assigned to the attribute from which it was\nreferenced. If base type was used it will be converted in a proper python type.\nIf not, it will be a python string that will contain the text that was matched\nfrom the input.\n\n\nIn this case a one of 4 words will be matched and that string will be assigned\nto the \ndirection\n attribute of the \nMoveCommand\n instance.\n\n\nThe final touch to the grammar is a definition of the comment rule. We want to\ncomment our robot code, right?\n\n\nIn textX a special rule called \nComment\n is used for that purpose.\nLets define a C-style single line comments.\n\n\nComment:\n  /\\/\\/.*$/\n;\n\n\n\nOur grammar is done. Save it in \nrobot.tx\n file. The content of this file\nshould now be:\n\n\nProgram:\n  'begin'\n    commands*=Command\n  'end'\n;\n\nCommand:\n  InitialCommand | MoveCommand\n;\n\nInitialCommand:\n  'initial' x=INT ',' y=INT\n;\n\nMoveCommand:\n  direction=Direction (steps=INT)?\n;\n\nDirection:\n  \"up\"|\"down\"|\"left\"|\"right\"\n;\n\nComment:\n  /\\/\\/.*$/\n;\n\n\n\nNotice that we have not constrained initial position command to be specified\njust once on the beginning of the program. This basically means that this\ncommand can be given multiple times throughout the program. I will leave as an\nexercise to the reader to implement this constraint.\n\n\nNext step during language design is meta-model visualization. It is usually\neasier to comprehend our language if rendered graphically. To do so we use\nexcellent \nGraphViz\n software package and its DSL for\ngraph specification called \ndot\n. It is a textual language for visual graph\ndefinition.\n\n\nLets check our meta-model and export it to the dot language.\n\n\n$ textx visualize robot.tx\nMeta-model OK.\nGenerating 'robot.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O robot.tx.dot'\n\n\n\ndot\n file can be opened with dot viewer (there are many to choose from) or\ntransformed with \ndot\n tool to raster or vector graphics.\n\n\nFor example:\n\n\ndot -Tpng robot_meta.dot -O robot_meta.png\n\n\n\nThis command will create \npng\n image out of \ndot\n file.\n\n\n\n\nInstantiating meta-model\n\u00b6\n\n\nIn order to parse our models we first need to construct a meta-model. A\ntextX meta-model is a Python object that contains all classes that can be\ninstantiated in our model. For each grammar rule a class is created.\nAdditionally, meta-model contains a parser that knows how to parse input\nstrings. From parsed input (parse tree) meta-model will create a model.\n\n\nMeta-models are created from our grammar description, in this case\n\nrobot.tx\n file. Open \nrobot.py\n Python file and write following:\n\n\nfrom textx import metamodel_from_file\nrobot_mm = metamodel_from_file('robot.tx')\n\n\n\n\n\n\nNote\n\n\nThis meta-model can be used to parse multiple models.\n\n\n\n\nInstantiating model\n\u00b6\n\n\nNow, when we have our meta-model we can parse models from strings or external\ntextual files. Extend your \nrobot.py\n with:\n\n\nrobot_model = robot_mm.model_from_file('program.rbt')\n\n\n\n\nThis command will parse file \nprogram.rbt\n and constructs our robot model.\nIn this file does not match our language a syntax error will be raised on the\nfirst error encountered.\n\n\nIn the same manner as meta-model visualization we can visualize our model too.\n\n\n$ textx visualize robot.tx program.rbt\nMeta-model OK.\nModel OK.\nGenerating 'robot.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O robot.tx.dot'\nGenerating 'program.rbt.dot' file for model.\nTo convert to png run 'dot -Tpng -O program.rbt.dot'\n\n\n\nThis will create \nprogram.dot\n file that can be visualized using proper viewer\nor transformed to image.\n\n\n$ dot -Tpng program.dot -O program.png\n\n\n\nFor the robot program above we should get an image like this:\n\n\n\n\nInterpreting model\n\u00b6\n\n\nWhen we have successfully parsed and loaded our model/program (or mogram or\nprodel ;) ) we can do various stuff. Usually what would you like to do is to\ntranslate your program to some other language (Java, Python, C#, Ruby,...) or\nyou could build an interpreter that will evaluate/interpret your model directly.\nOr you could analyse your model, extract informations from it etc. It is up to\nyou to decide.\n\n\nWe will show here how to build a simple interpreter that will start the robot\nfrom the initial position and print the position of the robot after each\ncommand.\n\n\nLets imagine that we have a robot that understands our language. In your\n\nrobot.py\n file add:\n\n\nclass Robot(object):\n\n  def __init__(self):\n    # Initial position is (0,0)\n    self.x = 0\n    self.y = 0\n\n  def __str__(self):\n    return \"Robot position is {}, {}.\".format(self.x, self.y)\n\n\n\nNow, our robot will have an \ninterpret\n method that accepts our robot model and\nruns it. At each step this method will update the robot position and print it.\n\n\ndef interpret(self, model):\n\n    # model is an instance of Program\n    for c in model.commands:\n\n        if c.__class__.__name__ == \"InitialCommand\":\n            print(\"Setting position to: {}, {}\".format(c.x, c.y))\n            self.x = c.x\n            self.y = c.y\n        else:\n            dir = c.direction\n            print(\"Going {} for {} step(s).\".format(dir, c.steps))\n\n            move = {\n                \"up\": (0, 1),\n                \"down\": (0, -1),\n                \"left\": (-1, 0),\n                \"right\": (1, 0)\n            }[dir]\n\n            # Calculate new robot position\n            self.x += c.steps * move[0]\n            self.y += c.steps * move[1]\n\n        print(self)\n\n\n\nNow lets give our \nrobot_model\n to \nRobot\n instance and see what happens.\n\n\nrobot = Robot()\nrobot.interpret(robot_model)\n\n\n\n\nYou should get this output:\n\n\nSetting position to: 3, 1\nRobot position is 3, 1.\nGoing up for 4 step(s).\nRobot position is 3, 5.\nGoing left for 9 step(s).\nRobot position is -6, 5.\nGoing down for 0 step(s).\nRobot position is -6, 5.\nGoing right for 1 step(s).\nRobot position is -5, 5.\n\n\n\nIt is \nalmost\n correct. We can see that down movement is for 0 steps because we\nhave not defined the steps for \ndown\n command and haven't done anything yet to\nimplement default of 1.\n\n\nThe best way to implement default value for step is to use so called \nobject\nprocessor\n for \nMoveCommand\n.\nObject processor is a callable that gets called whenever textX parses and\ninstantiates an object of particular class. Use \nregister_obj_processors\n\nmethod on meta-model to register callables/processors for classes your wish to\nprocess in some way immediately after instantiation.\n\n\nLets define our processor for \nMoveCommand\n in \nrobot.py\n file.\n\n\ndef move_command_processor(move_cmd):\n\n  # If steps is not given, set it do default 1 value.\n  if move_cmd.steps == 0:\n    move_cmd.steps = 1\n\n\n\n\nNow, register this processor on meta-model. After meta-model construction add a\nline for registration.\n\n\nrobot_mm.register_obj_processors({'MoveCommand': move_command_processor})\n\n\n\n\nregister_obj_processors\n accepts a dictionary keyed by class name. The\nvalues are callables that should handle instances of the given class.\n\n\nIf you run robot interpreter again you will get output like this:\n\n\nSetting position to: 3, 1\nRobot position is 3, 1.\nGoing up for 4 step(s).\nRobot position is 3, 5.\nGoing left for 9 step(s).\nRobot position is -6, 5.\nGoing down for 1 step(s).\nRobot position is -6, 4.\nGoing right for 1 step(s).\nRobot position is -5, 4.\n\n\n\nAnd now our robot behaves as expected!\n\n\n\n\nNote\n\n\nThe code from this tutorial can be found in the\n\nexamples/robot\n\nfolder.\n\n\nNext, you can read \nthe Entity tutorial\n where you can see how to\ngenerate source code from your models.",
            "title": "Robot"
        },
        {
            "location": "/tutorials/robot/#robot-tutorial",
            "text": "In this tutorial we will build a simple robot language to demonstrate\nthe basic workflow when working with textX.",
            "title": "Robot tutorial"
        },
        {
            "location": "/tutorials/robot/#robot-language",
            "text": "When building a DSL we should first do a domain analysis, to see what concepts\ndo we have and what are their relationships and constraints. In the following\nparagraph a short analysis is done. Important concepts are emphasized.  In this case we want an imperative language that should define  robot  movement\non the imaginary grid.  Robot should  move  in four base  direction . We will\ncall these directions  up, down, left  and  right  (you could use north, south,\nwest and east if you like).  Additionally, we shall have a robot coordinate\ngiven in x, y  position .  For simplicity, our robot can move in discrete steps . In each movement robot can move by 1 or more steps but in the same\ndirection. Coordinate is given as a pair of integer numbers. Robot will have an initial position . If not given explicitly it is assumed that position is  (0,\n0) .  So, lets build a simple robot language.",
            "title": "Robot language"
        },
        {
            "location": "/tutorials/robot/#grammar",
            "text": "First, we need to define a grammar for the language. In textX the grammar will\nalso define a meta-model (a.k.a. abstract syntax) for the language which can be\nvisualized and be used as a part of the documentation.  Usually we start by outlining some program in the language we are building.  Here is an example  program  on robot language:  begin\n    initial 3, 1\n    up 4\n    left 9\n    down\n    right 1\nend  We have  begin  and  end  keywords that define the beginning and end of the\nprogram. In this case we could do without these keywords but lets have it to\nmake it more interesting.  In between those two keywords we have a sequence of instruction. First\ninstruction will position our robot at coordinate  (3, 1) . After that robot\nwill move  up 4 steps ,  left 9 steps ,  down 1 step  (1 step is the default)\nand finally  1 step to the right .  Lets start with grammar definition. We shall start in a top-down manner so lets\nfirst define a program as a whole.  Program:\n  'begin'\n    commands*=Command\n  'end'\n;  Here we see that our program is defined with sequence of:   string match ( 'begin' ),  zero or more assignment to  commands  attribute,  string match ( 'end' ).   String matches will require literal strings given at the begin and end of\nprogram. If this is not satisfied a syntax error will be raised. This whole rule\n( Program ) will create a class with the same name in the meta-model. Each\nprogram will be an instance of this class.  commands  assignment will result in\na python attribute  commands  on the instance of  Program  class. This attribute\nwill be of Python  list  type (because  *=  assignment is used).  Each element\nof this list will be a specific command.  Now, we see that we have different types of commands. First command has two\nparameters and it defines the robot initial position. Other commands has one or\nzero parameters and define the robot movement.  To state that some textX rule is specialised in 2 or more rules we use an\nabstract rule. For  Command  we shall define two specializations: InitialCommand  and  MoveCommand  like this:  Command:\n  InitialCommand | MoveCommand\n;  Abstract rule is given as ordered choice of other rules. This can be read as Each command is either a InitialCommand or MoveCommand .  Lets now define command for setting initial position.  InitialCommand:\n  'initial' x=INT ',' y=INT\n;  This rule specifies a class  InitialCommand  in the meta-model. Each initial\nposition command will be an instance of this class.  So, this command should start with the keyword  initial  after which we give an\ninteger number (base type rule  INT  - this number will be available as\nattribute  x  on the  InitialCommand  instance), than a separator  ,  is\nrequired after which we have  y  coordinate as integer number (this will be\navailable as attribute  y ). Using base type rule  INT  matched number from\ninput string will be automatically converted to python type  int .  Now, lets define a movement command. We know that this command consists of\ndirection identifier and optional number of steps (if not given the default will\nbe 1).  MoveCommand:\n  direction=Direction (steps=INT)?\n;  So, the movement command model object will have two attributes. direction  attribute will define one of the four possible directions and steps  attribute will be an integer that will hold how many steps a robot\nwill move in given direction. Steps are optional so if not given in the program\nit will still be a correct syntax. Notice, that the default of 1 is not\nspecified in the grammar. The grammar deals with syntax constraints. Additional\nsemantics will be handled later in model/object processors (see below).  Now, the missing part is  Direction  rule referenced from the previous rule.\nThis rule will define what can be written as a direction.  We will define this\nrule like this:  Direction:\n  \"up\"|\"down\"|\"left\"|\"right\"\n;  This kind of rule is called a  match rule . This rule does not result in a new\nobject. It consists of ordered choice of simple matches (string, regex), base\ntype rules (INT, STRING, BOOL...) and/or other match rule references.  The result of this match will be assigned to the attribute from which it was\nreferenced. If base type was used it will be converted in a proper python type.\nIf not, it will be a python string that will contain the text that was matched\nfrom the input.  In this case a one of 4 words will be matched and that string will be assigned\nto the  direction  attribute of the  MoveCommand  instance.  The final touch to the grammar is a definition of the comment rule. We want to\ncomment our robot code, right?  In textX a special rule called  Comment  is used for that purpose.\nLets define a C-style single line comments.  Comment:\n  /\\/\\/.*$/\n;  Our grammar is done. Save it in  robot.tx  file. The content of this file\nshould now be:  Program:\n  'begin'\n    commands*=Command\n  'end'\n;\n\nCommand:\n  InitialCommand | MoveCommand\n;\n\nInitialCommand:\n  'initial' x=INT ',' y=INT\n;\n\nMoveCommand:\n  direction=Direction (steps=INT)?\n;\n\nDirection:\n  \"up\"|\"down\"|\"left\"|\"right\"\n;\n\nComment:\n  /\\/\\/.*$/\n;  Notice that we have not constrained initial position command to be specified\njust once on the beginning of the program. This basically means that this\ncommand can be given multiple times throughout the program. I will leave as an\nexercise to the reader to implement this constraint.  Next step during language design is meta-model visualization. It is usually\neasier to comprehend our language if rendered graphically. To do so we use\nexcellent  GraphViz  software package and its DSL for\ngraph specification called  dot . It is a textual language for visual graph\ndefinition.  Lets check our meta-model and export it to the dot language.  $ textx visualize robot.tx\nMeta-model OK.\nGenerating 'robot.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O robot.tx.dot'  dot  file can be opened with dot viewer (there are many to choose from) or\ntransformed with  dot  tool to raster or vector graphics.  For example:  dot -Tpng robot_meta.dot -O robot_meta.png  This command will create  png  image out of  dot  file.",
            "title": "Grammar"
        },
        {
            "location": "/tutorials/robot/#instantiating-meta-model",
            "text": "In order to parse our models we first need to construct a meta-model. A\ntextX meta-model is a Python object that contains all classes that can be\ninstantiated in our model. For each grammar rule a class is created.\nAdditionally, meta-model contains a parser that knows how to parse input\nstrings. From parsed input (parse tree) meta-model will create a model.  Meta-models are created from our grammar description, in this case robot.tx  file. Open  robot.py  Python file and write following:  from textx import metamodel_from_file\nrobot_mm = metamodel_from_file('robot.tx')   Note  This meta-model can be used to parse multiple models.",
            "title": "Instantiating meta-model"
        },
        {
            "location": "/tutorials/robot/#instantiating-model",
            "text": "Now, when we have our meta-model we can parse models from strings or external\ntextual files. Extend your  robot.py  with:  robot_model = robot_mm.model_from_file('program.rbt')  This command will parse file  program.rbt  and constructs our robot model.\nIn this file does not match our language a syntax error will be raised on the\nfirst error encountered.  In the same manner as meta-model visualization we can visualize our model too.  $ textx visualize robot.tx program.rbt\nMeta-model OK.\nModel OK.\nGenerating 'robot.tx.dot' file for meta-model.\nTo convert to png run 'dot -Tpng -O robot.tx.dot'\nGenerating 'program.rbt.dot' file for model.\nTo convert to png run 'dot -Tpng -O program.rbt.dot'  This will create  program.dot  file that can be visualized using proper viewer\nor transformed to image.  $ dot -Tpng program.dot -O program.png  For the robot program above we should get an image like this:",
            "title": "Instantiating model"
        },
        {
            "location": "/tutorials/robot/#interpreting-model",
            "text": "When we have successfully parsed and loaded our model/program (or mogram or\nprodel ;) ) we can do various stuff. Usually what would you like to do is to\ntranslate your program to some other language (Java, Python, C#, Ruby,...) or\nyou could build an interpreter that will evaluate/interpret your model directly.\nOr you could analyse your model, extract informations from it etc. It is up to\nyou to decide.  We will show here how to build a simple interpreter that will start the robot\nfrom the initial position and print the position of the robot after each\ncommand.  Lets imagine that we have a robot that understands our language. In your robot.py  file add:  class Robot(object):\n\n  def __init__(self):\n    # Initial position is (0,0)\n    self.x = 0\n    self.y = 0\n\n  def __str__(self):\n    return \"Robot position is {}, {}.\".format(self.x, self.y)  Now, our robot will have an  interpret  method that accepts our robot model and\nruns it. At each step this method will update the robot position and print it.  def interpret(self, model):\n\n    # model is an instance of Program\n    for c in model.commands:\n\n        if c.__class__.__name__ == \"InitialCommand\":\n            print(\"Setting position to: {}, {}\".format(c.x, c.y))\n            self.x = c.x\n            self.y = c.y\n        else:\n            dir = c.direction\n            print(\"Going {} for {} step(s).\".format(dir, c.steps))\n\n            move = {\n                \"up\": (0, 1),\n                \"down\": (0, -1),\n                \"left\": (-1, 0),\n                \"right\": (1, 0)\n            }[dir]\n\n            # Calculate new robot position\n            self.x += c.steps * move[0]\n            self.y += c.steps * move[1]\n\n        print(self)  Now lets give our  robot_model  to  Robot  instance and see what happens.  robot = Robot()\nrobot.interpret(robot_model)  You should get this output:  Setting position to: 3, 1\nRobot position is 3, 1.\nGoing up for 4 step(s).\nRobot position is 3, 5.\nGoing left for 9 step(s).\nRobot position is -6, 5.\nGoing down for 0 step(s).\nRobot position is -6, 5.\nGoing right for 1 step(s).\nRobot position is -5, 5.  It is  almost  correct. We can see that down movement is for 0 steps because we\nhave not defined the steps for  down  command and haven't done anything yet to\nimplement default of 1.  The best way to implement default value for step is to use so called  object\nprocessor  for  MoveCommand .\nObject processor is a callable that gets called whenever textX parses and\ninstantiates an object of particular class. Use  register_obj_processors \nmethod on meta-model to register callables/processors for classes your wish to\nprocess in some way immediately after instantiation.  Lets define our processor for  MoveCommand  in  robot.py  file.  def move_command_processor(move_cmd):\n\n  # If steps is not given, set it do default 1 value.\n  if move_cmd.steps == 0:\n    move_cmd.steps = 1  Now, register this processor on meta-model. After meta-model construction add a\nline for registration.  robot_mm.register_obj_processors({'MoveCommand': move_command_processor})  register_obj_processors  accepts a dictionary keyed by class name. The\nvalues are callables that should handle instances of the given class.  If you run robot interpreter again you will get output like this:  Setting position to: 3, 1\nRobot position is 3, 1.\nGoing up for 4 step(s).\nRobot position is 3, 5.\nGoing left for 9 step(s).\nRobot position is -6, 5.\nGoing down for 1 step(s).\nRobot position is -6, 4.\nGoing right for 1 step(s).\nRobot position is -5, 4.  And now our robot behaves as expected!   Note  The code from this tutorial can be found in the examples/robot \nfolder.  Next, you can read  the Entity tutorial  where you can see how to\ngenerate source code from your models.",
            "title": "Interpreting model"
        },
        {
            "location": "/tutorials/entity/",
            "text": "Entity tutorial\n\u00b6\n\n\nA tutorial for building ER-like language and generating Java code.\n\n\n\n\nEntity language\n\u00b6\n\n\nIn this example we will see how to make a simple language for data modeling.\nWe will use this language to generate Java source code (POJO classes).\n\n\nOur main concept will be \nEntity\n. Each entity will have one or more\n\nproperties\n.  Each property is defined by its \nname\n and its \ntype\n.\n\n\nLet's sketch out a model in our language.\n\n\nentity Person {\n  name : string       \n  address: Address   \n  age: integer      \n}\n\nentity Address {\n  street : string\n  city : string\n  country : string\n}\n\n\n\nThe grammar\n\u00b6\n\n\nIn our example we see that each entity starts with a keyword \nentity\n. After\nthat, we have a name that is the identifier and an open bracket.\nBetween the brackets we have properties. In textX this is written as:\n\n\nEntity:\n    'entity' name=ID '{'\n        properties+=Property\n    '}'\n;\n\n\n\nWe can see that the \nEntity\n rule references \nProperty\n rule from the\nassignment. Each property is defined by the \nname\n, colon (\n:\n) and the\ntype's name. This can be written as:\n\n\nProperty:\n    name=ID ':' type=ID\n;\n\n\n\nNow, grammar defined in this way will parse a single \nEntity\n. We haven't stated yet\nthat our model consists of many \nEntity\n instances.\n\n\nLet's specify that. We are introducing a rule for the whole model which states\nthat each entity model contains one or more \nentities\n.\n\n\nEntityModel:\n    entities+=Entity\n;\n\n\n\nThis rule must be the first rule in the textX grammar file. First rule is always\nconsidered a \nroot rule\n.\n\n\nThis grammar will parse the example model from the beginning.\n\n\nAt any time you can check and visualize entity meta-model and person model\nusing command:\n\n\n$ textx visualize entity.tx person.ent\n\n\n\nGiven grammar in file \nentity.tx\n and example Person model in file\n\nperson.ent\n.\n\n\nThis command will produce \nentity.tx.dot\n and \nperson.ent.dot\n files which can\nbe viewed by any dot viewer or converted to e.g. PNG format using command:\n\n\n$ dot -Tpng -O *.dot\n\n\n\nNote that \nGraphViz\n must be installed to use dot command\nline utility.\n\n\nMeta-model now looks like this:\n\n\n\n\nWhile the example (Person model) looks like this:\n\n\n\n\nWhat you see on the model diagram are actual Python objects.\nIt looks good, but it would be even better if a reference to \nAddress\n from\nproperties was an actual Python reference, not just a value of \nstr\n type.\n\n\nThis resolving of object names to references can be done automatically by textX.\nTo do so, we shall change our \nProperty\n rule to be:\n\n\nProperty:\n    name=ID ':' type=[Entity]\n;\n\n\n\nNow, we state that type is a reference (we are using \n[]\n) to an object of the \nEntity\n\nclass. This instructs textX to search for the name of the \nEntity\n after the \ncolon and when it is found to resolve it to an \nEntity\n instance with the same name\ndefined elsewhere in the model.\n\n\nBut, we have a problem now. There are no entities called \nstring\n and \ninteger\n\nwhich we used for several properties in our model. To remedy this, we must \nintroduce dummy entities with those names and change \nproperties\n attribute\nassignment to be \nzero or more\n (\n*=\n) since our dummy entities will have no\nattributes.\n\n\nAlthough, this solution is possible it wouldn't be elegant at all. So let's\ndo something better. First, let's introduce an abstract concept called \nType\n\nwhich as the generalization of simple types (like \ninteger\n and \nstring\n) and\ncomplex types (like \nEntity\n).\n\n\nType:\n  SimpleType | Entity \n;\n\n\n\nThis is called abstract rule, and it means that \nType\n is either a \nSimpleType\n\nor an \nEntity\n instance. \nType\n class from the meta-model will never be\ninstantiated.\n\n\nNow, we shall change our \nProperty\n rule definition:\n\n\nProperty:\n    name=ID ':' type=[Type]\n;\n\n\n\nAnd, at the end, there must be a way to specify our simple types. Let's do that\nat the beginning of our model.\n\n\nEntityModel:\n    simple_types *= SimpleType\n    entities += Entity\n;\n\n\n\nAnd the definition of \nSimpleType\n would be:\n\n\nSimpleType:\n  'type' name=ID\n;\n\n\n\nSo, simple types are defined at the beginning of the model using the keyword \ntype\n\nafter which we specify the name of the type.\n\n\nOur person model will now begin with:\n\n\ntype string\ntype integer\n\nentity Person {\n...\n\n\n\nMeta-model now looks like this:\n\n\n\n\nWhile the example (Person model) looks like this:\n\n\n\n\nBut, we can make this language even better. We can define some built-in simple\ntypes so that the user does not need to specify them for every model. This has\nto be done from python during meta-model instantiation. We can instantiate\n\ninteger\n and \nstring\n simple types and introduce them in every model\nprogrammatically.\n\n\nThe first problem is how to instantiate the \nSimpleType\n class. textX will dynamically\ncreate a Python class for each rule from the grammar but we do not have access\nto these classes in advance.\n\n\nLuckily, textX offers a way to override dynamically created classes with user\nsupplied ones. So, we can create our class \nSimpleType\n and register that class\nduring meta-model instantiation together with two of its instances (\ninteger\n and\n\nstring\n).\n\n\nclass SimpleType(object):\n  def __init__(self, parent, name):  # remember to include parent param.\n    self.parent = parent\n    self.name = name\n\n\n\nNow, we can make a dict of builtin objects.\n\n\nmyobjs =  {\n    'integer': SimpleType(None, 'integer')\n    'string': SimpleType(None, 'string')\n  }\n\n\nAnd register our custom class and two builtins on the meta-model:\n\n\nmeta = metamodel_from_file('entity.tx', \n                           classes=[SimpleType],\n                           builtins=myobjs)\n\n\n\nNow, if we use \nmeta\n to load our models we do not have to specify \ninteger\n and\n\nstring\n types. Furthermore, each instance of \nSimpleType\n will be an instance\nof our \nSimpleType\n class.\n\n\nWe, can use this custom classes support to implement any custom behaviour in\nour object graph.\n\n\nGenerating source code\n\u00b6\n\n\ntextX doesn't impose any specific library or process for source code generation.\nYou can use anything you like. From the \nprint\n function to template engines.\n\n\nI highly recommend you to use some of the well-established \ntemplate\nengines\n.\n\n\nHere, we will see how to use \nJinja2 template engine\n\nto generate Java source code from our entity models.\n\n\nFirst, install jinja2 using pip:\n\n\n$ pip install Jinja2\n\n\n\nNow, for each entity in our model we will render one Java file with a pair of \ngetters and setters for each property.\n\n\nLet's write Jinja2 template (file \njava.template\n):\n\n\n// Autogenerated from java.template file\nclass {{entity.name}} {\n\n  {% for property in entity.properties %}\n  protected {{property.type|javatype}} {{property.name}};\n  {% endfor %}\n\n  {% for property in entity.properties %}\n  public {{property.type|javatype}} get{{property.name|capitalize}}(){\n    return this.{{property.name}};\n  }\n\n  public void set{{property.name|capitalize}}({{property.type|javatype}} new_value){\n    this.{{property.name}} = new_value;\n  }\n\n  {% endfor %}\n}\n\n\n\nTemplates have static parts that will be rendered as they are, and variable parts \nwhose content depends on the model. Variable parts are written inside\n\n{{}}\n. For example \n{{entity.name}}\n from the second line is the name of\nthe current entity.\n\n\nThe logic of rendering is controlled by \ntags\n written in \n{%...%}\n (e.g. loops,\nconditions).\n\n\nWe can see that this template will render a warning that this is auto-generated\ncode (it is always good to do that!). Then it will render a Java class named after\nthe current entity and then, for each property in the entity (please note that\nwe are using textX model so all attribute names come from the textX grammar) we\nare rendering a Java attribute. After that, we are rendering getters and\nsetters.\n\n\nYou could notice that for rendering proper Java types we are using \n|javatype\n\nexpression. This is called \nfilter\n in Jinja2. It works similarly to \nunix\n pipes.\nYou have an object and you pass it to some filter. Filter will transform the given\nobject to some other object. In this case \njavatype\n is a simple python function\nthat will transform our types (\ninteger\n and \nstring\n) to proper Java types\n(\nint\n and \nString\n).\n\n\nNow, let's see how we can put this together. We need to initialize the Jinja2\nengine, instantiate our meta-model, load our model and then iterate over \nthe entities from our model and generate a Java file for each entity:\n\n\nfrom os import mkdir\nfrom os.path import exists, dirname, join\nimport jinja2\nfrom textx import metamodel_from_file\n\nthis_folder = dirname(__file__)\n\n\nclass SimpleType(object):\n    def __init__(self, parent, name):\n        self.parent = parent\n        self.name = name\n\n    def __str__(self):\n        return self.name\n\n\ndef get_entity_mm():\n    \"\"\"\n    Builds and returns a meta-model for Entity language.\n    \"\"\"\n    type_builtins = {\n            'integer': SimpleType(None, 'integer'),\n            'string': SimpleType(None, 'string')\n    }\n    entity_mm = metamodel_from_file(join(this_folder, 'entity.tx'),\n                                    classes=[SimpleType],\n                                    builtins=type_builtins)\n\n    return entity_mm\n\n\ndef main(debug=False):\n\n    # Instantiate the Entity meta-model\n    entity_mm = get_entity_mm()\n\n    def javatype(s):\n        \"\"\"\n        Maps type names from SimpleType to Java.\n        \"\"\"\n        return {\n                'integer': 'int',\n                'string': 'String'\n        }.get(s.name, s.name)\n\n    # Create the output folder\n    srcgen_folder = join(this_folder, 'srcgen')\n    if not exists(srcgen_folder):\n        mkdir(srcgen_folder)\n\n    # Initialize the template engine.\n    jinja_env = jinja2.Environment(\n        loader=jinja2.FileSystemLoader(this_folder),\n        trim_blocks=True,\n        lstrip_blocks=True)\n\n    # Register the filter for mapping Entity type names to Java type names.\n    jinja_env.filters['javatype'] = javatype\n\n    # Load the Java template\n    template = jinja_env.get_template('java.template')\n\n    # Build a Person model from person.ent file\n    person_model = entity_mm.model_from_file(join(this_folder, 'person.ent'))\n\n    # Generate Java code\n    for entity in person_model.entities:\n        # For each entity generate java file\n        with open(join(srcgen_folder,\n                      \"%s.java\" % entity.name.capitalize()), 'w') as f:\n            f.write(template.render(entity=entity))\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nAnd the generated code will look like this:\n\n\n// Autogenerated from java.template file\nclass Person {\n\n  protected String name;\n  protected Address address;\n  protected int age;\n\n  public String getName(){\n    return this.name;\n  }\n\n  public void setName(String new_value){\n    this.name = new_value;\n  }\n\n  public Address getAddress(){\n    return this.address;\n  }\n\n  public void setAddress(Address new_value){\n    this.address = new_value;\n  }\n\n  public int getAge(){\n    return this.age;\n  }\n\n  public void setAge(int new_value){\n    this.age = new_value;\n  }\n\n}\n\n\n\n\n\nNote\n\n\nThe code from this tutorial can be found in the\n\nexamples/Entity\n\nfolder.",
            "title": "Entity"
        },
        {
            "location": "/tutorials/entity/#entity-tutorial",
            "text": "A tutorial for building ER-like language and generating Java code.",
            "title": "Entity tutorial"
        },
        {
            "location": "/tutorials/entity/#entity-language",
            "text": "In this example we will see how to make a simple language for data modeling.\nWe will use this language to generate Java source code (POJO classes).  Our main concept will be  Entity . Each entity will have one or more properties .  Each property is defined by its  name  and its  type .  Let's sketch out a model in our language.  entity Person {\n  name : string       \n  address: Address   \n  age: integer      \n}\n\nentity Address {\n  street : string\n  city : string\n  country : string\n}",
            "title": "Entity language"
        },
        {
            "location": "/tutorials/entity/#the-grammar",
            "text": "In our example we see that each entity starts with a keyword  entity . After\nthat, we have a name that is the identifier and an open bracket.\nBetween the brackets we have properties. In textX this is written as:  Entity:\n    'entity' name=ID '{'\n        properties+=Property\n    '}'\n;  We can see that the  Entity  rule references  Property  rule from the\nassignment. Each property is defined by the  name , colon ( : ) and the\ntype's name. This can be written as:  Property:\n    name=ID ':' type=ID\n;  Now, grammar defined in this way will parse a single  Entity . We haven't stated yet\nthat our model consists of many  Entity  instances.  Let's specify that. We are introducing a rule for the whole model which states\nthat each entity model contains one or more  entities .  EntityModel:\n    entities+=Entity\n;  This rule must be the first rule in the textX grammar file. First rule is always\nconsidered a  root rule .  This grammar will parse the example model from the beginning.  At any time you can check and visualize entity meta-model and person model\nusing command:  $ textx visualize entity.tx person.ent  Given grammar in file  entity.tx  and example Person model in file person.ent .  This command will produce  entity.tx.dot  and  person.ent.dot  files which can\nbe viewed by any dot viewer or converted to e.g. PNG format using command:  $ dot -Tpng -O *.dot  Note that  GraphViz  must be installed to use dot command\nline utility.  Meta-model now looks like this:   While the example (Person model) looks like this:   What you see on the model diagram are actual Python objects.\nIt looks good, but it would be even better if a reference to  Address  from\nproperties was an actual Python reference, not just a value of  str  type.  This resolving of object names to references can be done automatically by textX.\nTo do so, we shall change our  Property  rule to be:  Property:\n    name=ID ':' type=[Entity]\n;  Now, we state that type is a reference (we are using  [] ) to an object of the  Entity \nclass. This instructs textX to search for the name of the  Entity  after the \ncolon and when it is found to resolve it to an  Entity  instance with the same name\ndefined elsewhere in the model.  But, we have a problem now. There are no entities called  string  and  integer \nwhich we used for several properties in our model. To remedy this, we must \nintroduce dummy entities with those names and change  properties  attribute\nassignment to be  zero or more  ( *= ) since our dummy entities will have no\nattributes.  Although, this solution is possible it wouldn't be elegant at all. So let's\ndo something better. First, let's introduce an abstract concept called  Type \nwhich as the generalization of simple types (like  integer  and  string ) and\ncomplex types (like  Entity ).  Type:\n  SimpleType | Entity \n;  This is called abstract rule, and it means that  Type  is either a  SimpleType \nor an  Entity  instance.  Type  class from the meta-model will never be\ninstantiated.  Now, we shall change our  Property  rule definition:  Property:\n    name=ID ':' type=[Type]\n;  And, at the end, there must be a way to specify our simple types. Let's do that\nat the beginning of our model.  EntityModel:\n    simple_types *= SimpleType\n    entities += Entity\n;  And the definition of  SimpleType  would be:  SimpleType:\n  'type' name=ID\n;  So, simple types are defined at the beginning of the model using the keyword  type \nafter which we specify the name of the type.  Our person model will now begin with:  type string\ntype integer\n\nentity Person {\n...  Meta-model now looks like this:   While the example (Person model) looks like this:   But, we can make this language even better. We can define some built-in simple\ntypes so that the user does not need to specify them for every model. This has\nto be done from python during meta-model instantiation. We can instantiate integer  and  string  simple types and introduce them in every model\nprogrammatically.  The first problem is how to instantiate the  SimpleType  class. textX will dynamically\ncreate a Python class for each rule from the grammar but we do not have access\nto these classes in advance.  Luckily, textX offers a way to override dynamically created classes with user\nsupplied ones. So, we can create our class  SimpleType  and register that class\nduring meta-model instantiation together with two of its instances ( integer  and string ).  class SimpleType(object):\n  def __init__(self, parent, name):  # remember to include parent param.\n    self.parent = parent\n    self.name = name  Now, we can make a dict of builtin objects.  myobjs =  {\n    'integer': SimpleType(None, 'integer')\n    'string': SimpleType(None, 'string')\n  }  And register our custom class and two builtins on the meta-model:  meta = metamodel_from_file('entity.tx', \n                           classes=[SimpleType],\n                           builtins=myobjs)  Now, if we use  meta  to load our models we do not have to specify  integer  and string  types. Furthermore, each instance of  SimpleType  will be an instance\nof our  SimpleType  class.  We, can use this custom classes support to implement any custom behaviour in\nour object graph.",
            "title": "The grammar"
        },
        {
            "location": "/tutorials/entity/#generating-source-code",
            "text": "textX doesn't impose any specific library or process for source code generation.\nYou can use anything you like. From the  print  function to template engines.  I highly recommend you to use some of the well-established  template\nengines .  Here, we will see how to use  Jinja2 template engine \nto generate Java source code from our entity models.  First, install jinja2 using pip:  $ pip install Jinja2  Now, for each entity in our model we will render one Java file with a pair of \ngetters and setters for each property.  Let's write Jinja2 template (file  java.template ):  // Autogenerated from java.template file\nclass {{entity.name}} {\n\n  {% for property in entity.properties %}\n  protected {{property.type|javatype}} {{property.name}};\n  {% endfor %}\n\n  {% for property in entity.properties %}\n  public {{property.type|javatype}} get{{property.name|capitalize}}(){\n    return this.{{property.name}};\n  }\n\n  public void set{{property.name|capitalize}}({{property.type|javatype}} new_value){\n    this.{{property.name}} = new_value;\n  }\n\n  {% endfor %}\n}  Templates have static parts that will be rendered as they are, and variable parts \nwhose content depends on the model. Variable parts are written inside {{}} . For example  {{entity.name}}  from the second line is the name of\nthe current entity.  The logic of rendering is controlled by  tags  written in  {%...%}  (e.g. loops,\nconditions).  We can see that this template will render a warning that this is auto-generated\ncode (it is always good to do that!). Then it will render a Java class named after\nthe current entity and then, for each property in the entity (please note that\nwe are using textX model so all attribute names come from the textX grammar) we\nare rendering a Java attribute. After that, we are rendering getters and\nsetters.  You could notice that for rendering proper Java types we are using  |javatype \nexpression. This is called  filter  in Jinja2. It works similarly to  unix  pipes.\nYou have an object and you pass it to some filter. Filter will transform the given\nobject to some other object. In this case  javatype  is a simple python function\nthat will transform our types ( integer  and  string ) to proper Java types\n( int  and  String ).  Now, let's see how we can put this together. We need to initialize the Jinja2\nengine, instantiate our meta-model, load our model and then iterate over \nthe entities from our model and generate a Java file for each entity:  from os import mkdir\nfrom os.path import exists, dirname, join\nimport jinja2\nfrom textx import metamodel_from_file\n\nthis_folder = dirname(__file__)\n\n\nclass SimpleType(object):\n    def __init__(self, parent, name):\n        self.parent = parent\n        self.name = name\n\n    def __str__(self):\n        return self.name\n\n\ndef get_entity_mm():\n    \"\"\"\n    Builds and returns a meta-model for Entity language.\n    \"\"\"\n    type_builtins = {\n            'integer': SimpleType(None, 'integer'),\n            'string': SimpleType(None, 'string')\n    }\n    entity_mm = metamodel_from_file(join(this_folder, 'entity.tx'),\n                                    classes=[SimpleType],\n                                    builtins=type_builtins)\n\n    return entity_mm\n\n\ndef main(debug=False):\n\n    # Instantiate the Entity meta-model\n    entity_mm = get_entity_mm()\n\n    def javatype(s):\n        \"\"\"\n        Maps type names from SimpleType to Java.\n        \"\"\"\n        return {\n                'integer': 'int',\n                'string': 'String'\n        }.get(s.name, s.name)\n\n    # Create the output folder\n    srcgen_folder = join(this_folder, 'srcgen')\n    if not exists(srcgen_folder):\n        mkdir(srcgen_folder)\n\n    # Initialize the template engine.\n    jinja_env = jinja2.Environment(\n        loader=jinja2.FileSystemLoader(this_folder),\n        trim_blocks=True,\n        lstrip_blocks=True)\n\n    # Register the filter for mapping Entity type names to Java type names.\n    jinja_env.filters['javatype'] = javatype\n\n    # Load the Java template\n    template = jinja_env.get_template('java.template')\n\n    # Build a Person model from person.ent file\n    person_model = entity_mm.model_from_file(join(this_folder, 'person.ent'))\n\n    # Generate Java code\n    for entity in person_model.entities:\n        # For each entity generate java file\n        with open(join(srcgen_folder,\n                      \"%s.java\" % entity.name.capitalize()), 'w') as f:\n            f.write(template.render(entity=entity))\n\n\nif __name__ == \"__main__\":\n    main()  And the generated code will look like this:  // Autogenerated from java.template file\nclass Person {\n\n  protected String name;\n  protected Address address;\n  protected int age;\n\n  public String getName(){\n    return this.name;\n  }\n\n  public void setName(String new_value){\n    this.name = new_value;\n  }\n\n  public Address getAddress(){\n    return this.address;\n  }\n\n  public void setAddress(Address new_value){\n    this.address = new_value;\n  }\n\n  public int getAge(){\n    return this.age;\n  }\n\n  public void setAge(int new_value){\n    this.age = new_value;\n  }\n\n}   Note  The code from this tutorial can be found in the examples/Entity \nfolder.",
            "title": "Generating source code"
        },
        {
            "location": "/tutorials/state_machine/",
            "text": "State machine language\n\u00b6\n\n\nThis is a video tutorial that explains the implementation of \nthe StateMachine example\n.\n\n\n\n\nSee \nthe blog post about this language\nimplementation\n.",
            "title": "State Machine"
        },
        {
            "location": "/tutorials/state_machine/#state-machine-language",
            "text": "This is a video tutorial that explains the implementation of  the StateMachine example .   See  the blog post about this language\nimplementation .",
            "title": "State machine language"
        },
        {
            "location": "/tutorials/toylanguage/",
            "text": "Toy language compiler\n\u00b6\n\n\nA toy language compiler tutorial\n\n\n\n\nWindel Bouwman\n wrote an \nexcellent\ntutorial\n for using textX\nand \nppci\n to write a\ncompiler for a simple language.",
            "title": "Toy language compiler"
        },
        {
            "location": "/tutorials/toylanguage/#toy-language-compiler",
            "text": "A toy language compiler tutorial   Windel Bouwman  wrote an  excellent\ntutorial  for using textX\nand  ppci  to write a\ncompiler for a simple language.",
            "title": "Toy language compiler"
        },
        {
            "location": "/about/comparison/",
            "text": "Comparing textX to other tools\n\u00b6\n\n\nThere are generally two classes of textual DSL tools textX could be compared to.\n\n\nThe first class comprises tools that use traditional parsing technologies, i.e.\ngiven a grammar (usually a \ncontext-free\ngrammar\n) they produce\nprogram code capable of recognizing whether a given textual input conforms to\nthe given grammar. Furthermore, they enable either transformation of textual\ninput to a tree structure (i.e. parse tree), that is processed afterwards, or\ndefinition of actions that should be executed during parsing if a particular\npattern is recognized. Most popular representatives in this class are \nlex and\nyacc\n, \nANTLR\n, \nGNU\nbison\n, These kind of tools are generally\nknown by the name \nParser Generators\n.\n\n\ntextX's differences in regard to this first class are following:\n\n\n\n\ntextX works as grammar interpreter i.e. parser code is not generated by the\n  tool but the tools is configured by the grammar to recognize textual input on\n  the language specified by the grammar. You can even embed your grammar as a\n  Python string. This enables faster round-trip from grammar to the working\n  parsers as the parser don't need to be regenerated but only reconfigured.\n\n\nMost of the classical parsing tools use context-free grammars while textX\n  uses \nPEG\n  grammars\n. The\n  consequences are that lookahead is unlimited and there are no ambiguities\n  possible as the \nalternative operator is ordered\n.\n  Additionally, there is no need for a separate lexer.\n\n\ntextX uses a single textual specification (grammar) to define not only the\n  syntax of the language but also its meta-model (a.k.a. abstract syntax). The\n  textX's meta-language is inspired by Xtext. This is very important feature\n  which enables automatic construction of the model (a.k.a. abstract semantic\n  graph - ASG or semantic model) without further work from the language\n  designer. In traditional parsing tools transformation to the model usually\n  involves coding of parse actions or manually written parse tree\n  transformation.\n\n\n\n\nThe second class of textual DSL tools are more powerful tools geared especially\ntowards DSL construction. These kind of tools are generally known by the name\n\nLanguage Workbenches\n coined by Martin\nFowler\n. Most popular\nrepresentatives of this class are \nXtext\n,\n\nSpoofax\n and\n\nMPS\n. These tools are much more complex,\nhighly integrated to the particular development environment (IDE) but provide\npowerful tooling infrastructure for language development, debugging and\nevolving. These tools will build not only parser but also a language-specific\neditor, debugger, validator, visualiser etc.\n\n\ntextX is positioned between these two classes of DSL tools. The goal of textX\nproject is not a highly sophisticated DSL engineering platform but a simple DSL\nPython library that can be used in various Python applications and development\nenvironment. It can also be used for non-Python development using code\ngeneration from textX models (see \nEntity tutorial\n).\nTooling infrastructure, editor support etc. will be developed as independent\nprojects (see for example\n\ntextx-tools\n).\n\n\nDifference to Xtext grammar language\n\u00b6\n\n\ntextX grammar language is inspired by Xtext and thus there are a lot of\nsimilarities between these tools when it come to grammar specification. But,\nthere are also differences in several places. In this section we shall outline\nthose differences to give users already familiar with Xtext a brief overview.\n\n\nLexer and terminal rules\n\u00b6\n\n\ntextX uses PEG parsing which doesn't needs separate lexing phase.  This\neliminate the need to define lexemes in the grammar.  Therefore, there is no\n\nterminal\n keyword in the textX nor any of special terminal definition rules\nused by Xtext.\n\n\nTypes used for rules\n\u00b6\n\n\nXtext integrates tightly with Java and Ecore typing system providing keyword\n\nreturns\n in rule definition by which language designer might define a class\nused to instantiate objects recognized by the parser.\n\n\ntextX integrates with Python typing system. In textX there is no keyword\n\nreturns\n. The class used for the rule will be dynamically created Python class\nfor all \nnon-match rules\n. Language designer can\nprovide class using \nuser classes\n registration on meta-model.  If the rule\nis of [match type] than it will always return Python string or some of base\nPython types for \nBASETYPES inherited rules\n.\n\n\nAssignments\n\u00b6\n\n\nIn textX there are two types of \nmany\n assignments (\n*=\n - zero or more, \n+=\n -\none or more) whereas in Xtext there is only one (\n+=\n) which defines the type\nof the inferred attribute but doesn't specify any information for the parser.\nThus, if there should be zero or more matched elements you must additionally\nwrap your expression in \nzero or more\n match:\n\n\nIn Xtext:\n\n\nDomainmodel :\n    (elements+=Type)*;\n\n\n\nIn textX:\n\n\nDomainmodel :\n    elements*=Type;\n\n\n\nSimilarly, optional assignment in Xtext is written as:\n\n\nstatic?='static'?\n\n\n\nIn textX a '?' at the end of the expression is implied, i.e. rhs of the\nassignment will be optional:\n\n\nstatic?='static'\n\n\n\nRegular expression match\n\u00b6\n\n\nIn Xtext \nterminal rules are described using\nEBNF\n.\n\n\nIn textX there is no difference between parser and terminal rules so you can\nuse the full textX language to define terminals. Furthermore, textX gives you\nthe full power of \nPython regular\nexpressions\n through\n\nregular expression match\n. Regex matches are defined\ninside \n/ /\n. Anything you can use in Python \nre\nmodule\n you can use here.  This\ngives you quite powerful sublanguage for pattern definition.\n\n\nIn Xtext:\n\n\nterminal ASCII:\n    '0x' ('0'..'7') ('0'..'9'|'A'..'F');\n\n\n\nIn textX:\n\n\nASCII:\n    /0x[0-7]([0-9]|[A-F])/;\n\n\n\nLiteral Regex match can be used anywhere a regular match rule can be used.\n\n\nFor example:\n\n\nPerson:\n    name=/[a-zA-Z]+/ age=INT;\n\n\n\nRepetition modifiers\n\u00b6\n\n\ntextX provides a syntactic construct called \nrepetition\nmodifier\n which enables parser to be altered\nduring parsing of a specific repetition expression.\n\n\nFor example, there is often a need to define a separated list of elements.\n\n\nTo match a list of integers separated by comma in Xtext you would write:\n\n\nlist_of_ints+=INT (',' list_of_ints+=INT)*\n\n\n\nIn textX the same expression can be written as:\n\n\nlist_of_inst+=INT[',']\n\n\n\nThe parser is instructed to parse one or more INT with commas in between.\nRepetition modifier can be a regular expression match too.\n\n\nFor example, to match one or more integer separated by comma or semi-colon:\n\n\nlist_of_ints+=INT[/,|;/]\n\n\n\nInside square brackets more than one repetition modifier can be defined.  See\n\nsection in the docs\n for additional\nexplanations.\n\n\nWe are not aware of the similar feature in Xtext.\n\n\nRule modifiers\n\u00b6\n\n\nSimilarly to repetition modifiers, in textX parser can be altered at the rule\nlevel too. Currently, only white-space alteration can be defined on the rule\nlevel:\n\n\nFor example:\n\n\n    Rule:\n        'entity' name=ID /\\s*/ call=Rule2;\n    Rule2[noskipws]:\n        'first' 'second';\n\n\n\nParser will be altered for \nRule2\n not to skip white-spaces. All rules down the\ncall chain inherit modifiers.\n\n\nThere are hidden rules in Xtext which can achieve the similar effect, even\ndefine different kind of tokens that can be hidden from the semantic model, but\nthe rule modifier in textX serve different purpose. It is a general mechanism\nfor parser alteration per rule that can be used in the future to define some\nother alteration (e.g. case sensitivity).\n\n\nUnordered groups\n\u00b6\n\n\nXtext support unordered groups using \n&\n operator.\n\n\nFor example:\n\n\nModifier: \n    static?='static'? & final?='final'? & visibility=Visibility;\n\nenum Visibility:\n    PUBLIC='public' | PRIVATE='private' | PROTECTED='protected';\n\n\n\nIn textX unordered groups are specified as a special kind of repetitions. Thus,\nrepetition modifiers can be applied also:\n\n\nModifier: \n    (static?='static' final?='final' visibility=Visibility)#[',']\n\nVisibility:\n    'public' | 'private' | 'protected';\n\n\n\nPrevious example will match any of the following:\n\n\nprivate, static, final\nstatic, private, final\n...\n\n\n\nNotice the use of \n,\n separator as a repetition modifier.\n\n\nSyntactic predicates\n\u00b6\n\n\ntextX is based on PEG grammars. Unlike CFGs, PEGs can't be ambiguous, i.e. if\nan input parses it has exactly one parse tree. textX is backtracking parser and\nwill try each alternative in predetermined order until it succeeds. Thus, textX\ngrammar can't be ambiguous. Nevertheless, sometimes it is not possible to\nspecify desired parse tree by reordering alternatives. In that case syntactic\npredicates are used. textX implements both \nand- and not- syntactic\npredicates\n.\n\n\nOn the other hand, predictive non-backtracking parsers (as is ANTLR used by\nXtext) must make a decision which alternative to chose. Thus, grammar might be\nambiguous and additional specification is needed by a language designer to\nresolve ambiguity and choose desired parse tree. Xtext uses a positive\nlookahead syntactic predicates (=> and ->).  See\n\nhere\n.\n\n\nHidden rules\n\u00b6\n\n\nXtext uses \nhidden terminal\nsymbols\n\nto suppress non-important parts of the input from the semantic model. This is\nused for comments, whitespaces etc.  Terminal rules are referenced from the\n\nhidden\n list in the parser rules. All rules called from the one using hidden\nterminals inherits them.\n\n\ntextX provides support for whitespaces alteration on the parser level and rule\nlevel and a special \nComment match rule\n that can\nbe used to describe comments pattern which are suppressed from the model.\nComment rule is currently defined for the whole grammar, i.e. can't be altered\non a per-rule basis.\n\n\nParent-child relationships\n\u00b6\n\n\ntextX will provide explicit \nparent\n reference on all objects that are\n\ncontained\n inside some other objects.  This attribute is a plain Python\nattribute. The relationship is imposed by the grammar.\n\n\nXtext, begin based on Ecore, provides similar mechanism through Ecore API.\n\n\nEnums\n\u00b6\n\n\nXtext support \nEnum\nrules\n\nwhile textX does not. In textX you use match rule with ordered choice to mimic\nenums.\n\n\nScoping\n\u00b6\n\n\nAt present stage textX doesn't provide builtin mechanism for scoping definition.\nHowever, this can be done in Python using object processors but there is no\nspecific scoping API that could help language developer in resolving links.\n\n\nXtext does provide a \nScoping\nAPI\n\nwhich can be used by the Xtend code to specify scoping rules.\n\n\nAdditional differences in the tool usage\n\u00b6\n\n\nSome of the differences in tools usage are outlined here.\n\n\nREPL\n\u00b6\n\n\ntextX is Python based, thus it is easy to interactively play with it on the\nPython console.\n\n\nExample ipython session:\n\n\nIn [1]: from textx import metamodel_from_str\n\nIn [2]: mm = metamodel_from_str(\"\"\"\n...: Model: points+=Point;\n...: Point: x=INT ',' y=INT ';';\n...: \"\"\")\n\nIn [3]: model = mm.model_from_str(\"\"\"\n...: 34, 45; 56, 78; 88, 12;\"\"\")\n\nIn [4]: model.points\nOut[4]: \n[<textx:Point object at 0x7fdfb4cda828>,\n<textx:Point object at 0x7fdfb4cdada0>,\n<textx:Point object at 0x7fdfb4cdacf8>]\n\nIn [5]: model.points[1].x\nOut[5]: 56\n\nIn [6]: model.points[1].y\nOut[6]: 78\n\n\n\n\nXtext is Java based and works as generator thus it is not possible, as far as\nwe know, to experiment in this way.\n\n\nPost-processing\n\u00b6\n\n\ntextX provide model objects post processing by registering a Python callable\nthat will receive object as it is constructed. Post-processing is used for\nall sorts of things, from model semantic validation to model augmentation.\n\n\nAn approach to augment model after loading in Xtext is given\n\nhere\n.\n\n\nParser control\n\u00b6\n\n\nIn textX several aspect of \nparsing can be controlled\n:\n\n\n\n\nWhitespaces\n\n\nCase sensitivity\n\n\nKeyword handling\n\n\n\n\nThese settings are altered during meta-model construction.  Whitespaces can be\nfurther controlled on a per-rule basis.\n\n\nXtext enable \nhidden terminal\nsymbols\n\nwhich can be used for whitespace handling. Case sensitivity can be altered\nfor \nparser rules but not for lexer\nrules\n.\n\n\nMapping to host language types\n\u00b6\n\n\ntextX will dynamically create ordinary Python classes from the grammar rules.\nYou can \nregister your own classes\n during\nmeta-model construction which will be used instead. Thus, it is easy to provide\nyour domain model in the form of Python classes.\n\n\nXtext is based on ECore model, thus all concepts will be instances of ECore\nclasses. Additionally, there is \nan\nAPI\n which can be\nused to dynamically build JVM types from the DSL concepts providing tight\nintegration with JVM.\n\n\nBuilt-in objects\n\u00b6\n\n\nIn textX you can provide objects that will be available to every model. It is\nused to provide, e.g. built-in types of the language. For more details see\n\nbuilt-in objects section\n in the docs.\n\n\nAn approach to augment model after loading in Xtext is given\n\nhere\n.\n\n\nAdditional languages\n\u00b6\n\n\nXtext use two additional DSLs:\n\n\n\n\nXbase - a general expression language\n\n\nXtend - a modern Java dialect which can be used in various places in the\n  Xtext framework\n\n\n\n\nThe only additional DSL used in textX is genconf which is a DSL for generator\nconfiguration and has been developed as a part of\n\ntextx-tools project\n.\n\n\nTemplate engines\n\u00b6\n\n\ntextX doesn't impose a particular template engine to be used for code\ngeneration. Although we use \nJinja2\n in some of the\nexamples, there is nothing in textX that is Jinja2 specific. You can use any\ntemplate engine you like.\n\n\nXtext provide it's own template language as a part of Xtend DSL. This language\nnicely integrates in the overall platform.\n\n\nIDE integration\n\u00b6\n\n\nXtext is integrated in Eclipse and InteliJ IDEs and generates full\nlanguage-specific tool-chain from the grammar description and additional\nspecifications.\n\n\ntextX does not provide IDE integrations. There\nis \ntextx-tools\n project which\nprovide pluggable platform for developing textX languages and generators with\nproject scaffolding. Integration for popular code editors is planned. There is\nsome basic \nsupport for vim\n\nand \nemacs\n at the moment. There is a\nsupport for visualization of grammars (meta-models) and models but the model\nvisualization is generic, i.e. it will show you the object graph of your model\nobjects. We plan to develop language-specific model visualization support.",
            "title": "Comparison"
        },
        {
            "location": "/about/comparison/#comparing-textx-to-other-tools",
            "text": "There are generally two classes of textual DSL tools textX could be compared to.  The first class comprises tools that use traditional parsing technologies, i.e.\ngiven a grammar (usually a  context-free\ngrammar ) they produce\nprogram code capable of recognizing whether a given textual input conforms to\nthe given grammar. Furthermore, they enable either transformation of textual\ninput to a tree structure (i.e. parse tree), that is processed afterwards, or\ndefinition of actions that should be executed during parsing if a particular\npattern is recognized. Most popular representatives in this class are  lex and\nyacc ,  ANTLR ,  GNU\nbison , These kind of tools are generally\nknown by the name  Parser Generators .  textX's differences in regard to this first class are following:   textX works as grammar interpreter i.e. parser code is not generated by the\n  tool but the tools is configured by the grammar to recognize textual input on\n  the language specified by the grammar. You can even embed your grammar as a\n  Python string. This enables faster round-trip from grammar to the working\n  parsers as the parser don't need to be regenerated but only reconfigured.  Most of the classical parsing tools use context-free grammars while textX\n  uses  PEG\n  grammars . The\n  consequences are that lookahead is unlimited and there are no ambiguities\n  possible as the  alternative operator is ordered .\n  Additionally, there is no need for a separate lexer.  textX uses a single textual specification (grammar) to define not only the\n  syntax of the language but also its meta-model (a.k.a. abstract syntax). The\n  textX's meta-language is inspired by Xtext. This is very important feature\n  which enables automatic construction of the model (a.k.a. abstract semantic\n  graph - ASG or semantic model) without further work from the language\n  designer. In traditional parsing tools transformation to the model usually\n  involves coding of parse actions or manually written parse tree\n  transformation.   The second class of textual DSL tools are more powerful tools geared especially\ntowards DSL construction. These kind of tools are generally known by the name Language Workbenches  coined by Martin\nFowler . Most popular\nrepresentatives of this class are  Xtext , Spoofax  and MPS . These tools are much more complex,\nhighly integrated to the particular development environment (IDE) but provide\npowerful tooling infrastructure for language development, debugging and\nevolving. These tools will build not only parser but also a language-specific\neditor, debugger, validator, visualiser etc.  textX is positioned between these two classes of DSL tools. The goal of textX\nproject is not a highly sophisticated DSL engineering platform but a simple DSL\nPython library that can be used in various Python applications and development\nenvironment. It can also be used for non-Python development using code\ngeneration from textX models (see  Entity tutorial ).\nTooling infrastructure, editor support etc. will be developed as independent\nprojects (see for example textx-tools ).",
            "title": "Comparing textX to other tools"
        },
        {
            "location": "/about/comparison/#difference-to-xtext-grammar-language",
            "text": "textX grammar language is inspired by Xtext and thus there are a lot of\nsimilarities between these tools when it come to grammar specification. But,\nthere are also differences in several places. In this section we shall outline\nthose differences to give users already familiar with Xtext a brief overview.",
            "title": "Difference to Xtext grammar language"
        },
        {
            "location": "/about/comparison/#lexer-and-terminal-rules",
            "text": "textX uses PEG parsing which doesn't needs separate lexing phase.  This\neliminate the need to define lexemes in the grammar.  Therefore, there is no terminal  keyword in the textX nor any of special terminal definition rules\nused by Xtext.",
            "title": "Lexer and terminal rules"
        },
        {
            "location": "/about/comparison/#types-used-for-rules",
            "text": "Xtext integrates tightly with Java and Ecore typing system providing keyword returns  in rule definition by which language designer might define a class\nused to instantiate objects recognized by the parser.  textX integrates with Python typing system. In textX there is no keyword returns . The class used for the rule will be dynamically created Python class\nfor all  non-match rules . Language designer can\nprovide class using  user classes  registration on meta-model.  If the rule\nis of [match type] than it will always return Python string or some of base\nPython types for  BASETYPES inherited rules .",
            "title": "Types used for rules"
        },
        {
            "location": "/about/comparison/#assignments",
            "text": "In textX there are two types of  many  assignments ( *=  - zero or more,  +=  -\none or more) whereas in Xtext there is only one ( += ) which defines the type\nof the inferred attribute but doesn't specify any information for the parser.\nThus, if there should be zero or more matched elements you must additionally\nwrap your expression in  zero or more  match:  In Xtext:  Domainmodel :\n    (elements+=Type)*;  In textX:  Domainmodel :\n    elements*=Type;  Similarly, optional assignment in Xtext is written as:  static?='static'?  In textX a '?' at the end of the expression is implied, i.e. rhs of the\nassignment will be optional:  static?='static'",
            "title": "Assignments"
        },
        {
            "location": "/about/comparison/#regular-expression-match",
            "text": "In Xtext  terminal rules are described using\nEBNF .  In textX there is no difference between parser and terminal rules so you can\nuse the full textX language to define terminals. Furthermore, textX gives you\nthe full power of  Python regular\nexpressions  through regular expression match . Regex matches are defined\ninside  / / . Anything you can use in Python  re\nmodule  you can use here.  This\ngives you quite powerful sublanguage for pattern definition.  In Xtext:  terminal ASCII:\n    '0x' ('0'..'7') ('0'..'9'|'A'..'F');  In textX:  ASCII:\n    /0x[0-7]([0-9]|[A-F])/;  Literal Regex match can be used anywhere a regular match rule can be used.  For example:  Person:\n    name=/[a-zA-Z]+/ age=INT;",
            "title": "Regular expression match"
        },
        {
            "location": "/about/comparison/#repetition-modifiers",
            "text": "textX provides a syntactic construct called  repetition\nmodifier  which enables parser to be altered\nduring parsing of a specific repetition expression.  For example, there is often a need to define a separated list of elements.  To match a list of integers separated by comma in Xtext you would write:  list_of_ints+=INT (',' list_of_ints+=INT)*  In textX the same expression can be written as:  list_of_inst+=INT[',']  The parser is instructed to parse one or more INT with commas in between.\nRepetition modifier can be a regular expression match too.  For example, to match one or more integer separated by comma or semi-colon:  list_of_ints+=INT[/,|;/]  Inside square brackets more than one repetition modifier can be defined.  See section in the docs  for additional\nexplanations.  We are not aware of the similar feature in Xtext.",
            "title": "Repetition modifiers"
        },
        {
            "location": "/about/comparison/#rule-modifiers",
            "text": "Similarly to repetition modifiers, in textX parser can be altered at the rule\nlevel too. Currently, only white-space alteration can be defined on the rule\nlevel:  For example:      Rule:\n        'entity' name=ID /\\s*/ call=Rule2;\n    Rule2[noskipws]:\n        'first' 'second';  Parser will be altered for  Rule2  not to skip white-spaces. All rules down the\ncall chain inherit modifiers.  There are hidden rules in Xtext which can achieve the similar effect, even\ndefine different kind of tokens that can be hidden from the semantic model, but\nthe rule modifier in textX serve different purpose. It is a general mechanism\nfor parser alteration per rule that can be used in the future to define some\nother alteration (e.g. case sensitivity).",
            "title": "Rule modifiers"
        },
        {
            "location": "/about/comparison/#unordered-groups",
            "text": "Xtext support unordered groups using  &  operator.  For example:  Modifier: \n    static?='static'? & final?='final'? & visibility=Visibility;\n\nenum Visibility:\n    PUBLIC='public' | PRIVATE='private' | PROTECTED='protected';  In textX unordered groups are specified as a special kind of repetitions. Thus,\nrepetition modifiers can be applied also:  Modifier: \n    (static?='static' final?='final' visibility=Visibility)#[',']\n\nVisibility:\n    'public' | 'private' | 'protected';  Previous example will match any of the following:  private, static, final\nstatic, private, final\n...  Notice the use of  ,  separator as a repetition modifier.",
            "title": "Unordered groups"
        },
        {
            "location": "/about/comparison/#syntactic-predicates",
            "text": "textX is based on PEG grammars. Unlike CFGs, PEGs can't be ambiguous, i.e. if\nan input parses it has exactly one parse tree. textX is backtracking parser and\nwill try each alternative in predetermined order until it succeeds. Thus, textX\ngrammar can't be ambiguous. Nevertheless, sometimes it is not possible to\nspecify desired parse tree by reordering alternatives. In that case syntactic\npredicates are used. textX implements both  and- and not- syntactic\npredicates .  On the other hand, predictive non-backtracking parsers (as is ANTLR used by\nXtext) must make a decision which alternative to chose. Thus, grammar might be\nambiguous and additional specification is needed by a language designer to\nresolve ambiguity and choose desired parse tree. Xtext uses a positive\nlookahead syntactic predicates (=> and ->).  See here .",
            "title": "Syntactic predicates"
        },
        {
            "location": "/about/comparison/#hidden-rules",
            "text": "Xtext uses  hidden terminal\nsymbols \nto suppress non-important parts of the input from the semantic model. This is\nused for comments, whitespaces etc.  Terminal rules are referenced from the hidden  list in the parser rules. All rules called from the one using hidden\nterminals inherits them.  textX provides support for whitespaces alteration on the parser level and rule\nlevel and a special  Comment match rule  that can\nbe used to describe comments pattern which are suppressed from the model.\nComment rule is currently defined for the whole grammar, i.e. can't be altered\non a per-rule basis.",
            "title": "Hidden rules"
        },
        {
            "location": "/about/comparison/#parent-child-relationships",
            "text": "textX will provide explicit  parent  reference on all objects that are contained  inside some other objects.  This attribute is a plain Python\nattribute. The relationship is imposed by the grammar.  Xtext, begin based on Ecore, provides similar mechanism through Ecore API.",
            "title": "Parent-child relationships"
        },
        {
            "location": "/about/comparison/#enums",
            "text": "Xtext support  Enum\nrules \nwhile textX does not. In textX you use match rule with ordered choice to mimic\nenums.",
            "title": "Enums"
        },
        {
            "location": "/about/comparison/#scoping",
            "text": "At present stage textX doesn't provide builtin mechanism for scoping definition.\nHowever, this can be done in Python using object processors but there is no\nspecific scoping API that could help language developer in resolving links.  Xtext does provide a  Scoping\nAPI \nwhich can be used by the Xtend code to specify scoping rules.",
            "title": "Scoping"
        },
        {
            "location": "/about/comparison/#additional-differences-in-the-tool-usage",
            "text": "Some of the differences in tools usage are outlined here.",
            "title": "Additional differences in the tool usage"
        },
        {
            "location": "/about/comparison/#repl",
            "text": "textX is Python based, thus it is easy to interactively play with it on the\nPython console.  Example ipython session:  In [1]: from textx import metamodel_from_str\n\nIn [2]: mm = metamodel_from_str(\"\"\"\n...: Model: points+=Point;\n...: Point: x=INT ',' y=INT ';';\n...: \"\"\")\n\nIn [3]: model = mm.model_from_str(\"\"\"\n...: 34, 45; 56, 78; 88, 12;\"\"\")\n\nIn [4]: model.points\nOut[4]: \n[<textx:Point object at 0x7fdfb4cda828>,\n<textx:Point object at 0x7fdfb4cdada0>,\n<textx:Point object at 0x7fdfb4cdacf8>]\n\nIn [5]: model.points[1].x\nOut[5]: 56\n\nIn [6]: model.points[1].y\nOut[6]: 78  Xtext is Java based and works as generator thus it is not possible, as far as\nwe know, to experiment in this way.",
            "title": "REPL"
        },
        {
            "location": "/about/comparison/#post-processing",
            "text": "textX provide model objects post processing by registering a Python callable\nthat will receive object as it is constructed. Post-processing is used for\nall sorts of things, from model semantic validation to model augmentation.  An approach to augment model after loading in Xtext is given here .",
            "title": "Post-processing"
        },
        {
            "location": "/about/comparison/#parser-control",
            "text": "In textX several aspect of  parsing can be controlled :   Whitespaces  Case sensitivity  Keyword handling   These settings are altered during meta-model construction.  Whitespaces can be\nfurther controlled on a per-rule basis.  Xtext enable  hidden terminal\nsymbols \nwhich can be used for whitespace handling. Case sensitivity can be altered\nfor  parser rules but not for lexer\nrules .",
            "title": "Parser control"
        },
        {
            "location": "/about/comparison/#mapping-to-host-language-types",
            "text": "textX will dynamically create ordinary Python classes from the grammar rules.\nYou can  register your own classes  during\nmeta-model construction which will be used instead. Thus, it is easy to provide\nyour domain model in the form of Python classes.  Xtext is based on ECore model, thus all concepts will be instances of ECore\nclasses. Additionally, there is  an\nAPI  which can be\nused to dynamically build JVM types from the DSL concepts providing tight\nintegration with JVM.",
            "title": "Mapping to host language types"
        },
        {
            "location": "/about/comparison/#built-in-objects",
            "text": "In textX you can provide objects that will be available to every model. It is\nused to provide, e.g. built-in types of the language. For more details see built-in objects section  in the docs.  An approach to augment model after loading in Xtext is given here .",
            "title": "Built-in objects"
        },
        {
            "location": "/about/comparison/#additional-languages",
            "text": "Xtext use two additional DSLs:   Xbase - a general expression language  Xtend - a modern Java dialect which can be used in various places in the\n  Xtext framework   The only additional DSL used in textX is genconf which is a DSL for generator\nconfiguration and has been developed as a part of textx-tools project .",
            "title": "Additional languages"
        },
        {
            "location": "/about/comparison/#template-engines",
            "text": "textX doesn't impose a particular template engine to be used for code\ngeneration. Although we use  Jinja2  in some of the\nexamples, there is nothing in textX that is Jinja2 specific. You can use any\ntemplate engine you like.  Xtext provide it's own template language as a part of Xtend DSL. This language\nnicely integrates in the overall platform.",
            "title": "Template engines"
        },
        {
            "location": "/about/comparison/#ide-integration",
            "text": "Xtext is integrated in Eclipse and InteliJ IDEs and generates full\nlanguage-specific tool-chain from the grammar description and additional\nspecifications.  textX does not provide IDE integrations. There\nis  textx-tools  project which\nprovide pluggable platform for developing textX languages and generators with\nproject scaffolding. Integration for popular code editors is planned. There is\nsome basic  support for vim \nand  emacs  at the moment. There is a\nsupport for visualization of grammars (meta-models) and models but the model\nvisualization is generic, i.e. it will show you the object graph of your model\nobjects. We plan to develop language-specific model visualization support.",
            "title": "IDE integration"
        },
        {
            "location": "/about/discuss/",
            "text": "Discuss, ask questions\n\u00b6\n\n\nFor bug reports, general discussion and help please use \nGitHub issue\ntracker\n.",
            "title": "Discuss"
        },
        {
            "location": "/about/discuss/#discuss-ask-questions",
            "text": "For bug reports, general discussion and help please use  GitHub issue\ntracker .",
            "title": "Discuss, ask questions"
        },
        {
            "location": "/about/contributing/",
            "text": "Contributing\n\u00b6\n\n\ntextX is open for contributions. You can contribute code, documentation, tests,\nbug reports.  If you plan to make a contribution it would be great if you first\nannounce that on the discussion forum.\n\n\nFor bug reports please use \ngithub issue\ntracker\n.\n\n\nFor code/doc/test contributions do the following:\n\n\n\n\nFork the \nproject on github\n.\n\n\nClone your fork.\n\n\nMake a branch for the new feature and switch to it.\n\n\nMake one or more commits.\n\n\nPush your branch to github.\n\n\nMake a pull request. I will look at the changes and if everything is ok I will pull it in.\n\n\n\n\n\n\nNote\n\n\nFor code contributions please try to adhere to the \nPEP-8\nguidelines\n.  Although I am not\nstrict in that regard it is useful to have a common ground for the coding style.\nTo make things easier use tools for code checking (PyLint, PyFlakes, pep8\netc.).",
            "title": "Contributing"
        },
        {
            "location": "/about/contributing/#contributing",
            "text": "textX is open for contributions. You can contribute code, documentation, tests,\nbug reports.  If you plan to make a contribution it would be great if you first\nannounce that on the discussion forum.  For bug reports please use  github issue\ntracker .  For code/doc/test contributions do the following:   Fork the  project on github .  Clone your fork.  Make a branch for the new feature and switch to it.  Make one or more commits.  Push your branch to github.  Make a pull request. I will look at the changes and if everything is ok I will pull it in.    Note  For code contributions please try to adhere to the  PEP-8\nguidelines .  Although I am not\nstrict in that regard it is useful to have a common ground for the coding style.\nTo make things easier use tools for code checking (PyLint, PyFlakes, pep8\netc.).",
            "title": "Contributing"
        },
        {
            "location": "/about/license/",
            "text": "textX is distributed under the terms of MIT license.\n\n\nCopyright (c) 2014-2015 Igor R. Dejanovi\u0107 \n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.",
            "title": "License"
        },
        {
            "location": "/whatsnew/release_1_5/",
            "text": "What's new in textX 1.5\n\u00b6\n\n\nIt's been quite a while since the last release of textX so this release brings a\nlot of new features and fixes.\n\n\nOperator precedence change\n\u00b6\n\n\nProbably the most important/visible change is the change in the operator\nprecendence. In the previous version, sequence had lower precedence than\nordered choice which is counter-intuitive to the users that had previous\nexperience with other tools where this is not the case.\n\n\nOrdered choice is now of lowest precedence which brings some backward\ncompatibility that should be addressed for migration to the new version.\n\n\nFor example, previously you would write:\n\n\nRule:\n    ('a'  'b') | ('c' 'd')\n;\n\n\n\nTo match either \na b\n or \nc d\n.\n\n\nNow you can drop parentheses as the precedence of sequence is higher:\n\n\nRule:\n    'a'  'b' | 'c' 'd'\n;\n\n\n\nFor the previous case there would be no problem in upgrade to 1.5 even if the\ngrammar is not changed. But consider this:\n\n\nRule:\n    'a' 'b' | 'c' 'd'\n;\n\n\n\nIn the previous version this would match \na\n, than \nb\n or \nc\n, and then \nd\n as\nthe \n|\n operator was of higher precedence than sequence.\n\n\nFor your grammar to match the same language you must now write:\n\n\nRule:\n  'a' ('b' | 'c') 'd'\n;\n\n\n\nUnordered groups\n\u00b6\n\n\nThere is often a need to specify several matches that should occur in an arbitrary\norder. \n\n\nRead more \nhere\n\n\nMatch filters\n\u00b6\n\n\nMatch rules always return Python strings. Built-in rules return proper Python\ntypes. In order to change what is returned by match rules you can now register\npython callables that can additionally process returned strings.\n\n\nRead more \nhere\n\n\nMultiple assignments to the same attribute\n\u00b6\n\n\ntextX had support for multiple assignments but it wasn't complete. When multiple\nassignment is detected, in the previous version, textX will decide that the\nmultiplicity of the attribute is \nmany\n but this lead to the problem if there is\nno way for parser to collect more than one value even if there is multiple\nassignments. For example, if all assignments belong to a different ordered\nchoice alternative (see issue #33).\n\n\nIn this version textX will correctly identify such cases. \n\n\nRead more \nhere\n\n\nModel API\n\u00b6\n\n\nThere is now a set of handful functions for model querying.\n\n\nRead more \nhere\n.\n\n\nAdditional special model attributes\n\u00b6\n\n\nIn addition to \n_tx_position\n there is now \n_tx_position_end\n attribute on each\nmodel object which has the value of the end of the match in the input stream.\n\n\nIn addition there is now \n_tx_metamodel\n attribute on the model which enables\neasy access to the language meta-model.\n\n\nRead more about it \nhere\n.\n\n\ntextX Emacs mode\n\u00b6\n\n\ntextX now has a \nsupport for Emacs\n.\nThis package is also available in \nMELPA\n.",
            "title": "Release 1.5"
        },
        {
            "location": "/whatsnew/release_1_5/#whats-new-in-textx-15",
            "text": "It's been quite a while since the last release of textX so this release brings a\nlot of new features and fixes.",
            "title": "What's new in textX 1.5"
        },
        {
            "location": "/whatsnew/release_1_5/#operator-precedence-change",
            "text": "Probably the most important/visible change is the change in the operator\nprecendence. In the previous version, sequence had lower precedence than\nordered choice which is counter-intuitive to the users that had previous\nexperience with other tools where this is not the case.  Ordered choice is now of lowest precedence which brings some backward\ncompatibility that should be addressed for migration to the new version.  For example, previously you would write:  Rule:\n    ('a'  'b') | ('c' 'd')\n;  To match either  a b  or  c d .  Now you can drop parentheses as the precedence of sequence is higher:  Rule:\n    'a'  'b' | 'c' 'd'\n;  For the previous case there would be no problem in upgrade to 1.5 even if the\ngrammar is not changed. But consider this:  Rule:\n    'a' 'b' | 'c' 'd'\n;  In the previous version this would match  a , than  b  or  c , and then  d  as\nthe  |  operator was of higher precedence than sequence.  For your grammar to match the same language you must now write:  Rule:\n  'a' ('b' | 'c') 'd'\n;",
            "title": "Operator precedence change"
        },
        {
            "location": "/whatsnew/release_1_5/#unordered-groups",
            "text": "There is often a need to specify several matches that should occur in an arbitrary\norder.   Read more  here",
            "title": "Unordered groups"
        },
        {
            "location": "/whatsnew/release_1_5/#match-filters",
            "text": "Match rules always return Python strings. Built-in rules return proper Python\ntypes. In order to change what is returned by match rules you can now register\npython callables that can additionally process returned strings.  Read more  here",
            "title": "Match filters"
        },
        {
            "location": "/whatsnew/release_1_5/#multiple-assignments-to-the-same-attribute",
            "text": "textX had support for multiple assignments but it wasn't complete. When multiple\nassignment is detected, in the previous version, textX will decide that the\nmultiplicity of the attribute is  many  but this lead to the problem if there is\nno way for parser to collect more than one value even if there is multiple\nassignments. For example, if all assignments belong to a different ordered\nchoice alternative (see issue #33).  In this version textX will correctly identify such cases.   Read more  here",
            "title": "Multiple assignments to the same attribute"
        },
        {
            "location": "/whatsnew/release_1_5/#model-api",
            "text": "There is now a set of handful functions for model querying.  Read more  here .",
            "title": "Model API"
        },
        {
            "location": "/whatsnew/release_1_5/#additional-special-model-attributes",
            "text": "In addition to  _tx_position  there is now  _tx_position_end  attribute on each\nmodel object which has the value of the end of the match in the input stream.  In addition there is now  _tx_metamodel  attribute on the model which enables\neasy access to the language meta-model.  Read more about it  here .",
            "title": "Additional special model attributes"
        },
        {
            "location": "/whatsnew/release_1_5/#textx-emacs-mode",
            "text": "textX now has a  support for Emacs .\nThis package is also available in  MELPA .",
            "title": "textX Emacs mode"
        }
    ]
}